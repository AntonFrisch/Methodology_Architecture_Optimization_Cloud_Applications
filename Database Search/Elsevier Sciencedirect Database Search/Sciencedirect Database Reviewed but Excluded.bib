@article{MENDES202416,
title = {MAS-Cloud+: A novel multi-agent architecture with reasoning models for resource management in multiple providers},
journal = {Future Generation Computer Systems},
volume = {154},
pages = {16-34},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004776},
author = {Aldo H.D. Mendes and Michel J.F. Rosa and Marcelo A. Marotta and Aleteia Araujo and Alba C.M.A. Melo and Célia Ghedini Ralha},
keywords = {Cloud computing, Heuristics, Optimization, Metaheuristics, Multi-agent system, Resource provisioning},
abstract = {Nowadays, scientific and commercial applications are often deployed to cloud environments requiring multiple resource types. This scenario increases the necessity for efficient resource management. However, efficient resource management remains challenging due to the complex nature of modern cloud-distributed systems since resources involve different characteristics, technologies, and financial costs. Thus, optimized cloud resource management to support the heterogeneous nature of applications balancing cost, time, and waste remains a challenge. Multi-agent technologies can offer noticeable improvements for resource management, with intelligent agents deciding on Virtual Machine (VM) resources. This article proposes MAS-Cloud+, a novel agent-based architecture for predicting, provisioning, and monitoring optimized cloud computing resources. MAS-Cloud+ implements agents with three reasoning models including heuristic, formal optimization, and metaheuristic. MAS-Cloud+ instantiates VMs considering Service Level Agreement (SLA) on cloud platforms, prioritizing user needs considering time, cost, and waste of resources providing appropriate selection for evaluated workloads. To validate MAS-Cloud+, we use a DNA sequence comparison application subjected to different workload sizes and a comparative study with state-of-the-art work with Apache Spark benchmark applications executed on the AWS EC2. Our results show that to execute the sequence comparison application, the best performance was obtained by the optimization model, whereas the heuristic model presented the best cost. By providing the choice among multiple reasoning models, our results show that MAS-Cloud+ could provide a more cost-effective selection of the instances reducing ≈58% of execution average cost of WorkdCount, Sort, and PageRank BigDataBench benchmarking workloads. As for the execution time, the WorkdCount and PageRank present reduction, the latter with ≈58%. The results indicate a promising solution for efficient cloud resource management.}
}
@article{JOSEPH2020101785,
title = {IntMA: Dynamic Interaction-aware resource allocation for containerized microservices in cloud environments},
journal = {Journal of Systems Architecture},
volume = {111},
pages = {101785},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101785},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120300758},
author = {Christina Terese Joseph and K. Chandrasekaran},
keywords = {Microservice placement, Container orchestration, Service oriented computing, Cloud computing, Performance optimization},
abstract = {The Information Technology sector has undergone tremendous changes arising due to the emergence and prevalence of Cloud Computing. Microservice Architectures have also been attracting attention from several industries and researchers. Due to the suitability of microservices for the Cloud environments, an increasing number of Cloud applications are now provided as microservices. However, this transition to microservices brings a wide range of infrastructural orchestration challenges. Though several research works have discussed the engineering of microservice-based applications, there is an inevitable need for research on handling the operational phases of the microservice components. Microservice application deployment in containerized datacenters must be optimized to enhance the overall system performance. In this research work, the deployment of microservice application modules on the Cloud infrastructure is first modelled as a Binary Quadratic Programming Problem. In order to reduce the adverse impact of communication latencies on the response time, the interaction pattern between the microservice components is modelled as an undirected doubly weighted complete Interaction Graph. A novel, robust heuristic approach IntMA is also proposed for deploying the microservices in an interaction-aware manner with the aid of the interaction information obtained from the Interaction Graph. The proposed allocation policies are implemented in Kubernetes. The effectiveness of the proposed approach is evaluated on the Google Cloud Platform, using different microservice reference applications. Experimental results indicate that the proposed approach improves the response time and throughput of the microservice-based systems.}
}
@article{ALZAMIL201591,
title = {Energy-Aware Profiling for Cloud Computing Environments},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {318},
pages = {91-108},
year = {2015},
note = {Twenty-ninth and thirtieth Annual UK Performance Engineering Workshops (UKPEW)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2015.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S1571066115000626},
author = {Ibrahim Alzamil and Karim Djemame and Django Armstrong and Richard Kavanagh},
keywords = {Cloud Computing, Energy Efficiency, Energy-Aware Profiling, Energy Efficiency Metrics},
abstract = {Cloud Computing has changed the way in which people use the IT resources today. Now, instead of buying their own IT resources, they can use the services offered by Cloud Computing with reasonable costs based on a “pay-per-use” model. However, with the wide adoption of Cloud Computing, the costs for maintaining the Cloud infrastructure have become a vital issue for the providers, especially with the large input of energy costs to underpin these resources. Thus, this paper proposes a system architecture that can be used for profiling the resources usage in terms of the energy consumption. From the profiled data, the application developers can enhance their energy-aware decisions when creating or optimising the applications to be more energy efficient. This paper also presents an adapted existing Cloud architecture to enable energy-aware profiling based on the proposed system. The results of the conducted experiments show energy-awareness at physical host and virtual machine levels.}
}
@article{NVHATKAR20221906,
title = {Optimal container resource allocation in cloud architecture: A new hybrid model},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {5},
pages = {1906-1918},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819307190},
author = {Kapil {N. Vhatkar} and Girish P. Bhole},
keywords = {Cloud computing, Container resource allocation, Microservices, Optimization, Lion algorithm, Whale optimization algorithm},
abstract = {A huge variety of fields and industries depend upon cloud computing based microservice due to its high-performance capability. Also, the merit of container usage is enormous; it enable larger portability, easier and faster deployment and restricted overheads. However, the rapid evolution causes issues in terms of container automation and management, Till now, a number of research works has concentrated on solving the open issues in container automation and management. In fact, container resource allocation is the major key hole for cloud providers since it directly influences the resource consumption and system performance. In this manner, this paper introduces a new optimized container resource allocation model by proposing a new optimization concept. To make the possibility of optimal container resource allocation, a new hybridized algorithm is implanted; namely, Whale Random update assisted Lion Algorithm (WR-LA), which is the hybrid form of Lion Algorithm (LA) and Whale Optimization Algorithm (WOA) is introduced. Moreover, the solution of optimized resource allocation is made by considering objectives like Threshold Distance, Balanced Cluster Use, System Failure, and Total Network Distance, respectively. Finally, the performance of the proposed model is compared over other conventional models and proves its superiority.}
}
@article{GRZEGOROWSKI2021100203,
title = {Cost Optimization for Big Data Workloads Based on Dynamic Scheduling and Cluster-Size Tuning},
journal = {Big Data Research},
volume = {25},
pages = {100203},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100203},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000204},
author = {Marek Grzegorowski and Eftim Zdravevski and Andrzej Janusz and Petre Lameski and Cas Apanowicz and Dominik Ślęzak},
keywords = {Big Data, ETL, Cloud computing, Spot price prediction, ARIMA, Spark},
abstract = {Analytical data processing has become the cornerstone of today's businesses success, and it is facilitated by Big Data platforms that offer virtually limitless scalability. However, minimizing the total cost of ownership (TCO) for the infrastructure can be challenging. We propose a novel method to build resilient clusters on cloud resources that are fine-tuned to the particular data processing task. The presented architecture follows the infrastructure-as-a-code paradigm so that the cluster can be dynamically configured and managed. It first identifies the optimal cluster size to perform a job in the required time. Then, by analyzing spot instance price history and using ARIMA models, it optimizes the schedule of the job execution to leverage the discounted prices of the cloud spot market. In particular, we evaluated savings opportunities when using Amazon EC2 spot instances comparing to on-demand resources. The performed experiments confirmed that the prediction module significantly improved the cost-effectiveness of the solution – up to 80% savings compared to the on-demand prices, and at the worst-case, 1% more cost than the absolute minimum. The production deployments of the architecture show that it is invaluable for minimizing the total cost of ownership of analytical data processing solutions.}
}
@article{PEREZ201850,
title = {Serverless computing for container-based architectures},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {50-59},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316485},
author = {Alfonso Pérez and Germán Moltó and Miguel Caballer and Amanda Calatrava},
keywords = {Cloud computing, Serverless, Docker, Elasticity, AWS lambda},
abstract = {New architectural patterns (e.g. microservices), the massive adoption of Linux containers (e.g. Docker containers), and improvements in key features of Cloud computing such as auto-scaling, have helped developers to decouple complex and monolithic systems into smaller stateless services. In turn, Cloud providers have introduced serverless computing, where applications can be defined as a workflow of event-triggered functions. However, serverless services, such as AWS Lambda, impose serious restrictions for these applications (e.g. using a predefined set of programming languages or difficulting the installation and deployment of external libraries). This paper addresses such issues by introducing a framework and a methodology to create Serverless Container-aware ARchitectures (SCAR). The SCAR framework can be used to create highly-parallel event-driven serverless applications that run on customized runtime environments defined as Docker images on top of AWS Lambda. This paper describes the architecture of SCAR together with the cache-based optimizations applied to minimize cost, exemplified on a massive image processing use case. The results show that, by means of SCAR, AWS Lambda becomes a convenient platform for High Throughput Computing, specially for highly-parallel bursty workloads of short stateless jobs.}
}
@article{FERRER201266,
title = {OPTIMIS: A holistic approach to cloud service provisioning},
journal = {Future Generation Computer Systems},
volume = {28},
number = {1},
pages = {66-77},
year = {2012},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1100104X},
author = {Ana Juan Ferrer and Francisco Hernández and Johan Tordsson and Erik Elmroth and Ahmed Ali-Eldin and Csilla Zsigri and Raül Sirvent and Jordi Guitart and Rosa M. Badia and Karim Djemame and Wolfgang Ziegler and Theo Dimitrakos and Srijith K. Nair and George Kousiouris and Kleopatra Konstanteli and Theodora Varvarigou and Benoit Hudzia and Alexander Kipp and Stefan Wesner and Marcelo Corrales and Nikolaus Forgó and Tabassum Sharif and Craig Sheridan},
abstract = {We present fundamental challenges for scalable and dependable service platforms and architectures that enable flexible and dynamic provisioning of cloud services. Our findings are incorporated in a toolkit targeting the cloud service and infrastructure providers. The innovations behind the toolkit are aimed at optimizing the whole service life cycle, including service construction, deployment, and operation, on a basis of aspects such as trust, risk, eco-efficiency and cost. Notably, adaptive self-preservation is crucial to meet predicted and unforeseen changes in resource requirements. By addressing the whole service life cycle, taking into account several cloud architectures, and by taking a holistic approach to sustainable service provisioning, the toolkit aims to provide a foundation for a reliable, sustainable, and trustful cloud computing industry.}
}
@article{ALAM2013194,
title = {5-Layered Architecture of Cloud Database Management System},
journal = {AASRI Procedia},
volume = {5},
pages = {194-199},
year = {2013},
note = {2013 AASRI Conference on Parallel and Distributed Computing and Systems},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2013.10.078},
url = {https://www.sciencedirect.com/science/article/pii/S2212671613000796},
author = {Bashir Alam and M.N. Doja and Mansaf Alam and Shweta Mongia},
keywords = {Cloud Computing, Cloud Database Management System, Database as a Service (DBaaS)},
abstract = {Cloud Database Management System is a new emerging concept recently introduced in the world. In Cloud the concept of Standard architecture of Cloud Database Management System is not yet been implemented. In this paper we are proposing a framework for 5-layered architecture in cloud database management system. First layer introduced is the External Layer, this layer is closest to the user, in which manageability, providing transparency and security are the important issue that should be considered. Second layer is the Conceptual Middleware Layer, as there are heterogeneous databases and clouds are available in the market, so here interoperability is the major issue. Third layer is the Conceptual Layer in which programming techniques, transaction management, query processing and optimization are the issues that should be considered. Forth layer is the Physical Middleware Layer, as there are various platforms available so here also, interoperability between various platforms are the biggest issue and the last layer is the Physical Layer in which how data can be stored so that it can be easily accessible without so much overhead so here data security, storage, backup, load balancing, partitioning, scaling, elasticity, fault tolerance and replication are the important issues that should be considered.}
}
@article{WAN201897,
title = {Application deployment using Microservice and Docker containers: Framework and optimization},
journal = {Journal of Network and Computer Applications},
volume = {119},
pages = {97-109},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518302273},
author = {Xili Wan and Xinjie Guan and Tianjing Wang and Guangwei Bai and Baek-Yong Choi},
keywords = {Application deployment, Microservice architecture, Docker container},
abstract = {To improve the scalability and elasticity of application deployment and operation in cloud computing environments, new architectures and techniques are developed and studied, e.g., microservice architecture, and Docker container. Especially, Docker container enables the sharing on operation system and supporting libraries, which is more lightweight, prompt and scalable than Hypervisor based virtualization. These features make it ideally suit for applications deployed in microservice architecture. However, existing models and schemes, which are mostly designed for Hypervisor based virtualization techniques, fall short to be efficiently used for Docker container based application deployment. To take the benefits of microservice architecture and Docker containers, we explore the optimization of application deployment in cloud data centers using microservice and Docker containers. Our goal is to minimize the application deployment cost as well as the operation cost while preserving service delay requirements for applications. In this paper, we first formulate the application deployment problem by examining the features of Docker, the requirements of microservice-based applications, and available resources in cloud data centers. We further propose a communication efficient framework and a suboptimal algorithm to determine the container placement and task assignment. The proposed algorithm works in a distributed and incremental manner, which makes it scalable to massive physical resources and diverse applications under the framework. We validate the efficiency of our solution through comparisons with three existing strategies in Docker Swarm using real traces from Google Cluster. The evaluation results show that the proposed framework and algorithm provide more flexibility and save more cost than existing strategies.}
}
@article{GARCIAVALDEZ2021234,
title = {A container-based cloud-native architecture for the reproducible execution of multi-population optimization algorithms},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {234-252},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330235},
author = {Mario {García Valdez} and Juan J. {Merelo Guervós}},
keywords = {Multi-population, Nature-inspired algorithm, Parallel genetic algorithms, Cloud-computing, Event-driven architecture},
abstract = {Splitting a population into multiple instances is a technique used extensively in recent years to help improve the performance of nature-inspired optimization algorithms. Work on those populations can be done in parallel, and they can interact asynchronously, a fact that can be leveraged to create scalable implementations based on, among other methods, distributed, multi-threaded, parallel, and cloud-native computing. However, the design of these cloud-native, distributed, multi-population algorithms is not a trivial task. Using as a foundation monolithic (single-instance) solutions, adaptations at several levels, from the algorithmic to the functional, must be made to leverage the scalability, elasticity, (limited) fault-tolerance, reproducibility, and cost-effectiveness of cloud systems while, at the same time, conserving the intended functionality. Instead of an evolutive approach, in this paper, we propose a cloud-native optimization framework created from scratch, that can include multiple (population-based) algorithms without increasing the number of parameters that need tuning. This solution goes beyond the current state of the art, since it can support different algorithms at the same time, work asynchronously, and also be readily deployable to any cloud platform. We evaluate this solution’s performance and scalability, together with the effect other design parameters had on it, particularly the number and the size of populations with respect to problem size. The implemented platform is an excellent alternative for running locally or in the cloud, thus proving that cloud-native bioinspired algorithms perform better in their “natural” environment than other algorithms, and set a new baseline for scaling and performance of this kind of algorithms in the cloud.}
}
@article{AMATO201752,
title = {Model transformations of MapReduce Design Patterns for automatic development and verification},
journal = {Journal of Parallel and Distributed Computing},
volume = {110},
pages = {52-59},
year = {2017},
note = {High Performance and Parallelism for Large Data Sets},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2016.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0743731516301939},
author = {Flora Amato and Francesco Moscato},
keywords = {Cloud computing, Cloud patterns, Formal verification, MapReduce, Model driven engineering},
abstract = {Mapping MapReduce frameworks to Cloud Architecture became a must in last years because of the need of managing large data sets and Big Data in fast, reliable (and as cheap as possible) way. Scientific Literature proposes many works about new architectures, frameworks and algorithms improving and optimizing performances while performing MapReduce operations. Anyway, MapReduce framework is only the starting point for building a plethora of services based on different analyses. This is the reason for recent application of Design Patterns methodologies to develop MapReduce applications. Here we propose a Model Driven Engineering methodology to design, verify and develop MapReduce applications on Cloud Systems. The methodology is driven by MapReduce Design Patterns and is used to analyse soundness and reliability of services based on MapReduce from early design stage to runtime.}
}
@article{BYUN20111011,
title = {Cost optimized provisioning of elastic resources for application workflows},
journal = {Future Generation Computer Systems},
volume = {27},
number = {8},
pages = {1011-1026},
year = {2011},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X11000744},
author = {Eun-Kyu Byun and Yang-Suk Kee and Jin-Soo Kim and Seungryoul Maeng},
keywords = {Resource capacity estimation, Resource allocation, Application workflow, Cloud computing economy},
abstract = {Workflow technologies have become a major vehicle for easy and efficient development of scientific applications. In the meantime, state-of-the-art resource provisioning technologies such as cloud computing enable users to acquire computing resources dynamically and elastically. A critical challenge in integrating workflow technologies with resource provisioning technologies is to determine the right amount of resources required for the execution of workflows in order to minimize the financial cost from the perspective of users and to maximize the resource utilization from the perspective of resource providers. This paper suggests an architecture for the automatic execution of large-scale workflow-based applications on dynamically and elastically provisioned computing resources. Especially, we focus on its core algorithm named PBTS (Partitioned Balanced Time Scheduling), which estimates the minimum number of computing hosts required to execute a workflow within a user-specified finish time. The PBTS algorithm is designed to fit both elastic resource provisioning models such as Amazon EC2 and malleable parallel application models such as MapReduce. The experimental results with a number of synthetic workflows and several real science workflows demonstrate that PBTS estimates the resource capacity close to the theoretical low bound.}
}
@article{FONTANADENARDIN2021102858,
title = {On revisiting energy and performance in microservices applications: A cloud elasticity-driven approach},
journal = {Parallel Computing},
volume = {108},
pages = {102858},
year = {2021},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102858},
url = {https://www.sciencedirect.com/science/article/pii/S0167819121001010},
author = {Igor {Fontana de Nardin} and Rodrigo {da Rosa Righi} and Thiago Roberto {Lima Lopes} and Cristiano {André da Costa} and Heon Young Yeom and Harald Köstler},
keywords = {Elasticity, Energy, Performance, Cloud computing, Microservices},
abstract = {Monolithic applications are a subject that includes several knowledge areas. Sometimes it can be a challenge to optimize CPU or IO requirements because it is not trivial to recognize the problem itself and improve it. There are many approaches to resolve this situation, where a trending one is the microservices. As a variant of the service-oriented architecture, microservices is a technique that arranges an application as a collection of loosely coupled services. This decomposition enables better software management in cloud-based environments since we can replicate each part individually using cloud elasticity to avoid execution bottlenecks. Also, since elasticity mitigates resource overprovisioning, it favors better energy consumption: the cloud owner can redistribute finite available resources among different tenants, and users can pay less to use the infrastructure. However, elasticity tuning is not trivial and depends on several factors, such as user experience, application architecture, and parameter modeling. Today, we observe a lack of initiatives in the literature that address both performance and energy perspectives to support the execution of microservices applications in the cloud. Concerning this context, this article introduces Elergy as a lightweight proactive elasticity model that provides resource reorganization for a cloud-based microservices application. Its differential approach appears in improving energy consumption by periodically handling the most appropriate amount of resources to execute an application while maintaining or yet improving the performance of CPU-bound applications. Elergy performs these functions proactively, in such a way of preventing future problems related to either resource under- or overprovisioning. The results showed energy consumption reduction and a competitive cost (application time x consumed resources) when comparing Elergy with a non-elastic scenario. Elergy obtained savings from 1.93% to 27.92% for energy consumption.}
}
@article{SUN2016146,
title = {ROAR: A QoS-oriented modeling framework for automated cloud resource allocation and optimization},
journal = {Journal of Systems and Software},
volume = {116},
pages = {146-161},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001715},
author = {Yu Sun and Jules White and Sean Eade and Douglas C. Schmidt},
keywords = {Cloud computing, Resource optimization, Load testing and benchmarking},
abstract = {Cloud computing offers a fast, easy and cost-effective way to configure and allocate computing resources for web applications, such as consoles for smart grid applications, medical records systems, and security management platforms. Although a diverse collection of cloud resources (e.g., servers) is available, choosing the most optimized and cost-effective set of cloud resources for a given web application and set of quality of service (QoS) goals is not a straightforward task. Optimizing cloud resource allocation is a critical task for offering web applications using a software as a service model in the cloud, where minimizing operational cost while ensuring QoS goals are met is critical to meeting customer demands and maximizing profit. Manual load testing with different sets of cloud resources, followed by comparison of test results to QoS goals is tedious and inaccurate due to the limitations of the load testing tools, challenges characterizing resource utilization, significant manual test orchestration effort, and challenges identifying resource bottlenecks. This paper introduces our work using a modeling framework – ROAR (Resource Optimization, Allocation and Recommendation System) to simplify, optimize, and automate cloud resource allocation decisions to meet QoS goals for web applications, including complex multi-tier application distributed in different server groups. ROAR uses a domain-specific language to describe the configuration of the web application, the APIs to benchmark and the expected QoS requirements (e.g., throughput and latency), and the resource optimization engine uses model-based analysis and code generation to automatically deploy and load test the application in multiple resource configurations in order to derive a cost-optimal resource configuration that meets the QoS goals.}
}
@article{EVANGELINOU201567,
title = {A Joint Benchmark-Analytic Approach For Design-Time Assessment of Multi-Cloud Applications},
journal = {Procedia Computer Science},
volume = {68},
pages = {67-77},
year = {2015},
note = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.224},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030690},
author = {Athanasia Evangelinou and Michele Ciavotta and George Kousiouris and Danilo Ardagna},
keywords = {Benchmarking, Cloud applications, QoS, Model Driven Design.},
abstract = {Verifying that a software system shows certain non-functional properties is a primary concern for cloud applications. Given the heterogeneous technology offer and the pricing models currently available in the cloud market it is extremely complex to find the deployment that fits the application requirements and provides the best Quality of Service (QoS) and cost trade-offs. This task can be very challenging, even infeasible if performed manually, since the number of solutions may become extremely large depending on the number of possible providers and available technology stacks. Furthermore, with the increasing adoption of cloud computing, there is a need for fair evaluation of cloud systems. Today's cloud services differ among others by cost, performance, consistency guarantees, load-balancing, caching, fault tolerance, and SLAs. Moreover, cloud systems are inherently multi-tenant and their performance can vary over time, depending on the congestion level, provider policies, and the competition among running applications. System architects and developers are challenged with this variety of services and trade-offs. Hence, the purpose of a cloud benchmark should be to help developers when choosing the right architecture and services for their applications. In this paper we propose a joint benchmarking and optimization methodology to support the design and migration of legacy applications to Cloud. Our approach is effective in identifying the deployment of minimum costs, which provide also QoS guarantees.}
}
@article{CORTELLESSA2023107159,
title = {Many-objective optimization of non-functional attributes based on refactoring of software models},
journal = {Information and Software Technology},
volume = {157},
pages = {107159},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107159},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923000137},
author = {Vittorio Cortellessa and Daniele {Di Pompeo} and Vincenzo Stoico and Michele Tucci},
keywords = {Multi-objective optimization, Search-based software engineering, Performance, Reliability, Refactoring, Model-driven engineering, Software architecture},
abstract = {Context:
Software quality estimation is a challenging and time-consuming activity, and models are crucial to face the complexity of such activity on modern software applications. In this context, software refactoring is a crucial activity within development life-cycles where requirements and functionalities rapidly evolve.
Objective:
One main challenge is that the improvement of distinctive quality attributes may require contrasting refactoring actions on software, as for trade-off between performance and reliability (or other non-functional attributes). In such cases, multi-objective optimization can provide the designer with a wider view on these trade-offs and, consequently, can lead to identify suitable refactoring actions that take into account independent or even competing objectives.
Method:
In this paper, we present an approach that exploits the NSGA-II as the genetic algorithm to search optimal Pareto frontiers for software refactoring while considering many objectives. We consider performance and reliability variations of a model alternative with respect to an initial model, the amount of performance antipatterns detected on the model alternative, and the architectural distance, which quantifies the effort to obtain a model alternative from the initial one.
Results:
We applied our approach on two case studies: a Train Ticket Booking Service, and CoCoME. We observed that our approach is able to improve performance (by up to 42%) while preserving or even improving the reliability (by up to 32%) of generated model alternatives. We also observed that there exists an order of preference of refactoring actions among model alternatives.
Conclusion:
Based on our analysis, we can state that performance antipatterns confirmed their ability to improve performance of a subject model in the context of many-objective optimization. In addition, the metric that we adopted for the architectural distance seems to be suitable for estimating the refactoring effort.}
}