@article{ZHENG2024132,
title = {A balanced and reliable data replica placement scheme based on reinforcement learning in edge–cloud environments},
journal = {Future Generation Computer Systems},
volume = {155},
pages = {132-145},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000499},
author = {Mengke Zheng and Xin Du and Zhihui Lu and Qiang Duan},
keywords = {Replica placement, Load balancing, Data reliability, Heterogeneous edge–cloud, Reinforcement learning},
abstract = {With the rapid development of edge–cloud computing, distributing resources to edge nodes and terminal devices to provide high-quality services for latency-sensitive applications and reduce network communication costs has become increasingly important. However, the complexity and heterogeneity of the edge–cloud environment pose significant challenges to the reliability of data storage and device load balancing. To address these issues, in this paper, we propose a Deep Reinforcement Learning (DRL)-based data replica placement scheme, BRPS. This scheme considers the different geographic locations of hardware devices, the heterogeneous storage capacity and reliability, and the different data requirements of varying user services in edge–cloud environments. Firstly, we constructed a model for data replica placement in the edge–cloud environment, addressing key factors such as latency, reliability, and load, focusing on the heterogeneity of device resources and reliability, and the diverse data needs of user services. Furthermore, we propose the BRPS scheme using the Double Deep Q-Network (DDQN) method of DRL, transforming the data replica placement issue into a multi-objective optimization problem. By placing the DRL-based decision process in a separate management edge node, the separation of decision and execution is achieved, which enhances the efficiency of data replica placement, ensures data reliability, reduces latency, and achieves system load balance. Experimental results demonstrate that BRPS significantly outperforms the existing comparison schemes while ensuring data reliability. The BRPS scheme reduces latency by 8.12% compared to the Random scheme and outperforms the best heuristic in system load balancing and memory utilization by 13.15% and 11.91%, respectively. Moreover, BRPS shows superior adaptability in extreme network congestion scenarios and effectively adapts to the dynamic changes of nodes in edge–cloud environments.}
}
@article{FONTANADENARDIN2021102858,
title = {On revisiting energy and performance in microservices applications: A cloud elasticity-driven approach},
journal = {Parallel Computing},
volume = {108},
pages = {102858},
year = {2021},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102858},
url = {https://www.sciencedirect.com/science/article/pii/S0167819121001010},
author = {Igor {Fontana de Nardin} and Rodrigo {da Rosa Righi} and Thiago Roberto {Lima Lopes} and Cristiano {André da Costa} and Heon Young Yeom and Harald Köstler},
keywords = {Elasticity, Energy, Performance, Cloud computing, Microservices},
abstract = {Monolithic applications are a subject that includes several knowledge areas. Sometimes it can be a challenge to optimize CPU or IO requirements because it is not trivial to recognize the problem itself and improve it. There are many approaches to resolve this situation, where a trending one is the microservices. As a variant of the service-oriented architecture, microservices is a technique that arranges an application as a collection of loosely coupled services. This decomposition enables better software management in cloud-based environments since we can replicate each part individually using cloud elasticity to avoid execution bottlenecks. Also, since elasticity mitigates resource overprovisioning, it favors better energy consumption: the cloud owner can redistribute finite available resources among different tenants, and users can pay less to use the infrastructure. However, elasticity tuning is not trivial and depends on several factors, such as user experience, application architecture, and parameter modeling. Today, we observe a lack of initiatives in the literature that address both performance and energy perspectives to support the execution of microservices applications in the cloud. Concerning this context, this article introduces Elergy as a lightweight proactive elasticity model that provides resource reorganization for a cloud-based microservices application. Its differential approach appears in improving energy consumption by periodically handling the most appropriate amount of resources to execute an application while maintaining or yet improving the performance of CPU-bound applications. Elergy performs these functions proactively, in such a way of preventing future problems related to either resource under- or overprovisioning. The results showed energy consumption reduction and a competitive cost (application time x consumed resources) when comparing Elergy with a non-elastic scenario. Elergy obtained savings from 1.93% to 27.92% for energy consumption.}
}
@article{ASGARI2021531,
title = {Hybrid surrogate model for online temperature and pressure predictions in data centers},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {531-547},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19329036},
author = {Sahar Asgari and Hosein Moazamigoodarzi and Peiying Jennifer Tsai and Souvik Pal and Rong Zheng and Ghada Badawy and Ishwar K. Puri},
keywords = {Data center, Data-driven models, Row-based cooling architecture, Temperature prediction, ANN, SVR},
abstract = {The increase in cloud computing and big data storage has led to significant growth in data center (DC) infrastructure that is now estimated to consume more than 1.5% of the world’s electricity. Due to suboptimal DC design and operation, a significant fraction of this energy is wasted because of the cooling systems inability to effectively distribute cold air to servers. Consequently, additional cooling air must be circulated inside a DC to prevent local hot spots, which leads to undercooling at other locations. Row-based cooling is an emerging architecture that provides more effective airflow distribution, which lowers energy consumption. Since available methods are unsuitable for accurate online predictions, a general thermal model is required to predict spatiotemporal temperature changes inside a DC and hence optimize airflow distribution for this architecture. Typical approaches include physical models, computational fluid dynamics (CFD) simulations, and black-box data-driven models (DDMs). All three approaches are limited because they do not encapsulate the entirety of relevant operational parameters, are time-consuming and can provide unacceptable errors during extrapolative predictions. We address these deficiencies by developing a fast, adaptive, and accurate hybrid surrogate model by combining a DDM and the thermofluid transport relations to predict temperatures in a DC. Training data for the DDM is obtained from CFD simulations. An artificial neural network (ANN) with the Rectified Linear Unit (ReLU) activation function is shown to predict pressure distributions accurately in a row-based cooling DC. These predicted pressures are inputs for thermofluid transport equations to determine the temperature distribution. The applicability of the model is demonstrated by comparing predictions with experimental measurements that characterize the influence of varying server workload distribution and cooling unit operational conditions, i.e., temperature set-point, airflow rate, and fan locations, on the temperature distribution. The model can be used to (1) improve cooling configuration design, (2) facilitate thermally aware workload management, and (3) test “what if” scenarios to characterize the influence of operating conditions on the temperature distribution.}
}
@article{THAKUR2021107750,
title = {Emerging architecture for heterogeneous smart cyber-physical systems for industry 5.0},
journal = {Computers & Industrial Engineering},
volume = {162},
pages = {107750},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107750},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221006549},
author = {Payal Thakur and Vivek {Kumar Sehgal}},
keywords = {Cyber-physical systems, Design automation, System synchronization, DVFS control},
abstract = {Industrial process automation is becoming more advance due to the new technological revolutions like Industrial Internet of things (IIoT), Machine to Machine Communication (M2M), cloud computing, cognitive computing, artificial intelligence, and big little multicore-based ARM processors for embedded applications. Modern IP-enabled sensors, actuators, and controllers are transforming industrial automation into industry 5.0 compliances to attain full autonomy with minimal human intervention. With these emerging technologies, Smart Cyber-Physical System (SCPS) is the most important part of the fourth industrial revolution, where diffident programmed embedded systems are networked together to perform, computation, communication, control, and actuation. In this paper, we proposed a heterogeneous architecture for SCPS, where different electrical, pneumatic, and hydraulic processes can be integrated to execute hybrid process dynamics. The proposed architecture is enabled to separate all aspects like computation, control, communication, and actuation, of a process dynamic by estimating the process disturbances, sensor delay, actuator delay, and conversion delay. The allotment of computational embedded cores to different physical processes is done through voltage frequency islands (VFI) with high modularity which is different for different processes. All the mapped process dynamics are optimized through Dynamic Voltage and Frequency Scaling (DVFS). The best implementation of the proposed architecture is the upcoming era of industry 5.0 where human intervention is also in a fold in various industries like petroleum, fertilizer, paper, cement, space exploration, and automobile manufacturing.}
}
@article{LUO2022103399,
title = {Optimizing multicast flows in high-bandwidth reconfigurable datacenter networks},
journal = {Journal of Network and Computer Applications},
volume = {203},
pages = {103399},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103399},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000583},
author = {Long Luo and Klaus-Tycho Foerster and Stefan Schmid and Hongfang Yu},
keywords = {Computer networks, Multicast communication, Reconfigurable architectures, Scheduling algorithms},
abstract = {Modern cloud applications has led to a huge increase in multicast flows, which is becoming one of the primary communication patterns in nowadays datacenter networks. Emerging datacenter technologies enable interesting new opportunities to support such multicast traffic more effectively and flexibly in the physical layer: novel circuit switches offer high-bandwidth and reconfigurable inter-rack multicasting capabilities. However, not much is known today about the algorithmic challenges introduced by this new technology, especially in optimizing the completion times for multicast flows. This paper presents SplitCast, a preemptive multicast scheduling approach that fully exploits emerging high-bandwidth physical-layer multicasting capabilities to reduce flow times. SplitCast dynamically reconfigures the circuit switches to adapt to the multicast traffic, accounting for reconfiguration delays. In particular, SplitCast relies on simple single-hop routing and leverages transfer flexibilities by supporting splittable multicast so that a transfer can already be delivered to just a subset of receivers when the circuit capacity is insufficient. Moreover, SplitCast supports two common forwarding models, the all-stop and the not-all-stop, during circuit reconfiguration. We conduct extensive simulation to evaluate the performance of SplitCast, and the results show that SplitCast can cut down flow times significantly compared to state-of-the-art solutions.}
}
@article{TIAN2024100825,
title = {An identity authentication and key agreement protocol for the internet of vehicles based on trusted cloud-edge-terminal architecture},
journal = {Vehicular Communications},
volume = {49},
pages = {100825},
year = {2024},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2024.100825},
url = {https://www.sciencedirect.com/science/article/pii/S2214209624001001},
author = {Jun-feng Tian and Rui Ni},
keywords = {Authentication protocol, EDGE computing, Telematics, Time tree, Security proof},
abstract = {The continuous progression in cloud computing, edge computing, and associated technologies has notably hastened the progress of vehicle networking technology. This advancement is increasingly assuming a crucial role in enhancing driving safety, optimizing traffic management, and revolutionizing traffic control methodologies. The principal aim of Internet of Vehicles (IoV) technology is to establish a secure, convenient, and efficient novel driving paradigm, enabling intelligent transportation through wireless communication connecting roadside units and vehicles. Nevertheless, this wireless communication method is susceptible to potential attacks, including remote control, information monitoring, and identity simulation. Given this situation, effective authentication is required to address this security concern. Thus, this study proposes an identity authentication and key negotiation protocol grounded in a trusted cloud-edge-terminal architecture. This protocol facilitates mutual authentication, generates secure session keys for communication, guarantees the security of vehicle communication, and supports functionalities including privacy protection and password alteration for vehicle users. Time tree technology is employed for managing the edge nodes, facilitating the sharing of vehicle certification information among these nodes, and enhancing certification efficiency. Formal security analysis and informal security analysis are conducted to demonstrate the security of the proposed protocol, evaluating its security and practicality. Theoretical comparisons and experimental results demonstrate the outstanding computational and communication performance of the proposed protocol.}
}
@article{WEI2023110216,
title = {Multi-objective evolving long–short term memory networks with attention for network intrusion detection},
journal = {Applied Soft Computing},
volume = {139},
pages = {110216},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110216},
url = {https://www.sciencedirect.com/science/article/pii/S156849462300234X},
author = {Wenhong Wei and Yi Chen and Qiuzhen Lin and Junkai Ji and Ka-Chun Wong and Jianqiang Li},
keywords = {Intrusion detection system, Neural architecture search, Multi-objective evolutionary algorithm},
abstract = {Cyber security has received increasing attention, as people use more Internet applications in their lives and worry about the security of their personal data on the Internet. Intrusion Detection Systems (IDSs) are critical security tools that can detect and respond to intrusions. In recent years, Deep Learning (DL) techniques have gained popularity in IDS design due to their promising performance in terms of detection accuracy. However, the design of DL architectures usually requires professional knowledge and significantly impacts the performance of the DL model. Furthermore, the existence of a small ratio of abnormal traffic in vast network traffic leads to a serious imbalanced data problem, which negatively affects the performance of the DL model in detecting minority attack classes. To alleviate these problems, this paper proposes a multi-objective evolutionary DL model (called EvoBMF) to detect network intrusion behaviors. The model incorporates bidirectional Long–short Term Memory (BiLSTM) for preliminary feature extraction, Multi-Head Attention (MHA) for further capturing features and global information of the network traffic, and Full-Connected Layer (FCL) module to perform final classification. To deal with the challenge of manually tuning the parameters of the DL model when tackling different tasks, the parameters of the EvoBMF model are first encoded as the chromosome of the Multi-objective Evolutionary Algorithm (MOEA), which aims to optimize the two conflicting objectives (complexity and classification ability) of the model. A state-of-the-art MOEA (MOEA/D-DRA) is then used to optimize the above two objectives, aiming to obtain the optimal architecture for EvoBMF, which can be easily deployed in cloud computing scenarios to detect and respond to network intrusions. Additionally, to alleviate the severe imbalance in routine network traffic, the synthetic minority over-sampling technique is introduced to generate representative samples of minority classes to improve the overall performance of the model. At last, the experimental results conducted on two popular datasets (UNSW-NB15 and CIC-IDS 2018) have demonstrated that the proposed EvoBMF model can provide superior performance for intrusion detection when compared to some state-of-the-art IDSs.}
}
@article{YUAN2024102616,
title = {Toward dynamic rehabilitation management: A novel smart product-service system development approach based on fine-tuned large vision model and Fuzzy-Dematel},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102616},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102616},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002647},
author = {Wenyu Yuan and Hua Zhao and Xiongjie Yang and Ting Han and Danni Chang},
keywords = {Smart PSS, Fine-tuned large model, Personalized service, Fuzzy DEMATEL, Rehabilitation management},
abstract = {Nowadays, transformative technologies such as artificial intelligence, big data, and cloud computing are significantly influencing and reshaping the daily lives of individuals. Guided by the overarching concept of digital transformation, data-driven Smart Product-Service Systems (SPSS) have emerged, prompting scholars to investigate development approaches tailored to diverse data sources. However, the current approaches employed in the construction of SPSS exhibit limited capability in processing vast amounts of user-generated unstructured data. The relationship between big data intelligence and personalized services remains undisclosed. Moreover, the current focus of SPSS orientation predominantly addresses end consumers or manufacturers, with inadequate attention given to dynamic collaborative models that involve multiple stakeholders. These gaps are particularly conspicuous in complex industries such as rehabilitation management. To tackle these challenges, this study introduces a novel SPSS development approach that integrates a large vision model and the fuzzy-DEMATEL method. Specifically, a data-driven predictive assessment module was proposed, which constructs a medical image dataset and trains a rehabilitation predictive assessment model based on the transformer architecture. Secondly, personalized intervention services were generated, involving the representation of system elements, configuration, and optimization of service parameters. The fuzzy-DEMATEL method is mainly used for the initialization of service parameters. Then, interactive feedback is integrated into rehabilitation exercises for achieving continuous rehabilitation evaluation and service improvement. To validate the proposed approach, a FPRM-SPSS case was implemented, and it shows that the predictive assessment model achieved a high level of accuracy when applied to the clinical dataset constructed in this study, and the system was evaluated with high scores in user satisfaction.}
}
@article{MENAKA2024192,
title = {Supportive particle swarm optimization with time-conscious scheduling (SPSO-TCS) algorithm in cloud computing for optimized load balancing},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {5},
pages = {192-198},
year = {2024},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2024.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666307424000160},
author = {M. Menaka and K.S. {Sendhil Kumar}},
keywords = {Cloud computing, Load balancing, SPSO-TCS, Makespan time, Virtual machine},
abstract = {Task scheduling for virtual machines (VMs) has shown to be essential for the effective development of cloud computing at the lowest cost and fastest turnaround time. A number of research gaps about job schedule optimization are included in the current paper. A thorough analysis of the data generated by this activity is essential to resolving the resource allocation mechanism of the cloud architecture. To fully utilize virtual machines with a similar weight distribution, a strategy-oriented mixed support and load balancing structure has been developed in this work. To minimize make-span time and accomplish initial load balancing, the SPSO-TCS technique combines Time-Conscious Scheduling with Supportive Particle Swarm Optimization. Finding the optimal make span time minimization for each virtual environment is the aim of this stage. Its main objective is to discover the sequence of activities with the least computation time and to reduce the time required to finish each operation. Utilizing the hybrid idea leads to a decrease in makespan and the use of the least amount of energy.}
}
@article{LI2022100806,
title = {A new fuzzy-based method for energy-aware resource allocation in vehicular cloud computing using a nature-inspired algorithm},
journal = {Sustainable Computing: Informatics and Systems},
volume = {36},
pages = {100806},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100806},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922001378},
author = {Can Li and Xiaode Zuo and Amin Salih Mohammed},
keywords = {Resource allocation, Vehicular, Cloud computing, Fuzzy, Algorithm},
abstract = {Vehicular cloud computing is a hopeful solution to utilize underused vehicle resources such as processing energy, storage space, Internet connection, etc. These resources can be shared among vehicles or rented by landlords for multiple purposes, such as meeting the hardware needs of automotive network services and applications. It is possible to meet the growing need for resources in the automotive network. Although this plan seems possible, its implementation has some problems. Several scholars have concentrated on architectural design to solve various difficulties and provide users with trustworthy service. This paper presents a fuzzy-based method to allocate resources in vehicular cloud computing using a nature-inspired (cuckoo search algorithm). The suggested algorithm is compared to some state-of-the-art algorithms. The outcomes illustrated that the recommended method outperforms other algorithms in terms of execution time, delay, and makespan.}
}
@article{LI202219,
title = {Strategy for dynamic blockchain construction and transmission in novel edge computing networks},
journal = {Future Generation Computer Systems},
volume = {130},
pages = {19-32},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004866},
author = {Guozhi Li and Yifan Dong and Jirui Li and Xuekun Song},
keywords = {Dynamic blockchain, Edge bandwidth and storage optimization (EBSO) algorithm, Edge blockchain generation (EBG) algorithm, Edge computing networks, Tradeoff parameter },
abstract = {The distributed transmission of a dynamic blockchain in a cloud computing platform may cause network overload. To overcome this challenge, a novel idea for a network architecture that is suitable for blockchain transmission and storage is proposed. Emerging edge computing technology has been proven to improve the efficiency of blockchain construction and operation. In this paper, we focus on solving the problem of unbalanced resource allocation for blockchain construction and transmission in a novel edge computing network architecture. We propose the edge bandwidth and storage optimization (EBSO) algorithm to balance effective bandwidth allocation and storage space selection during dynamic blockchain processing. In addition, we build a dynamic blockchain based on a link bandwidth allocation analysis in an edge computing network environment. Subsequently, the reasonable value range of the tradeoff parameter γ between the link bandwidth and storage space during the transmission of the constructed dynamic blockchain is determined. Finally, we use version 2 of the network simulator (ns-2) to simulate the performance of the EBSO algorithm and compare it with other excellent algorithms in terms of transmission bandwidth efficiency (TBE), storage space efficiency (SSE), blockchain construction efficiency (BCE), average throughput, and average delay in the same network environment.}
}
@article{DENG2024,
title = {Leveraging public cloud infrastructure for real-time connected vehicle speed advisory at a signalized corridor},
journal = {International Journal of Transportation Science and Technology},
year = {2024},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2024.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S2046043024000352},
author = {Hsien-Wen Deng and M Sabbir Salek and Mizanur Rahman and Mashrur Chowdhury and Mitch Shue and Amy W. Apon},
keywords = {Public cloud, Cyber-physical system, Connected vehicle, Roadway traffic management, Amazon Web Services},
abstract = {In this study, we developed a real-time connected vehicle (CV) speed advisory application that uses public cloud services and tested it on a simulated signalized corridor for different roadway traffic conditions. First, we developed a scalable serverless cloud computing architecture leveraging public cloud services offered by Amazon Web Services (AWS) to support the requirements of a real-time CV application. Second, we developed an optimization-based real-time CV speed advisory algorithm by taking a modular design approach, which makes the application automatically scalable and deployable in the cloud using the serverless architecture. Third, we developed a cloud-in-the-loop simulation testbed using AWS and an open-source microscopic roadway traffic simulator called Simulation of Urban Mobility (SUMO). Our analyses based on different roadway traffic conditions showed that the serverless CV speed advisory application meets the latency requirement of real-time CV mobility applications. Besides, our serverless CV speed advisory application reduced the average stopped delay (by 77%) and the aggregated risk of collision (by 21%) at the signalized intersections of a corridor. These prove the feasibility as well as the efficacy of utilizing public cloud infrastructure to implement real-time roadway traffic management applications in a CV environment.}
}
@article{DASAPPA2024114235,
title = {Multi-sensor data fusion framework for energy optimization in smart homes},
journal = {Renewable and Sustainable Energy Reviews},
volume = {193},
pages = {114235},
year = {2024},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2023.114235},
url = {https://www.sciencedirect.com/science/article/pii/S1364032123010936},
author = {Nirupam Sannagowdara Dasappa and Krishna {Kumar G} and Nivethitha Somu},
keywords = {Energy optimization, Data fusion, Micro-moments, Smart spaces, Smart home, Recommendations},
abstract = {Advancements in Internet of Energy (IoE) technologies drive the development of several energy efficient frameworks for better energy optimization, economic savings, safety, and security in smart homes. However, certain challenges such as real-time operational data for each micro-moment, proper application of data fusion techniques, and end-to-end computing and deployment architecture prevent the establishment of an effective energy-efficient framework to provide personalized energy-saving recommendations. This work presents energy management for smart spaces (EMSS), the proposed energy efficiency framework implemented in an edge-cloud computing platform that fuses data from heterogeneous data sources (environmental sensors, camera, plug data, etc.) at appropriate data fusion levels and process them to generate actionable, explainable, personalized, and persuasive recommendations at the right moment. The user response to the generated recommendations triggers the actuators to perform respective energy-saving actions and provide more personalized future recommendations. Further, SMARTHome - a data generation framework based on configurable scenario files and a set of software codes was proposed to generate synthetic data with respect to different building types and micro-moments. The functionalities of the EMSS (device and user registration), user dashboard, analytics, and energy-saving recommendations were made accessible to the user through web and mobile applications. The validation analysis of the EMSS was performed by (i) comparative analysis of the machine learning and deep learning algorithms used by the decision engine to generate energy-saving recommendations and (ii) benchmarking of EMSS based on the taxonomy of data fusion-based energy efficiency frameworks for smart homes.}
}
@article{ABDULJABARABDULLA2021125070,
title = {Strength models for uPVC-confined concrete},
journal = {Construction and Building Materials},
volume = {310},
pages = {125070},
year = {2021},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2021.125070},
url = {https://www.sciencedirect.com/science/article/pii/S0950061821028154},
author = {Nwzad {Abduljabar Abdulla}},
keywords = {uPVC confined-concrete, RHA-cement, Plastic tube thickness, Strength models, Experimental database, Repeated loading},
abstract = {In the present study, an experimental program was executed to investigate the performance of a composite structural system under direct compression load. The system consists of a uPVC tube as the mold for pouring the fresh concrete and a confining device for structural concrete. Three thicknesses of plastic tubes (4, 5, 7 mm) were investigated using four sets of three stub columns. Rice husk ash (RHA) was used as a cement replacement material to improve the strength and other mechanical properties of concrete core with target compressive strengths of 30 MPa for sets one and two and 60 MPa for sets three and four, respectively. The load capacity of uPVC strengthened blended concrete improved, and the ultimate failure is ductile due to the elongation capacity of the polymeric tube. To examine the post-peak behavior of the confining device at near-collapse conditions, two stub columns were subjected to five repeated loading that almost produced full collapse conditions in each cycle of loading after the first cycle. The tubes effectively dissipated energy by plastic yielding, which could be considered a partial progressive failure. Several analytical expressions have been proposed, in total twenty-four, for predicting the strength capacity of the composite system. However, most of these empirical expressions have been derived based on limited experimental tests. Therefore, to assess the performance of the existing strength models in predicting the capacity of PVC-confined concrete (PCC) and uPVC-confined concrete (uPCC); a comprehensive database of 389 test results was assembled from the published literature that covers the studies published in the period 1979–2021 combined with the twelve data from the present study. To account for the height/diameter ratio (H/D), three cases were examined; case one (3 ≤ H/D >3), case two (H/D ≤ 3), and case three (H/D >3). Based on the compiled database, two strength-based confinement models were proposed for circular concrete specimens confined externally with PVC or uPVC tube. The two models include several parameters that influence the properties of the concrete core and the confining device. The proposed models are the most accurate among all the models yielding lower values of absolute average error for the three cases examined.}
}
@article{B2021103716,
title = {Optimized mobile cloud resource discovery architecture based on dynamic cognitive and intelligent technique},
journal = {Microprocessors and Microsystems},
volume = {81},
pages = {103716},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103716},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120308619},
author = {Thanikaivel B and Venkatalakshmi K and Kannan A},
keywords = {Mobile cloud computing, Cloudlet, Resource discovery optimization, Prediction, Reliability, Resource Availability, Information Management},
abstract = {In current world, the Mobile Cloud Computing (MCC) will undergo a dramatic upgrade with integrating of mobile cloud resources in small regions to provide a better MCC service. This paper considers the MCC resource availability problem in integrated mobile cloud environment to optimization resources availability with a better effectiveness, accuracy, reliable, low latency and low complexity. The DCICRD architecture is proposed in this paper optimizes the resource discover process with better resource availability. The proposed DCICRD architecture runs various operations such Resource demand prediction to find the required level of resources, Cloudlet Resource Discovery process which discovers resources based on the requirement predicted using two states expand and shrink. The expand state is used to discover resources on-demand in resource scarcity situation and the shrink state is called when abundant resource available to the required level.  Further, Resource reliability check is performed to identify the reliable resource with energy level and signal strength above the threshold level. Finally, Local Resource Information Management Table is used to store the local resource provider information and Central Resource Information Management Table is used to store all local resource information for handover of resource provider. The implementation and evaluation is conducted by comparing the HARD architecture. The comparison result shows that proposed DCICRD architecture is better than HARD architecture by optimizing the resource discovery process and produce reliable resources on demand as per the required level with a better effectiveness using expand and shrink state, accuracy by making resource available for required level, reliable by sending message for frequent checking of Resource with required energy level and signal strength, low latency by making resource available locally and reduce complexity by performing only on-demand resource discovery operation after initial startup. Thus, the proposed DCICRD architecture is better when compared to HARD architecture.}
}
@article{LIU2023102683,
title = {Real-time traffic impedance and priority based cooperative path planning mechanism for SOC-ITS: Efficiency and equilibrium},
journal = {Simulation Modelling Practice and Theory},
volume = {122},
pages = {102683},
year = {2023},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2022.102683},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X22001526},
author = {Yuxi Liu and Kailong Zhang and Boyuan Hou and Qiang Li and Jingkai Feng and Thi-Mai-Trang Nguyen and Arnaud {de La Fortelle}},
keywords = {SoC-ITS, Traffic impedance, Path planning, Quality of transportation service, Traffic equilibrium},
abstract = {With the support of new information technologies such as the Internet of Vehicles (IoV) and cloud computing, the Intelligent Transportation System (ITS) has begun a new stage role as Cooperative ITS (C-ITS). Moreover, featuring the collaboration of vehicles, roads, clouds and humans, it becomes possible to scientifically mitigate several serious phenomena in the traditional ITS, including traffic congestion, blocked emergency vehicles, etc. While focusing on optimizing the Quality of Transportation Service (QoTS) requirements from vehicles with different types of tasks, covering emergency vehicles (fire engine, police vehicle and ambulance), shuttle buses, and others, this paper studies and proposes a new service-oriented and cloud-based cooperative path planning mechanism for such heterogeneous vehicles. First, the architecture of service-oriented cooperative path planning is established, where the types and QoTS of traffic tasks are illustrated. Then, a hybrid traffic impedance model is put forward, in which three influencing factors of traffic impedance are concluded and combined through the entropy weight method. On this basis, a service-oriented cooperative path planning mechanism with load balancing strategies named SoCPPLB is designed. The main principle of SoCPPLB is to provide different levels of path planning service for vehicles according to their QoTS requirements per the constraint of the QoTS of vehicles and traffic equilibrium. Finally, all these proposed models and algorithms have been implemented within the Simulation of Urban MObility (SUMO), and further verified under several typical traffic scenarios. The experimental results show that, this new mechanism can effectively improve the QoTS of emergency vehicles and traffic equilibrium performance obviously.}
}
@article{YANG2021100088,
title = {Implementation for a cloud battery management system based on the CHAIN framework},
journal = {Energy and AI},
volume = {5},
pages = {100088},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000422},
author = {Shichun Yang and Zhengjie Zhang and Rui Cao and Mingyue Wang and Hanchao Cheng and Lisheng Zhang and Yinan Jiang and Yonglin Li and Binbin Chen and Heping Ling and Yubo Lian and Billy Wu and Xinhua Liu},
keywords = {Battery, CHAIN, Cloud, Battery management system, SOX estimation, end-edge-cloud architecture},
abstract = {Summary
An intelligent battery management system is a crucial enabler for energy storage systems with high power output, increased safety and long lifetimes. With recent developments in cloud computing and the proliferation of big data, machine learning approaches have begun to deliver invaluable insights, which drives adaptive control of battery management systems (BMS) with improved performance. In this paper, a general framework utilizing an end-edge-cloud architecture for a cloud-based BMS is proposed, with the composition and function of each link described. Cloud-based BMS leverages from the Cyber Hierarchy and Interactional Network (CHAIN) framework to provide multi-scale insights, more advanced and efficient algorithms can be used to realize the state-of-X estimation, thermal management, cell balancing, fault diagnosis and other functions of traditional BMS system. The battery intelligent monitoring and management platform can visually present battery performance, store working-data to help in-depth understanding of the microscopic evolutionary law, and provide support for the development of control strategies. Currently, the cloud-based BMS requires more effects on the multi-scale integrated modeling methods and remote upgrading capability of the controller, these two aspects are very important for the precise management and online upgrade of the system. The utility of this approach is highlighted not only for automotive applications, but for any battery energy storage system, providing a holistic framework for future intelligent and connected battery management.}
}
@article{YURONG2024101085,
title = {Research on the remote monitoring system for falls in the elderly based on the internet of things and six axis acceleration sensors},
journal = {Measurement: Sensors},
volume = {32},
pages = {101085},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101085},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424000618},
author = {Niu Yurong},
keywords = {Internet of things, Six axis acceleration, Sensors, Remote monitoring system},
abstract = {With the integration and collision of the Internet of Things, machine learning, Big data computing and other technologies, related applications of the Internet of Things also need to process a lot of real-time data streams. This article focuses on the research of remote monitoring of falls using the Internet of Things and six axis acceleration sensors, and explores the efficient application of Internet of Things technology in the system. The hardware design and related software development of remote monitoring of falls for the elderly are the key parts, and the overall framework, main modules, and specific implementation of the system are elaborated in detail. A complete remote monitoring system is designed by selecting a suitable six-axis acceleration sensor, collecting and analyzing the data. The continuous development of the Internet of Things and six axis acceleration sensor technology can provide real-time intelligent remote monitoring. Compared to cloud computing platforms, edge clusters have limited computing and storage resources and diverse types of computing node architectures. Therefore, it is necessary to use lightweight application service deployment methods to build an efficient and autonomous data processing platform. Through research and innovation on the remote monitoring system for elderly falls, with optimized and comprehensive technology and detailed research support, the overall system design was experimentally debugged and the experimental plan was ultimately determined. Through data communication module, fall detection and diagnosis module, and database management module, rapid analysis of remote acceleration data and information exchange are achieved, thereby minimizing the possibility of accidents caused by falls in the elderly.}
}
@article{SABYASACHI20242651,
title = {Deep CNN and LSTM Approaches for Efficient Workload Prediction in Cloud Environment},
journal = {Procedia Computer Science},
volume = {235},
pages = {2651-2661},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.250},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924009268},
author = {Abadhan Saumya Sabyasachi and Biswa Mohan Sahoo and Abadhan Ranganath},
keywords = {Deep CNN, LSTM, SLA, Cloud Computing, Time Series Forecasting},
abstract = {In the dynamic landscape of cloud computing, efficient resource allocation and workload prediction are paramount for optimizing infrastructure utilization, cost management, and overall service quality. As a result, an efficient resource management approach involves pooling available resources among many customers in a manner that considers their energy usage, SLAs, and forecasting accuracy all at once. Based on the performance analysis results, it is determined that integrating these techniques would result in the most efficient and adaptable cloud data centre. We proposed a Model based on Deep Convolutional Neural Networks (DCNN) and Long Short-Term Memory (LSTM) for handling SLAs in the cloud from both the perspective of consumers and service providers. Subsequently, we delve into the methodology of applying deep CNN and LSTM models to the problem of workload prediction in cloud environments. This methodology encompasses data preprocessing, model architecture, training parameters, and the choice of performance metrics. To anticipate CPU consumption from time series data and detect SLA violations, we suggested a DCNN-LSTM model. The accuracy prediction, energy usage, CPU use, and Service Level Agreement monitoring are all a part of this model. The proposed method is effective in helping cloud providers cut down on service violations and associated fines. Regarding the composite metric of Energy SLA Violation, which assesses the combined aspects of energy use and adherence to Service Level Agreements (SLAs), DCNN-LSTM surpasses ARIMA-LSTM, CNN, LSTM, and ARIMA 6.8%, 10.88%, 16.6%, and 22.4%, respectively.}
}
@article{DONG2023101632,
title = {A dynamic distributed edge-cloud manufacturing with improved ADMM algorithms for mass personalization production},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101632},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101632},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001866},
author = {Chen Dong and JiHai Luo and Qiyu Hong and Zhenyi Chen and Yuzhong Chen},
keywords = {Industry 4.0, Mass personalization production, Intelligent manufacturing, Cloud computing, Edge computing, Convex optimization},
abstract = {The primary feature of Industry 4.0 is MPP (mass personalization production), which requires that consumers’ individual requests are met in large-scale production. Under MPP, there is a multitude of subtasks decomposed from production tasks that are derived from individualized requests, and allocating these subtasks properly brings high economic benefits. However, existing approaches to achieve MPP, such as cloud manufacturing and social manufacturing, generally can not provide customers with deep participation in the entire production cycle, or respond to consumers’ modification needs by a triggered mechanism. Besides, some methods are of centralized architecture, which is vulnerable to single point error and with large cloud load that is not conducive to quickly responding to consumers’ dynamic demand changes. Therefore, this paper proposes a dynamic edge-cloud manufacturing mode for MPP, which can make subtask allocation with high economic benefit through distributed computing and implementing modifications of alternating direction method of multiplier (ADMM) algorithm. Also, it proposes an original improved ADMM algorithm, named Relaxation-Based ADMM algorithm, to increase the optimization speed in large-scale cases. The experimental results show that the proposed method generally obtains a superior solution under a certain iteration count.}
}
@article{YIN2022111123,
title = {A stochastic algorithm for scheduling bag-of-tasks applications on hybrid clouds under task duration variations},
journal = {Journal of Systems and Software},
volume = {184},
pages = {111123},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111123},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100220X},
author = {Lu Yin and Junlong Zhou and Jin Sun},
keywords = {Bag-of-tasks applications, Hybrid clouds, Stochastic task scheduling, Probabilistic constraint, Profit maximization},
abstract = {Hybrid cloud computing, which typically involves a hybrid architecture of public and private clouds, is an ideal platform for executing bag-of-tasks (BoT) applications. Most existing BoT scheduling algorithms ignore the uncertainty of task execution times in practical scenarios and schedule tasks by assuming that the task durations can be determined accurately in advance, often leading to the violation of the deadline constraint. In view of this fact, this paper devotes to maximizing the profit of the private cloud provider while guaranteeing the quality-of-service provided by the cloud platform, through designing an effective stochastic BoT scheduling algorithm based on the distribution of task duration variations. With the varying task execution times modeled as random variables, we formulate a stochastic scheduling framework that incorporates a probabilistic constraint upon the makespans of BoT applications. The resultant stochastic optimization model is capable of characterizing the complete distribution information of makespan variations and satisfying the deadline constraint in a probabilistic sense. We further design an immune algorithm-based metaheuristic to solve this stochastic optimization problem. Simulations results justify that our proposed algorithm outperforms several competing algorithms in maximizing the cloud provider’s profit while satisfying the user-specified deadline constraint under the impact of uncertain task durations.}
}
@article{ZHOU2021103342,
title = {Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation},
journal = {Journal of Energy Storage},
volume = {44},
pages = {103342},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103342},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21010331},
author = {Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng},
keywords = {Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction},
abstract = {This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.}
}
@article{GRZEGOROWSKI2021100203,
title = {Cost Optimization for Big Data Workloads Based on Dynamic Scheduling and Cluster-Size Tuning},
journal = {Big Data Research},
volume = {25},
pages = {100203},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100203},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000204},
author = {Marek Grzegorowski and Eftim Zdravevski and Andrzej Janusz and Petre Lameski and Cas Apanowicz and Dominik Ślęzak},
keywords = {Big Data, ETL, Cloud computing, Spot price prediction, ARIMA, Spark},
abstract = {Analytical data processing has become the cornerstone of today's businesses success, and it is facilitated by Big Data platforms that offer virtually limitless scalability. However, minimizing the total cost of ownership (TCO) for the infrastructure can be challenging. We propose a novel method to build resilient clusters on cloud resources that are fine-tuned to the particular data processing task. The presented architecture follows the infrastructure-as-a-code paradigm so that the cluster can be dynamically configured and managed. It first identifies the optimal cluster size to perform a job in the required time. Then, by analyzing spot instance price history and using ARIMA models, it optimizes the schedule of the job execution to leverage the discounted prices of the cloud spot market. In particular, we evaluated savings opportunities when using Amazon EC2 spot instances comparing to on-demand resources. The performed experiments confirmed that the prediction module significantly improved the cost-effectiveness of the solution – up to 80% savings compared to the on-demand prices, and at the worst-case, 1% more cost than the absolute minimum. The production deployments of the architecture show that it is invaluable for minimizing the total cost of ownership of analytical data processing solutions.}
}
@article{PALANI2024100988,
title = {A secured energy aware resource allocation and task scheduling based on improved cuckoo search algorithm and deep reinforcement learning for e-healthcare applications},
journal = {Measurement: Sensors},
volume = {31},
pages = {100988},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2023.100988},
url = {https://www.sciencedirect.com/science/article/pii/S2665917423003240},
author = {S. Palani and K. Rameshbabu},
keywords = {Cloud computing, Resource allocation, Task scheduling, Improved cuckoo search algorithm (ICSA), Deep reinforcement learning (DRL), e-health cloud, Healthcare services, Security, Hybrid optimization with improved blowfish encryption algorithm},
abstract = {Healthcare industries have begun using modern technologies and tools like cloud computing these days. When cloud computing and assisted technologies are used in the healthcare domain the storage cost improves along with the quality and time availability. The present work makes use of cryptography based architecture for improving the overall security aspects. However, the tasks of the user take a longer duration for the execution to complete when the needed resources are unavailable on the main server. ICSA (Improved Cuckoo Search Algorithm) with DRL (Deep Reinforcement Learning) to solve this issue. The main goal of this work is to increase overall security and reduce execution times of jobs. The objective also includes using underutilized resources. Scheduling jobs, Binary In-order Traversal Trees with weights are used. Subsequently, DRL algorithm has been used to decrease the space complexity by splitting the individual resources. There will be idle resources in a state space that are used in allocation of tasks. The scheduled tasks will then do a search on the resources based on the ICSA algorithm. The server will then distribute the resources to the action area after an optimal resource has been chosen and assigned to the job. DRL for resource allocation so that the inactive resource usage is minimized and the execution time is reduced. Finally, MTAC-IBEA (multi tenant authentication control with improved blowfish encryption algorithm) for decrypt health data. The outcomes of simulation demonstrate that the suggested method offers optimum work schedules, resource allocations, and security in e-healthcare systems.}
}
@incollection{MARINESCU2023135,
title = {Chapter 5 - Cloud resource virtualization},
editor = {Dan C. Marinescu},
booktitle = {Cloud Computing (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {135-173},
year = {2023},
isbn = {978-0-323-85277-7},
doi = {https://doi.org/10.1016/B978-0-32-385277-7.00012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323852777000129},
author = {Dan C. Marinescu},
keywords = {hypervisor, hypercalls, virtual machine, Virtual Machine Control Structure (VMCS), address space compression, interrupt virtualization, Virtual-Machine Based Rootkit (VMBR), zero-copy semantics},
abstract = {Virtualization, a basic tenet of cloud computing, simplifies some of the resource management tasks. For example, the state of a virtual machine (VM) running under a hypervisor can be saved and migrated to another server to balance the load. At the same time, virtualization allows users to operate in environments they are familiar with, rather than forcing them to work in idiosyncratic environments. This chapter starts with a discussion of virtualization principles and the motivation for virtualization in Section 5.1. Section 5.2 is focused on performance and security isolation. Alternatives for the implementation of virtualization are analyzed in Section 5.3. Two distinct approaches for processor virtualization, full virtualization and paravirtualization are presented in Section 5.4. Full virtualization is feasible when the hardware abstraction provided by the hypervisor is an exact replica of the physical hardware. In this case any operating system running on the hardware will run without modifications under a hypervisor implementing full virtualization. Paravirtualization requires modifications of the guest operating systems because the hardware abstraction provided by the hypervisor is not supported by the physical hardware. Traditional processor architectures were conceived for one level of control as they support two execution modes, the kernel and the user mode. In a virtualized environment, all resources are under the control of a hypervisor, and a second level of control is exercised by the guest OS. While two-level scheduling for sharing CPU cycles can be easily implemented, sharing of resources such as cache, memory, and I/O bandwidth is more intricate. In 2005 and 2006, the x86 processor architecture was extended to provide hardware support for virtualization, as discussed in Section 5.5. Nested virtualization allows hypervisors to run inside a VM, complicating even further the virtualization landscape. Section 5.6 covers QEMU, an open-source emulator and virtualizer. Several hypervisors are used nowadays; KVM, a virtualization infrastructure of the Linux kernel, is discussed in Section 5.7, and Xen and an optimization of its network performance are presented in Sections 5.8 and 5.9. Nested virtualization is analyzed in Section 5.10, followed by the presentation of trusted kernel virtualization in Section 5.11. High-performance processors have multiple functional units but do not provide explicit support for virtualization, as discussed in Section 5.12, which covers Itanium paravirtualization. System functions critical for the performance of a VM environment are cache and memory management, handling of privileged instructions, and I/O handling. Cache misses are an important source of performance degradation in a VM environment as we shall see in Section 5.13, which compares the performance of virtual machines. An overview of open-source software platforms for virtualization is presented in Section 5.14. The potential risks of virtualization are the subject of Section 5.15, and virtualization software is discussed in Section 5.16. Further readings and exercises and problems are contained in Sections 5.17 and 5.18.}
}
@incollection{SANGULAGI20211,
title = {Chapter 1 - Optimization in the sensor cloud: Taxonomy, challenges, and survey},
editor = {Siddhartha Bhattacharyya and Paramartha Dutta and Debabrata Samanta and Anirban Mukherjee and Indrajit Pan},
booktitle = {Recent Trends in Computational Intelligence Enabled Research},
publisher = {Academic Press},
pages = {1-21},
year = {2021},
isbn = {978-0-12-822844-9},
doi = {https://doi.org/10.1016/B978-0-12-822844-9.00036-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228449000360},
author = {Prashant Sangulagi and Ashok Sutagundar},
keywords = {Sensor cloud, optimization, taxonomy, load balancing, information classification, information transmission, information processing},
abstract = {In recent years, the integration of wireless sensor networks and cloud computing has played an important role in fast and reliable computation and also communication. This integration is also called the sensor cloud. The sensor cloud systems are very specific and the use of simulations is necessary in their architecture, implementation, and operational characteristics. The sensor cloud collects information from the sensor network and via the gateway it is stored into servers for user access irrespective of the access location. There are several issues which require attention in order to optimize the sensor cloud in a more intelligent and efficient manner. The focus of this survey study is to assist researchers in this manner by outlining the challenges, survey, and taxonomy of an intelligent sensor cloud optimization for the new methodology that is still evolving. The key objectives of this research are the new insights into sensor cloud optimization, such as increasing network lifetime, which is achieved by addressing critical parameters such as load balancing, classification, processing, and also transmission of information. The survey also briefly outlines the future focus on intelligent sensor cloud optimization.}
}
@article{ALTARAWNEH2023103288,
title = {An optimal algorithm for energy harvesting in optical networks},
journal = {Optical Fiber Technology},
volume = {78},
pages = {103288},
year = {2023},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2023.103288},
url = {https://www.sciencedirect.com/science/article/pii/S1068520023000676},
author = {Luae Al-Tarawneh},
keywords = {Energy harvesting, Optical fiber, Optical power optimization, Quality-of-service},
abstract = {Optical fiber cables, which make up the majority of the world's telecommunications infrastructure, now carry the vast majority of digital data. However, because to the Internet's exponential expansion and a slew of bandwidth-hungry new services like online gaming, 3D or high-definition television, and cloud computing, this infrastructure will soon run out of capacity. The growing need for faster transmission speeds by clients has led to the development of heterogeneous optical networks. High throughput, portability, and future information interconnects may all be accommodated by such a system. Delivering such high-capacity connections to a mobile client should need heterogeneous optical networks. Different multiple access user technologies have been suggested to meet the needs of a group of future heterogeneous optical network users. In order to address the high energy consumption, poor QoS and low resource utilization in cloud radio access network, this paper proposed a novel algorithm. The main idea is to allocate virtual resources with energy-awareness and hybrid energy supply. Then, the energy arrival and consumption models were established based on different network devices. Simulation results show that the proposed algorithm effectively improved the energy consumption performance and QoS as compared with existing methods.}
}
@article{ROSERO2021117770,
title = {Cloud and machine learning experiments applied to the energy management in a microgrid cluster},
journal = {Applied Energy},
volume = {304},
pages = {117770},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117770},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921011090},
author = {D.G. Rosero and N.L. Díaz and C.L. Trujillo},
keywords = {Cloud computing, Machine learning, Energy management system, Prosumer, Microgrid clustering, Renewable energy},
abstract = {The way to organize the generation, storage, and management of renewable energy and energy consumption features has taken relevance in recent years due to demands that define the social welfare of this century. Like demand increases, other factors require grid infrastructure improvement, updates, and opening to other technologies that assuage the final customer needs. Precisely, the interest in renewable energy sources, the constant evolution of energy storage technologies, the continuous research involving microgrid management systems, and the evolution of cloud computing technologies and machine learning strategies motivate the development of this article. Tasks associated with a microgrid cluster like the integration of a considerable number of heterogeneous devices, real-time support, information processing, massive storage capabilities, security considerations, and advanced optimization techniques usage could take place in an autonomous and scalable energy management system architecture under a machine learning perspective running in real-time and using Cloud resources. This paper focuses on identifying the elements considered by different authors to define a cloud-based architecture and ensure the appropriately supervised learning functionality under a microgrids cluster environment. Namely, it was necessary to revise and run microgrid simulations, real-time simulation platforms usage, connection to a virtual server for microgrid control and set the energy management system using cloud computing and machine learning. Based on the review and considering the scenarios mentioned, this article presents a scalable and autonomous cloud-based architecture that allows power generation forecast, energy consumption prediction, a real-time energy management system using machine learning techniques.}
}
@article{WANG2024104882,
title = {Cloud-edge-end workflow scheduling with multiple privacy levels},
journal = {Journal of Parallel and Distributed Computing},
volume = {189},
pages = {104882},
year = {2024},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2024.104882},
url = {https://www.sciencedirect.com/science/article/pii/S0743731524000467},
author = {Shuang Wang and Zian Yuan and Xiaodong Zhang and Jiawen Wu and Yamin Wang},
keywords = {Cloud-edge-end computing, Workflow scheduling, Multi privacy level constraints, Rental cost optimization},
abstract = {The cloud-edge-end architecture satisfies the execution requirements of various workflow applications. However, owing to the diversity of resources, the complex hierarchical structure, and different privacy requirements for users, determining how to lease suitable cloud-edge-end resources, schedule multi-privacy-level workflow tasks, and optimize leasing costs is currently one of the key challenges in cloud computing. In this paper, we address the scheduling optimization problem of workflow applications containing tasks with multiple privacy levels. To tackle this problem, we propose a heuristic privacy-preserving workflow scheduling algorithm (PWHSA) designed to minimize rental costs which includes time parameter estimation, task sub-deadline division, scheduling sequence generation, task scheduling, and task adjustment, with candidate strategies developed for each component. These candidate strategies in each step undergo statistical calibration across a comprehensive set of workflow instances. We compare the proposed algorithm with modified classical algorithms that target similar problems. The experimental results demonstrate that the PWHSA algorithm outperforms the comparison algorithms while maintaining acceptable execution times.}
}
@article{FATIMA2022101641,
title = {Integration of multi access edge computing with unmanned aerial vehicles: Current techniques, open issues and research directions},
journal = {Physical Communication},
volume = {52},
pages = {101641},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101641},
url = {https://www.sciencedirect.com/science/article/pii/S1874490722000271},
author = {Nida Fatima and Paresh Saxena and Manik Gupta},
keywords = {Unmanned aerial vehicle, Multi-access edge computing, 5th generation wireless communication system (5G), Beyond 5G},
abstract = {During the last decade, research and development in the field of multi access edge computing (MEC) has rapidly risen to prominence. One of the factors propelling MEC’s evolution is the ability to deploy edge servers capable of providing both communication and computational services in close proximity to the mobile user terminal. MEC has been regarded as a potentially transformative technique for fifth-generation (5G) and beyond 5G (B5G) wireless communication systems, as well as a possible complement to traditional cloud computing. Additionally, unmanned aerial vehicles (UAVs) integrated with MEC will play a critical role by introducing an additional mobility based computational layer to provide more secure, efficient and faster services. UAV enabled MEC offers seamless connectivity, fulfilling the promise of 5G’s ubiquitous connectivity. Due to the enormous interest in UAV enabled MEC, there has been a tremendous increase in the number of published research articles in this domain; however, the research area still lacks a systematic study and categorization. We present a systematic literature review (SLR) on UAV enabled MEC, examining and analyzing data on the current state of the art using preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines. To streamline our assessment, this study analyzes several research papers carefully selected through a multi-stage process satisfying the eligibility criteria defined in the paper. One of the SLR’s primary contributions is to broadly classify the research in the UAV enabled MEC domain into different categories including energy efficiency, resource allocation, security, architecture, and latency. We have identified key findings, technology, and pros and cons for the selected articles under each category. Additionally, we discuss the key open issues related to scalability and fairness, resource allocation and offloading optimization, service delivery with a focus on quality of experience (QoE) and quality of service (QoS), and standardization. Finally, we discuss several future research directions that would address the aforementioned issues and emerging use cases for UAV enabled MEC.}
}
@article{ZHU2022108458,
title = {Cloud-edge collaborative distributed optimal dispatching strategy for an electric-gas integrated energy system considering carbon emission reductions},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {143},
pages = {108458},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108458},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522004677},
author = {Xu Zhu and Jun Yang and Xiangpeng Zhan and Yuanzhang Sun and Yuwei Zhang},
keywords = {Alternating direction method of multipliers, Integrated energy system, Distributed optimization, Carbon trading, Demand response},
abstract = {To reduce carbon emissions and improve the energy efficiency of energy systems, integrated energy systems (IESs) have been promoted in recent years. However, the data of different energy networks cannot be fully shared. The data privacyprotection is attracting more attention in demand side. It is difficult for centralized dispatching strategies, which must obtain all the data of energy systems during optimization, to be applied. To solve these issues, a cloud-edge collaborative distributed optimal dispatching strategy for electric-gas IESs is proposed in this paper. The centralized IES scheduling problem is reasonably divided into multiple subproblems. Considering information barrier, the cloud computing centers are separately set in different energy networks to solve energy flow optimization subproblems. Considering prosumer privacy protection, edge computing units are separately set in energy hubs to deal with cost and carbon emissions minimization of regional integrated energy systems (RIESs). Based on the proximal Jacobian alternating direction method of multipliers, the common optimization variables interactive iteration and parallel solution of multiple dispatching models are developed. The simulation results show that the proposed distributed optimization method can achieve the same accuracy as the centralized optimization method and improve problem-solvingefficiency in general.}
}
@article{MUNEESWARI2024101076,
title = {GEP optimization for load balancing of virtual machines (LBVM) in cloud computing},
journal = {Measurement: Sensors},
volume = {33},
pages = {101076},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101076},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424000527},
author = {G. Muneeswari and Jhansi Bharathi Madavarapu and R. Ramani and C. Rajeshkumar and C. {John Clement Singh}},
keywords = {Load balancing, Cloud computing, Virtual machines, Bi-LSTM, Genetic expression programming},
abstract = {Cloud computing relies heavily on load balancing to distribute workloads evenly among servers, network connections, and drives. The cloud system has been assigned some load which can be underloaded, overloaded, or balanced depending on the cloud architecture and user requests. An important component of task scheduling in clouds is the load balancing of workloads that may be dependent or independent of virtual machines (VMs). To overcome these drawbacks, a novel Load Balancing of Virtual Machine (LBVM) in Cloud Computing has been proposed in this paper. The input tasks from multiple users were collected in a single task collector and sent towards the load balancer, which contains the deep learning network called the Bi-LSTM technique. When the load is unbalanced, the VM migration will begin by sending the task details to the load balancer. The Bi-LSTM is optimized by a Genetic Expression Programming (GEP) optimizer and finally, it balances the input loads in VMs. The efficiency of the proposed LBVM has been determined using the existing techniques such as MVM, PLBVM, and VMIS in terms of evaluation metrics such as configuration latency, detection rate, accuracy etc. Experimental results shows that the proposed method reduces the Migration Time of 49%, 41.7%, and 17.8% than MVM, PLBVM, VMIS existing techniques respectively.}
}
@article{HOU2024102177,
title = {EETS: An energy-efficient task scheduler in cloud computing based on improved DQN algorithm},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {8},
pages = {102177},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102177},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824002660},
author = {Huanhuan Hou and Azlan Ismail},
keywords = {Cloud computing, Task scheduling, Deep reinforcement learning, Energy-efficient, Double dueling DQN, Prioritized experience replay},
abstract = {The huge energy consumption of data centers in cloud computing leads to increased operating costs and high carbon emissions to the environment. Deep Reinforcement Learning (DRL) technology combines of deep learning and reinforcement learning, which has an obvious advantage in solving complex task scheduling problems. Deep Q Network(DQN)-based task scheduling has been employed for objective optimization. However, training the DQN algorithm may result in value overestimation, which can negatively impact the learning effectiveness. The replay buffer technique, while increasing sample utilization, does not distinguish between sample importance, resulting in limited utilization of valuable samples. This study proposes an enhanced task scheduling algorithm based on the DQN framework, which utilizes a more optimized Dueling-network architecture as well as Double DQN strategy to alleviate the overestimation bias and address the shortcomings of DQN. It also incorporates a prioritized experience replay technique to achieve importance sampling of experience data, which overcomes the problem of low utilization due to uniform sampling from replay memory. Based on these improved techniques, we developed an energy-efficient task scheduling algorithm called EETS (Energy-Efficient Task Scheduling). This algorithm automatically learns the optimal scheduling policy from historical data while interacting with the environment. Experimental results demonstrate that EETS exhibits faster convergence rates and higher rewards compared to both DQN and DDQN. In scheduling performance, EETS outperforms other baseline algorithms in key metrics, including energy consumption, average task response time, and average machine working time. Particularly, it has a significant advantage when handling large batches of tasks.}
}
@article{DARAGHMEH2024102925,
title = {Optimizing serverless computing: A comparative analysis of multi-output regression models for predictive function invocations},
journal = {Simulation Modelling Practice and Theory},
volume = {134},
pages = {102925},
year = {2024},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2024.102925},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X2400039X},
author = {Mustafa Daraghmeh and Anjali Agarwal and Yaser Jararweh},
keywords = {Serverless computing, Predictive function invocation, Multi-output regression models, Windowing techniques, Comparative analysis framework},
abstract = {In the rapidly evolving domain of serverless computing, the need for efficient and accurate predictive methods of function invocation becomes paramount. This study introduces a comprehensive suite of innovations to improve the predictability and efficiency of function invocation within serverless architectures. By employing multi-output regression models, we perform a multi-level analysis of function invocation patterns across user, application, and function levels, revealing insights into granular workload behaviors. We rigorously investigate the impact of windowing techniques and dimensionality reduction on model performance via Principal Component Analysis (PCA), offering a nuanced understanding of data complexities and computational implications. Our novel comparative analysis framework meticulously evaluates the performance of these methods against various windowing configurations, utilizing the Azure Functions dataset for real-world applicability. In addition, we assess the temporal stability of the models and the variation of day-to-day performance, providing a holistic view of their operational viability. Our contributions address critical gaps in the predictive modeling of serverless computing and set a new benchmark for operational efficiency and data-driven decision-making in cloud environments. This study is poised to guide future advancements in serverless computing, driving theoretically sound and practically viable innovations.}
}
@article{AYGUN2021106870,
title = {Application of binary PSO for public cloud resources allocation system of video on demand (VoD) services},
journal = {Applied Soft Computing},
volume = {99},
pages = {106870},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106870},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620308085},
author = {Betul Aygun and Banu {Gunel Kilic} and Nursal Arici and Ahmet Cosar and Bedriye Tuncsiper},
keywords = {Cost optimization, Cloud computing, Resource allocation, Quality of service, Linear programming, Constraint optimization, Particle swarm optimization, Binary optimization},
abstract = {Video streaming, whether on demand or live, has become one of the most popular internet applications. However, financial investments required for it is a severe problem since it needs more real time storage, higher data transfer and a significant amount of computation than other kinds of multimedia data. To tackle this problem, cloud computing, offering services without investing in hardware or software, emerges as a preferred technology. However, there are a large number of cloud service providers and they offer different pricing strategies for various applications in various regions. Therefore, it is of great importance for them that incoming service requests are assigned to the appropriate cloud services at minimum cost and provide maximum user satisfaction [quality of service (QoS) attributes]. Due to the issues, such as multiple cloud providers, different QoS requirements, different service level agreements and uncertainties in demand, price and availability, the optimization of resource allocation present further challenges. The objective of our study is to optimize the cost and performance of video on demand applications using cloud content delivery networks, storage and transcoders based on the QoS requirements of users. To solve the NP-hard problem, Particle Swarm Optimization (PSO) technique is used due to the easiness in its concept and coding, less sensitive to the nature of the objective function, limited number of parameters and generating high quality solution within a short time. We propose a new method in which the optimum solution is affected not only by the best solution of the particle and global best solution but also by the best solution of the neighborhood particles in that iteration. This ternary approach is implemented into the well-known discrete and constrained PSO, achieving the minimum cost with user satisfaction for allocation of video requests to cloud resources. Although the proposed method yields better results in terms of accuracy, execution time of the algorithm is not reasonable. To overcome this inefficiency; ternary approach is embedded into multi-swarm PSO and it is parallelized and combined with greedy heuristic algorithms. The results of the comparison with the benchmarking algorithms show that our proposed method yields better results from the standpoint of both accuracy and execution time.}
}
@article{LU2020106497,
title = {Parallel and distributed architecture of genetic algorithm on Apache Hadoop and Spark},
journal = {Applied Soft Computing},
volume = {95},
pages = {106497},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106497},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620304361},
author = {Hao-Chun Lu and F.J. Hwang and Yao-Huei Huang},
keywords = {Genetic algorithm, Parallel and distributed computing, Traveling salesman problems, Apache Hadoop, Apache Spark},
abstract = {The genetic algorithm (GA), one of the best-known metaheuristic algorithms, has been extensively utilized in various fields of management science, operational research, and industrial engineering. The efficiency of GAs in solving large-scale optimization problems would be enhanced if the iterative processes required by the genetic operators can be implemented in a parallel and distributed computing architecture. Apache Hadoop has recently been one of the most popular systems for distributed storage and parallel processing of big data. By integrating the GA highly into Apache Hadoop, this study proposes an advanced GA parallel and distributed computing architecture that achieves the effectiveness and efficiency of GA evolution. Characterized by the sophisticated mechanism of dispatching the GA core operators into Apache Hadoop, the developed computing framework fits well with the cloud computing model. The presented GA parallelization architecture outperforms the state-of-the-art reference architectures according to the computational experiments where the testing instances of traveling salesman problems are employed. Our numerical experiments also demonstrate that the proposed architecture can readily be extended to Apache Spark.}
}
@article{ZOLFAGHARI2021100524,
title = {Application of virtual machine consolidation in cloud computing systems},
journal = {Sustainable Computing: Informatics and Systems},
volume = {30},
pages = {100524},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100524},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921000172},
author = {Rahmat Zolfaghari and Amir Sahafi and Amir Masoud Rahmani and Reza Rezaei},
keywords = {Cloud computing systems, Virtual machines migration, Virtual machines consolidation, Energy utilization, Datacenter},
abstract = {Cloud systems play a vital and significant role in our daily lives due to various internet services. For instance, email services, social networks, and others. Consequently, their energy consumption has also become an increasingly essential concern in Cloud Computing Systems (CCSs). Virtualization technologies are widely used to facilitate the management of CCSs and reduce their energy consumption. Virtualization enables live migration of Virtual Machines (VMs) where several VMs can be loaded on some Physical machines (PMs) called VM consolidation. A VM consolidation algorithm can be an effective technique for reducing energy consumption, operational cost, hardware cost, Service Level Agreements (SLAs) compliance/violation, CO2 emissions, and enhancing the hardware and service reliability, performance, and hardware lifetime, load balancing, and utilization in CCSs. Essentially, VM consolidation must minimize energy utilization and service quality in the cloud system. This study presents a taxonomy comprising resource assignment method, metrics, objective functions, migration methods, algorithmic methods, co-location criteria of VMs, architectures, workload dataset, and evaluation approaches in VM consolidation CCSs. Also, we reviewed related work regarding the resources of PMs, algorithm methods, metrics, architectures, and the objectives in static/dynamic VM consolidation.}
}
@article{SANGWAN20221783,
title = {Fuzzy Firefly Based Intelligent Algorithm for Load Balancing in Mobile Cloud Computing},
journal = {Computers, Materials and Continua},
volume = {74},
number = {1},
pages = {1783-1799},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.031729},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822002508},
author = {Suman Sangwan},
keywords = {Cloud computing, cloudlet, mobile cloud computing, fuzzy, firefly, load balancing, makespan, degree of imbalance},
abstract = {This paper presents a novel fuzzy firefly-based intelligent algorithm for load balancing in mobile cloud computing while reducing makespan. The proposed technique implicitly acts intelligently by using inherent traits of fuzzy and firefly. It automatically adjusts its behavior or converges depending on the information gathered during the search process and objective function. It works for 3-tier architecture, including cloudlet and public cloud. As cloudlets have limited resources, fuzzy logic is used for cloudlet selection using capacity and waiting time as input. Fuzzy provides human-like decisions without using any mathematical model. Firefly is a powerful meta-heuristic optimization technique to balance diversification and solution speed. It balances the load on cloud and cloudlet while minimizing makespan and execution time. However, it may trap in local optimum; levy flight can handle it. Hybridization of fuzzy firefly with levy flight is a novel technique that provides reduced makespan, execution time, and Degree of imbalance while balancing the load. Simulation has been carried out on the Cloud Analyst platform with National Aeronautics and Space Administration (NASA) and Clarknet datasets. Results show that the proposed algorithm outperforms Ant Colony Optimization Queue Decision Maker (ACOQDM), Distributed Scheduling Optimization Algorithm (DSOA), and Utility-based Firefly Algorithm (UFA) when compared in terms of makespan, Degree of imbalance, and Figure of Merit.}
}
@article{ARJONA2021215,
title = {Triggerflow: Trigger-based orchestration of serverless workflows},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {215-229},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001989},
author = {Aitor Arjona and Pedro García López and Josep Sampé and Aleksander Slominski and Lionel Villard},
keywords = {Event-based, Orchestration, Serverless},
abstract = {As more applications are being moved to the Cloud thanks to serverless computing, it is increasingly necessary to support the native life cycle execution of those applications in the data center. But existing cloud orchestration systems either focus on short-running workflows (like IBM Composer or Amazon Step Functions Express Workflows) or impose considerable overheads for synchronizing massively parallel jobs (Azure Durable Functions, Amazon Step Functions). None of them are open systems enabling extensible interception and optimization of custom workflows. We present Triggerflow: an extensible Trigger-based Orchestration architecture for serverless workflows. We demonstrate that Triggerflow is a novel serverless building block capable of constructing different reactive orchestrators (State Machines, Directed Acyclic Graphs, Workflow as code, Federated Learning orchestrator). We also validate that it can support high-volume event processing workloads, auto-scale on demand with scale down to zero when not used, and transparently guarantee fault tolerance and efficient resource usage when orchestrating long running scientific workflows.}
}
@article{XIAO2023150,
title = {Explore deep reinforcement learning for efficient task processing based on federated optimization in big data},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {150-161},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23002492},
author = {Shan Xiao and Chunyi Wu},
keywords = {Big data, Consumer electronics, Deep reinforcement learning, Federated optimization, Virtual network embedding},
abstract = {In recent years, along with the extensive application of consumer electronics, the task execution with cloud computing for big data has become one of the research focuses. Nevertheless, the traditional theories and algorithms are still employed by existing research work to explore the feasible solutions, which takes a beating from low generalization performance, system load imbalance, more response delay, etc. To solve the matter, a task execution method called DROP (Deep Reinforcement network aided Optimization method aiming at task Processing) has been put forward, which is capable of completing task request allocation through virtual network embedding. The prominence of this method is explained by its effect in reducing load balancing degree, minimizing bandwidth resource overhead, and preserving electric energy as well as meeting customer demands. It makes use of Deep Deterministic Policy Gradient (DDPG) instead of depending on tons of iterations for better path selection schemes in previous methods, through continuous environment interaction and trial-and-error evaluation to get better strategy selection for virtual link embedding. To realize the virtual node embedding in the federated optimization based system architecture, the intentional deep feature learning network is applied. Compared with the cutting edge approaches, the performance benefits of DROP can be verified by the experimental results in terms of bringing down the extra cost on resources and energy of the substrate network during the task execution for big data.}
}
@article{JEREMIAH2024103120,
title = {A comprehensive survey of digital twins: Applications, technologies and security challenges},
journal = {Journal of Systems Architecture},
volume = {151},
pages = {103120},
year = {2024},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2024.103120},
url = {https://www.sciencedirect.com/science/article/pii/S1383762124000572},
author = {Sekione Reward Jeremiah and Abir {El Azzaoui} and Neal N. Xiong and Jong Hyuk Park},
keywords = {Digital twin, Virtual twin, Digital twin security, Digital twin network, Digital twin modeling, DT enabling technologies},
abstract = {Alongside advancements in Artificial Intelligence (AI), significant progress has been made in big data processing, edge/cloud computing, and ubiquitous computing in the past two decades. These advancements catalyzed the development and adoption of Digital Twins (DT) across various domains, serving as virtual replicas of Physical Objects (POs). DTs provide advanced visualization and simulation capabilities, enabling effective estimation, optimization, and forecasting of PO's behaviors. However, the widespread adoption of DTs has introduced various security threats, vulnerabilities, and attacks. Despite ongoing research in DT applications and security, there is a lack of systematic review of the DT security literature across domains and architectural layers. This study fills this gap by systematically reviewing DT research, focusing on three interrelated aspects: DT applications, architectural layers, and security. We explore DT's architectural layers, functional requirements, application, and creation software to identify potential threats, attacks, and vulnerabilities specific to DT layers and application domains. We then systematize our findings under a unified security framework and pinpoint countermeasures against identified security challenges. Furthermore, our study explores DT's role in mitigating existing cyber threats, and we conclude our work by identifying open challenges and potential research directions.}
}
@article{CENTOFANTI2024110371,
title = {Impact of power consumption in containerized clouds: A comprehensive analysis of open-source power measurement tools},
journal = {Computer Networks},
volume = {245},
pages = {110371},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110371},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624002032},
author = {Carlo Centofanti and José Santos and Venkateswarlu Gudepu and Koteswararao Kondepu},
keywords = {Energy efficiency, Power consumption, Sustainability, Containers, Cloud computing, Service orchestration, Kubernetes},
abstract = {Recently, container-based solutions have become de facto compute units of modern cloud-native applications. However, the exponential growth in data traffic and the power consumption of these technologies to handle high data traffic alarm the strong need for energy evaluation approaches in containerized clouds. Furthermore, the proliferation of highly distributed edge clouds raises additional concerns regarding the power consumption of future cloud architectures. This article presents a detailed overview of methods and techniques for monitoring power consumption within popular cloud platforms. The study offers an in-depth evaluation of these approaches, demonstrating variations in measured power consumption based on the chosen technique. A well-known container orchestration platform named Kubernetes (K8s) has been applied in our extensive measurements. This work argues that energy-efficient container clouds will play a vital role in building a more sustainable and eco-friendly digital infrastructure by optimizing power consumption and reducing carbon footprint, paving the way for a greener future. The paper also discusses open challenges and future research directions on energy sustainability, leading to the conclusion, offering lessons learned and prospects on potential solutions to foster sustainable practices within the container ecosystem.}
}
@article{COELHO2021778,
title = {Parallel Metaheuristics for Shop Scheduling: enabling Industry 4.0},
journal = {Procedia Computer Science},
volume = {180},
pages = {778-786},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.328},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003793},
author = {Pedro Coelho and Cristovão Silva},
keywords = {Industry 4.0, production scheduling, metaheuristics, parallel processing},
abstract = {Production scheduling is one of the most critical activities in manufacturing. Under the context of Industry 4.0 paradigm, shop scheduling becomes even more complex. Metaheuristics present the potential to solve these harder problems but demand substantial computational power. The use of high-performance parallel architectures, present in cloud computing and edge computing, may support the develop of better metaheuristics, enabling Industry 4.0 with solution techniques to deal with their scheduling complexity. This study provides an overview of parallel metaheuristics for shop scheduling in recent literature. We reviewed 28 papers and classified them, according to parallel architectures, shop configuration, metaheuristics and optimization criteria. The results support that parallel metaheuristic have potential to tackle Industry 4.0 scheduling problems. However, it is essential to extend the research to the cloud and edge computing, flexible shop configurations, dynamic problems with multi-resource, and multi-objective optimization. Future studies should consider the use of real-world data instances.}
}
@article{SANCHEZRIBES2020103336,
title = {Mobile Cloud computing architecture for massively parallelizable geometric computation},
journal = {Computers in Industry},
volume = {123},
pages = {103336},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103336},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305704},
author = {Víctor {Sánchez Ribes} and Higinio Mora and Andrzej Sobecki and Francisco José {Mora Gimeno}},
keywords = {GPU computing, GPU, CUDA, Cloud offloading, Cloud computing},
abstract = {Cloud Computing is one of the most disruptive technologies of this century. This technology has been widely adopted in many areas of the society. In the field of manufacturing industry, it can be used to provide advantages in the execution of the complex geometric computation algorithms involved on CAD/CAM processes. The idea proposed in this research consists in outsourcing part of the load to be computed in the client machines to the cloud through the Mobile Cloud Computing paradigm. This practice gives substantial benefits to both the clients and the software-provider in terms of costs, flexibility, ubiquity and performance. In this document, an outsourcing architecture is proposed based on this paradigm. Extensive experiments have been done using highly parallelizable computational geometry operations to show the strengths and weaknesses of the proposal in combination of specialized computing platforms in the cloud. The results suggest that there are some issues that affect the overall performance and the stability of the QoS: the network communication delay, and the number of simultaneous clients and multiple requests. Some solutions have been proposed to face these challenges.}
}
@article{ZENG2024121,
title = {Joint optimization of multi-dimensional resource allocation and task offloading for QoE enhancement in Cloud-Edge-End collaboration},
journal = {Future Generation Computer Systems},
volume = {155},
pages = {121-131},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000311},
author = {Chao Zeng and Xingwei Wang and Rongfei Zeng and Ying Li and Jianzhi Shi and Min Huang},
keywords = {Cloud–edge–end collaboration, Task offloading, Resources allocation, Quality of experience, Multi-agent reinforcement learning},
abstract = {Cloud-Edge-End Collaboration (CEEC) computing architecture inherits many merits from both edge computing and cloud computing and thus is considered as a promising candidate for future network services. In CEEC, user’s QoE is impacted by offload performance which should consider offload strategy, computational resources and network status simultaneously. However, previous offload optimization studies neglect the joint consideration of dependent task offloading, computational resources and channel resources, which may not produce potential performance improvement. In this paper, we investigate the joint optimization of dependent task offloading, computational resource allocation, user transmission power control, and channel resource allocation in the CEEC scenario, with the goal of maximizing user’s QoE. Initially, a new QoE metric is defined to capture the impacts of delay and energy consumption on user’s QoE. Following this definition, we formulate the joint optimization problem as a Mixed Integer Nonlinear Programming (MINLP) problem and introduce a method of multi-agent deep reinforcement learning to solve our MINLP problem with high computation complexity. Extensive experiments are performed, and experimental results show that our proposed scheme outperforms baselines in a series of metrics.}
}
@article{HARIS20229696,
title = {Mantaray modified multi-objective Harris hawk optimization algorithm expedites optimal load balancing in cloud computing},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {9696-9709},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821003372},
author = {Mohammad Haris and Swaleha Zubair},
keywords = {Load balancing, Multi-objective, MRFO, HHO, Cloudsim},
abstract = {Task scheduling in the cloud is a difficult optimization challenge. The cloud system is assigned with a specific load based on the cloud architecture and user demands. However, both underloaded and overloaded situations result in a variety of system failures in terms of power consumption, machine failure, and so on. As a result, task-load balancing on virtual machines (VMs) is considered as an important part of cloud task scheduling. The present study proposes a dynamic load balancing algorithm based on the hybrid optimization algorithms named as Mantaray modified multi-objective Harris hawk optimization (MMHHO). The hybridization process updates the search space of Harris Hawk Optimization (HHO) by utilizing the Manta Ray Forging Optimization (MRFO) algorithm by considering the cost, response time, and resource utilization etc. The hybrid scheme, proposed in the present study, improves the system performance by enhancing the VMs throughput, balancing the load between the VMs, and sustaining the balance among priorities of tasks by adjusting the waiting time of the involved tasks. The proposed MMHHO based load balancing algorithm is implemented in CloudSim tool. The effectiveness of the suggested algorithm has been analyzed in terms of various parameters and compared with other existing algorithms. The simulation results show that the suggested MMHHO load balancing scheme outperforms other algorithms.}
}
@article{HSU2022101945,
title = {Big data analysis and optimization and platform components},
journal = {Journal of King Saud University - Science},
volume = {34},
number = {4},
pages = {101945},
year = {2022},
issn = {1018-3647},
doi = {https://doi.org/10.1016/j.jksus.2022.101945},
url = {https://www.sciencedirect.com/science/article/pii/S1018364722001264},
author = {Kenglung Hsu},
keywords = {Big data platform, Cloud computing technology, Platform construction, Network database},
abstract = {Communication operators are paying more and more attention to the value of data and are demanding more and bigger data technologies. Many companies have started to take advantage of their resources to tap the value of data and develop their own core business. The use of a high-performance, secure, scalable and easy-to-manage big data management system will help companies avoid the tedious system operation and maintenance, especially in the communication business system, and help them focus on their own business development. In this paper, we first investigate existing data management systems and analyze their strengths and weaknesses. In response to the problems of not too light, insufficient timeliness of data migration, and not enough innovation in data analysis, we design a more efficient, convenient and easy-to-use big data management platform. Firstly, the big data management system is designed. According to the process of big data processing, six modules of interface acquisition, program scheduling, data aggregation, platform alerting, marketing analysis and visualization are designed based on the communication big data platform architecture. The management system mainly focuses on data access and data mining analysis, so the main modules of this paper are program scheduling, data aggregation and marketing analysis modules, while other modules are based on the original big data management system of the enterprise with a small amount of improvement. In order to realize the needs of dynamically creating data testing environment and isolating production and experimental environments under the communication application scenario, a mechanism of the big data system for production and the virtualized system for experiments acting together is proposed. Then the corresponding scheduling module architecture process is designed and built, the corresponding scheduling rules and related scheduling information field tables are designed, and the data aggregation storage is improved. The program scheduling module was designed to be more lightweight and easy to use, and the data migration module increased the timeliness of data migration.}
}
@article{ISSAC2023100265,
title = {Development and deployment of a big data pipeline for field-based high-throughput cotton phenotyping data},
journal = {Smart Agricultural Technology},
volume = {5},
pages = {100265},
year = {2023},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2023.100265},
url = {https://www.sciencedirect.com/science/article/pii/S2772375523000953},
author = {Amanda Issac and Alireza Ebrahimi and Javad {Mohammadpour Velni} and Glen Rains},
keywords = {High-throughput cotton phenotyping, Big data pipeline, Lambda architecture, Computer vision, Deep neural networks},
abstract = {In this study, we propose a big data pipeline for cotton bloom detection using a Lambda architecture, which enables real-time and batch processing of data. Our proposed approach leverages Azure resources such as Data Factory, Event Grids, Rest APIs, and Databricks. This work is the first to develop and demonstrate the implementation of such a pipeline for plant phenotyping through Azure's cloud computing service. The proposed pipeline consists of data preprocessing, object detection using a YOLOv5 neural network model trained through Azure AutoML, and visualization of object detection bounding boxes on output images. The trained model achieves a mean Average Precision (mAP) score of 0.96, demonstrating its high performance for cotton bloom classification. We evaluate our Lambda architecture pipeline using 9,000 images yielding an optimized runtime of 34 minutes. The results illustrate the scalability of the proposed pipeline as a solution for deep learning object detection, with the potential for further expansion through additional Azure processing cores. This work advances the scientific research field by providing a new method for cotton bloom detection on a large dataset and demonstrates the potential of utilizing cloud computing resources, specifically Azure, for efficient and accurate big data processing in precision agriculture.}
}
@article{WU2021102257,
title = {Accelerating DNNs from local to virtualized FPGA in the Cloud: A survey of trends},
journal = {Journal of Systems Architecture},
volume = {119},
pages = {102257},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102257},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121001752},
author = {Chen Wu and Virginie Fresse and Benoit Suffran and Hubert Konik},
keywords = {FPGA virtualization, Cloud computing, Deep neural network, Accelerator, Trends},
abstract = {Field-programmable gate arrays (FPGAs) are widely used locally to speed up deep neural network (DNN) algorithms with high computational throughput and energy efficiency. Virtualizing FPGA and deploying FPGAs in the cloud are becoming increasingly attractive methods for DNN acceleration because they can enhance the computing ability to achieve on-demand acceleration across multiple users. In the past five years, researchers have extensively investigated various directions of FPGA-based DNN accelerators, such as algorithm optimization, architecture exploration, capacity improvement, resource sharing, and cloud construction. However, previous DNN accelerator surveys mainly focused on optimizing the DNN performance on a local FPGA, ignoring the trend of placing DNN accelerators in the cloud’s FPGA. In this study, we conducted an in-depth investigation of the technologies used in FPGA-based DNN accelerators, including but not limited to architectural design, optimization strategies, virtualization technologies, and cloud services. Additionally, we studied the evolution of DNN accelerators, e.g., from a single DNN to framework-generated DNNs, from physical to virtualized FPGAs, from local to the cloud, and from single-user to multi-tenant. We also identified significant obstacles for DNN acceleration in the cloud. This article enhances the current understanding of the evolution of FPGA-based DNN accelerators.}
}
@article{SHARMA2022100531,
title = {Ant colony based optimization model for QoS-based task scheduling in cloud computing environment},
journal = {Measurement: Sensors},
volume = {24},
pages = {100531},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100531},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422001659},
author = {Neetu Sharma and  Sonal and Puneet Garg},
keywords = {GA, LBACO, ACO, EBS, Data centers, Service oriented architecture},
abstract = {Cloud computing has evolved with various techniques for satisfying the needs of users to reduce costs and offer better results. The users' needs may include sharing of resources like memories, processors, apps, information, data, applications, etc., whereas, performance should be in terms of improved DC processing period & response time. Apart from the above-said, there is also a requirement to manage the stability of the system and be flexible to make amendments to the system. Static data is used to optimize the project schedule. However, the traditional does not take into account the personnel allocation matrix when scheduling projects. The ACO model is not a suitable solution to the scheduling problem. The classic ACO methodology operates in two phases: the first phase uses an event-based scheduler to address the complex planning issue. Planning and allocating resources for a project are both constrained by these two approaches. The event-based ACO model was developed to match the resource restrictions and activity schedule to handle dynamic data allocation. When it comes to multi-objective scheduling, EBS with ACO is not an adequate approach. An updated ACO technique to optimum global search employing a neural network approach was presented to schedule many activities to tackle the difficulty of multiple objectives. Using the suggested multi-objective technique, an activity with a defined number of tasks and necessary resources may be optimally organized. The algorithm LBACO is the most effective one to apply for optimizing the objective function, according to the test results, as well as the fastest. An advantage of ACO over other techniques is its ability to provide better plans with higher statistics and mean access times, as well as more consistent job assignments.}
}
@article{HE2022100678,
title = {QoS-aware hybrid cloudlet placement over joint fiber and wireless backhaul access network},
journal = {Optical Switching and Networking},
volume = {45},
pages = {100678},
year = {2022},
issn = {1573-4277},
doi = {https://doi.org/10.1016/j.osn.2022.100678},
url = {https://www.sciencedirect.com/science/article/pii/S1573427722000145},
author = {Chao He and Ruyan Wang and Dapeng Wu and Hong Zhang and Zefu Tan},
keywords = {Hybrid cloudlet deployment, FiWi access network, Backhaul-aware infrastructures, Task request offloading, Cost optimization, QoS constraint},
abstract = {Over the last decades, the emerging paradigms, e.g., centralized mobile cloud computing (MCC), multi-access edge computing (MEC), and collaborative computing offloading (CCO), have attracted extensive attention in adaptive 5G low latency communication for Tactile Internet. Besides, a novel integrated wireless optical transport network emerges in access networks and supports joint fronthaul and backhaul services. Therefore, considering that the task offloading response time, the needs of proximity, and the ultra-dense 5G small cell deployment, a novelty hybrid cloudlet deployment scheme over fiber and wireless backhaul-aware infrastructure is created with efficient cost optimization in mind. In this work, the envisioned Fiber Wireless networks (FiWi) consists of optical backhaul and wireless fronthaul (i.e., integrated access network and backhaul link), whereby base stations (BSs) with fiber and wireless backhauls are referred as wired-BS (WBS) and unwired-BS (UBS), respectively. Therefore, to meet the quality of service (QoS) delay constraints, cloudlets can be deployed in either UBSs, WBS, or remote node (RN). However, we apply mixed-integer line programming (MILP) to resolve the convex optimization problem in terms of minimization deployment cost. Besides, we describe the QoS-aware hybrid cloudlet placement cost algorithm for WBS and UBS coverage areas against different network conditions. It was shown through experimental measurements that the proposed architecture can achieve the scalability in different deployment scenarios. Also, the dependency of minimization cloudlet deployment cost on variable network parameters in terms of user density, network framework, and network QoS can be validated.}
}
@article{LIANG2022116762,
title = {A high-applicability heterogeneous cloud data centers resource management algorithm based on trusted virtual machine migration},
journal = {Expert Systems with Applications},
volume = {197},
pages = {116762},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116762},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422002275},
author = {Bin Liang and Xiaoshe Dong and Yufei Wang and Xingjun Zhang},
keywords = {Cloud data center, Virtual machine integration, Virtual machine migration, Multi-dimensional virtual machine, Scheduling algorithm},
abstract = {With the continuous development and maturity of cloud computing technology, the scale and number of cloud data center (CDC) are also expanding. This increasingly draws attention to the problem of high energy consumption in CDCs. Dynamic virtual machine (VM) consolidation is a promising approach for reducing energy consumption. VM migration, as a VM consolidation technology, can effectively improve the utilization of physical machine (PM) and optimize the scheduling process of CDCs. However, most VM integration algorithms, in existing research, are aimed at improving the utilization of PMs. Excessive utilization of PMs may increase the competition for shared resources among the VMs running on them. As a result, the performance of these VMs deteriorates, and the execution time of cloud tasks is increased or even interrupted. This study systematically analyzes the overall architecture of CDCs. Subsequently, migration rules are established for the one-dimensional and multidimensional trusted VMs. A high- applicability heterogeneous CDC resource management algorithm based on trusted VM migration (HTVM2) is then proposed. The proposed algorithm not only solves the one-dimensional VM migration problem of homogeneous and heterogeneous CDCs but also those of multi-dimensional VMs. This improves the success rate of VM migration, reduces the energy consumption of the CDC, and improves load balancing while ensuring VM performance. Finally, the algorithm was compared with the other three algorithms outperforming them all, as demonstrated by experimental results.}
}
@article{CAIAZZA2021108140,
title = {Measurement-driven design and runtime optimization in edge computing: Methodology and tools},
journal = {Computer Networks},
volume = {194},
pages = {108140},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108140},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002085},
author = {Chiara Caiazza and Claudio Cicconetti and Valerio Luconi and Alessio Vecchio},
keywords = {Edge computing, ETSI MEC, Network measurements, GSMA platform operator},
abstract = {Edge computing is projected to become the dominant form of cloud computing in the future because of the significant advantages it brings to both users (less latency, higher throughput) and telecom operators (less Internet traffic, more local management). However, to fully unlock its potential at scale, system designers and automated optimization systems alike will have to monitor closely the dynamics of both processing and communication facilities. Especially the latter is often neglected in current systems since network performance in cloud computing plays only a minor role. In this paper, we propose the architecture of MECPerf, which is a solution to collect network measurements in a live edge computing domain, to be collected for offline provisioning analysis and simulations, or to be provided in real-time for on-line system optimization. MECPerf has been validated in a realistic testbed funded by the European Commission (Fed4Fire+), and we describe here a summary of the results, which are fully available as open data and through a Python library to expedite their utilization. This is demonstrated via a use case involving the optimization of a system parameter for migrating clients in a federated edge computing system adopting the GSMA platform operator concept.}
}
@article{RAZA2024109405,
title = {SAARC super smart grid: Navigating the future - unleashing the power of an energy-efficient integration of renewable energy resources in the saarc region},
journal = {Computers and Electrical Engineering},
volume = {118},
pages = {109405},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109405},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624003331},
author = {Ali Raza and Marriam Liaqat and Muhammad Adnan and Muhammad Sajid Iqbal and Li Jingzhao and Ijaz Ahmad},
keywords = {Carbon dioxide emissions, Demand and response management, High voltage transmission, Renewable energy, Super smart grid, Power flow control, SAARC, FACTs, Clean energy},
abstract = {The major problems in the power system network of the South Asian Association for Regional Cooperation (SAARC) region include the utilization of substantial fossil fuels, which leads to significant emissions of CO2. Moreover, due to the depletion of fossil fuels, it is very difficult to manage the demand requirement of the SAARC regions, which leads to significant power shortage issues. These problems would be resolved through the integration of the substantial amount of renewable energy resources (RERs) that are available in excess in the SAARC regions into the power system networks. This will provide a futuristic roadmap to a sustainable and green energy environment in the SAARC region. In this regard, the SAARC region has devised a new plan to transform its smart grid infrastructure into the super smart grid (SSG), which would connect the SAARC countries using the high voltage transmission lines utilizing each country's substantial RERs. To accelerate this ecofriendly effort, the key objective of this review study is to evaluate the potential of the SAARC SSG for demand management and emissions reduction. This study provides a comprehensive overview of SSG technology, covering its technical background, management, power flow improvement techniques, power stability techniques, simulation models, and future steps for development in the SAARC region. First, the regional conflicts, energy deficiency, and potential of the SAARC region have been discussed. Second, a potential architecture of the SAARC SSG has been simulated. Third, a novel Markov chain modeling has been presented for demand management in the SAARC regions. The results and discussions section presents findings from simulations and models. The paper also discusses CO2 reduction techniques, including methods for capturing, storing, and utilizing carbon in biofuel. The study also discusses futuristic optimization techniques in SSG, including blockchain, federated learning, reinforcement learning, the metaverse, digital twin technology, artificial intelligence, and cloud computing. The SSG challenges and limitations section summarizes the challenges and limitations faced by SSG. Finally, a roadmap to a sustainable and green energy environment for the SAARC power grids has been identified to support the implementation of the futuristic SAARC SSG. The results exhibited that there is a good potential for successful demand management and emissions reduction through the SAARC SSG.}
}
@article{YAN2021107216,
title = {HANSEL: Adaptive horizontal scaling of microservices using Bi-LSTM},
journal = {Applied Soft Computing},
volume = {105},
pages = {107216},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107216},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621001393},
author = {Ming Yan and XiaoMeng Liang and ZhiHui Lu and Jie Wu and Wei Zhang},
keywords = {Edge computing, LSTM, Elastic scaling, Microservice},
abstract = {With the rapid development of 5G network, business scenarios such as intelligent service and new retail are becoming more and more popular. The demand for more flexible and scalable real-time data processing, in particular, the AI-related data processing has also increased in edge computing. Therefore, how to meet such business development has become a major challenge. Focusing on this requirement, microservice architecture, proposed and developed by some big cloud computing companies’ platform, such as Google Kubernetes platform, has gradually become a mainstream technology solution in edge computing. However, many microservices used in edge computing cannot achieve an even time distribution, which is random or sudden. Kubernetes built-in Horizontal POD Autoscaling (HPA) is unable to well handle the change of microservice load, which inevitably leads to the waste of system resources and affects the SLA of microservice. To solve this issue, this paper proposes a HANSEL system based on Kubernetes platform, which can optimize the horizontal elastic scaling policy of Kubernetes by accurately predicting the load of microservices based on the Bi-LSTM load prediction algorithm with attention mechanism. Furthermore, active elastic scaling is realized through reinforcement learning method, and we design a hybrid elastic scaling mechanism through combining reactive and active methods, so as to construct an elastic scaling system for automatic scheduling of working nodes. Our experimental results show that HANSEL system can improve the system resource utilization by about 20% when meeting the microservice SLA of edge computing.}
}
@article{CUI2024103822,
title = {Optical amplification across the whole communication windows in PbS quantum-dot-doped fiber},
journal = {Optical Fiber Technology},
volume = {88},
pages = {103822},
year = {2024},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2024.103822},
url = {https://www.sciencedirect.com/science/article/pii/S1068520024001676},
author = {Junjie Cui and Daoyuan Chen and Zaijin Fang and Xiaofeng Liu and Zhi Chen and Jianrong Qiu and Beibei Xu},
keywords = {PbS quantum dots, Glass fiber, Ultra-broadband optical amplification},
abstract = {The surge of big data, cloud computing, and AI bring in explosive requirement of data transmission capacities. However, the transmission capacity of traditional single-mode-fiber already reaches its limit due to the narrow emission bandwidth of rare-earth-ion. Here, we achieved ultra-broadband optical amplification in O + E + S + C + L bands for the first time in PbS quantum-dot-doped glass fiber with the maximum gain of 5 dB at 1370 nm under 100 mW pump. The mechanism of the broadband optical amplification, the advantages of quantum dot-doped fiber amplifier (QDFA), and a new measurement scheme for QDFA are systematically discussed. It is believed that in the future, the further optimization of the composition and architectures of the glass fiber, and the preparation method of quantum dot-doped fiber may lead to important applications in optical interconnection, artificial intelligence, Internet of things, mobile electronics and so on.}
}
@article{PR2024100490,
title = {A heart disease prognosis pipeline for the edge using federated learning},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {7},
pages = {100490},
year = {2024},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2024.100490},
url = {https://www.sciencedirect.com/science/article/pii/S277267112400072X},
author = {Mahalingam P․R․ and Dheeba J․},
keywords = {Cloud computing, Machine learning, Serverless architectures, Edge computing, Heart diseases, Neural networks, Internet of things},
abstract = {Cloud computing and edge computing have revolutionized deployments by giving virtually unlimited computing and storage to ensure the scalability and availability of applications. This paper explores an application that can be used as a decision support system for heart disease prognosis. It discusses deployment strategies on a cloud-native model and an edge-optimized model. The application contains a customized prediction pipeline named ClassifyIT with a custom neural network architecture called IPANN, supported by a feature selector named MIST-CC and a regularizer named STIR. ClassifyIT was observed to give an accuracy of 87.16% on the Cleveland dataset, compared to 78.80% for a regular deep network. The addition of the MIST-CC feature selection algorithm to the deep network was shown to improve its accuracy to 81.97%, and it is further enhanced to 85.54% by adding STIR. This pipeline is then deployed on an application based on a cloud-native architecture that uses microservices. The design is expanded to an edge-optimized architecture that improves scalability by moving part of the computation to the user device. The machine learning pipeline is further enhanced using federated learning to improve localization and collaborative learning. Both architectures are compared in a subjective fashion based on various parameters.}
}
@article{YU2024101095,
title = {Application of building BIM technology based on sensor systems and 5G cloud computing},
journal = {Measurement: Sensors},
volume = {33},
pages = {101095},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101095},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424000710},
author = {Xin Yu and Guoliang Ren},
keywords = {Voice system, Cloud computing, Architectural design, BIM technology},
abstract = {At present, the technology of artificial intelligence has been developing, making people's life more convenient and production more efficient. As the artificial intelligence software Siri is accepted and used by more and more people, some voice assistant software with similar functions gradually appear in people's vision, such as Sogou voice assistant, Xiaoai, Xiaodu and so on. ASR is the entrance of Siri and other software systems. The quality of software is directly affected by ASR. If there is no good ASR, it is impossible to have high-quality voice assistant applications. Therefore, we must correctly understand the important value of ASR and deeply study this technology in order to achieve better human-computer interaction. In recent years, the development of communication network technology is changing rapidly, from 2G to 5G, technology has been in progress. Compared with the previous 4G, 5G has a higher technical level, whether in network equipment or in data computing technology. For 5 g network, cloud computing technology is very important and has a great impact on it, so we need to focus on the research to realize the optimization and innovation development of 5G network. The construction sector constitutes a crucial segment of China's economy, exerting a significant influence on its overall growth trajectory. Given the prevailing developmental context, this document provides a concise overview of the BIM (Building Information Modeling) Technology, encompassing its fundamental concept, distinguishing attributes, and core principles. Furthermore, it delves into the manifold junctures where BIM Technology finds application across the entire construction life cycle. By employing an actual project as an illustrative instance, this study compiles a synopsis of BIM Technology's utilization at each phase, conducting a comprehensive evaluation of the project through the lens of the accrued experience and outcomes. This assessment lays the groundwork for an extensive exploration of potential expansion and application avenues, culminating in the eventual advancement of BIM Technology's achievements.}
}
@article{LIU2021102389,
title = {Model maturity-based model service composition in cloud environments},
journal = {Simulation Modelling Practice and Theory},
volume = {113},
pages = {102389},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102389},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X21000964},
author = {Ying Liu and Lin Zhang and Yongkui Liu and Yuanjun Laili and Weicun Zhang},
keywords = {Modeling and simulation (M&S), Cloud computing, MSaaS, Model service composition for simulation (MSCS), Model maturity, Evolutionary algorithm},
abstract = {With the development of cloud computing (CC), service-oriented architecture (SOA), and container technology, modeling and simulation (M&S) resources, such as simulation software and different sorts of models, can be shared and reused in a cloud environment. Modeling and Simulation as a Service (MSaaS), as a new paradigm, supports sharing simulation models or modeling tools and has enabled a wide range of model reuse. However, reusing or combining some immature models may result in inefficient M&S activities or even false simulation results. To make sure the appropriate reuse and composition of simulation models in cloud environments, which is also termed as model service composition for simulation (MSCS), this paper incorporates model maturity with service cooperation as a metric to evaluate the quality of model composition in cloud. Then, as a multi-objective optimization problem with multiple constraints, the MSCS problem and its process are described in detail. To solve the MSCS problem, a novel evolutionary algorithm named CA-AO-NSGAII is proposed. In the algorithm, adaptive crossover and mutation operators, as well as probabilistic initialization are developed. Furthermore, a half-local search algorithm in an elitist mechanism is designed for efficient decision-making. To validate the performance of CA-AO-NSGAII, experiments with respect to four different cases are conducted. Results show that the proposed method for addressing MSCS issue is effective and feasible.}
}
@article{LIU2022108488,
title = {A four-terminal-architecture cloud-edge-based digital twin system for thermal error control of key machining equipment in production lines},
journal = {Mechanical Systems and Signal Processing},
volume = {166},
pages = {108488},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108488},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021008311},
author = {Jialan Liu and Chi Ma and Hongquan Gui and Shilong Wang},
keywords = {Production line, Digital twin, Cloud computing, Edge computing, LSTM neural network},
abstract = {Production lines are important for the high-accuracy and efficient machining of parts. The thermal error of key machining equipment in production lines has a significant effect on the geometric accuracy of machined parts. To improve the geometric accuracy of machined parts, the thermal error of key machining equipment in a production line should be controlled. Then the collection, storage, analysis, and calculation of the large-volume manufacturing data are essential. But the processing involving the large-volume manufacturing data is time-consuming and challenging, which leads to low executing efficiency. To solve the problem that the system is inefficient in the processing of the large-volume manufacturing data, a four-terminal-architecture cloud-edge-based digital twin system (CEDTS) is proposed with a reasonable functional division of four terminals, and thus the executing efficiency of CEDTS is expedited. Then the error mechanism is studied to prove the long-term memorizing behavior, and an improved seagull optimization algorithm (ISOA) is proposed based on the chaos thought to optimize the weights, thresholds, and the number of iterations of an improved long short term memory (ILSTM) network with the attention mechanism. The ISOA-ILSTM error model is embedded into the intelligent decision-making terminal of CEDTS to predict the thermal error. Moreover, a comprehensive machining error model is proposed and embedded into the intelligent decision-making terminal of CEDTS to control the thermal error. Finally, the effectiveness of CEDTS is verified on a production line. The results show that the reduction of the large-volume manufacturing data for the collection, storage, analysis, and calculation is significant. With the implementation of CEDTS, the fluctuation range of geometric errors of machined parts is reduced significantly. The executing time is reduced by more than half by CEDTS with the GPU acceleration.}
}
@article{CORREIA2023103035,
title = {Monintainer: An orchestration-independent extensible container-based monitoring solution for large clusters},
journal = {Journal of Systems Architecture},
volume = {145},
pages = {103035},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.103035},
url = {https://www.sciencedirect.com/science/article/pii/S138376212300214X},
author = {Miguel Correia and Wellington Oliveira and José Cecílio},
keywords = {Monitoring, Container-based infrastructure, Cloud monitoring tools, Resource allocation, Internet of Things, Performance metrics},
abstract = {Container virtualization has recently gained popularity due to its low performance and resource allocation overhead. The rise of this technology can be attributed to the advancement of cloud computing and the adoption of micro-services architecture. These new approaches offer a more efficient and fine-grained system design through the benefits of containerization, such as isolation, portability, and improved performance. However, container-based systems have created new challenges in monitoring due to their automated flexibility, ephemerality, and the increasing number of containers in a system. So there is a practical need for effective monitoring and performance management tools. This paper analyses the key performance metrics for machine, container and application services, including CPU usage, memory usage, disk usage, and network usage. Furthermore, we review several widespread tools for collecting and monitoring these metrics and present the Monintainer tool. It is a solution designed to monitor entire container-based systems, from applications to their underlying infrastructure, allowing users to better understand their systems’ behavior in run-time. The tool’s results can aid container-based systems’ design, implementation and optimization.}
}
@article{CUI2021543,
title = {Cyber-Physical System (CPS) architecture for real-time water sustainability management in manufacturing industry},
journal = {Procedia CIRP},
volume = {99},
pages = {543-548},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.074},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003619},
author = {Xinyue Cui},
keywords = {Water sustainability, Water footprint (WF), Cyber-physical system (CPS), Real-time monitoring, Management & optimization, Manufacturing},
abstract = {Water sustainability is critical to environmental sustainability, social stability and economic development. A major barrier to realizing water sustainability in manufacturing is the onerous nature of data collection and subsequent impact evaluation. In response a cyber-physical-system (CPS) architecture for real-time water footprint (WF) assessment is proposed, in which in-line smart sensing techniques collect data related to water and wastewater, and cloud storage and cloud computing are used for WF calculation and sustainability evaluation. The dynamic capabilities of a CPS can assist industry to decrease water use and water pollution, to reduce costs, and to lower its environmental footprint.}
}
@article{ZHU2022134,
title = {A novel rate control algorithm for low latency video coding base on mobile edge cloud computing},
journal = {Computer Communications},
volume = {187},
pages = {134-143},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422000469},
author = {Jinlei Zhu and Houjin Chen and Pan Pan},
keywords = {Mobile edge computing, Cloud computing, Video coding, Rate control},
abstract = {The current low-latency video coding rate control algorithm has similar video sequence structure, low rate control accuracy, long control time, etc., which cannot meet the expectations of modern workers. Based on this, this paper proposes a mobile edge cloud computing-based Low-latency video coding rate control algorithm, optimize the original control algorithm, improve its rate control accuracy, etc. By analyzing the relationship between mobile edge computing and cloud computing, combined with edge computing in the context of cloud computing, this paper proposes a mobile edge cloud computing method and designs the mobile edge cloud computing architecture. Low latency video sequences are obtained using mobile edge cloud computing, and a mobile edge cloud computing model is established to control the update rate parameters of delayed video files. According to the principle of rate control, based on joint rate–distortion theory and frame coding complexity calculation model, the low latency video coding rate control is realized. The experimental results show that the byte data volume of the low-latency video coding sequence of the proposed method is 5653 dB. The rate control accuracy can be as high as 99.64%, and the rate control time is 1.37 s. The proposed method has a high similarity of video sequence structure and high rate control accuracy and can effectively shorten the rate control time of low latency video coding.}
}
@article{NEVES2021100475,
title = {Samsara architecture: Exploring situation awareness in cloud computing management},
journal = {Sustainable Computing: Informatics and Systems},
volume = {29},
pages = {100475},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100475},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920301980},
author = {Vilnei Neves and Marília Pit and Renata Reiser and Adenauer Yamin and Mauricio Pilla},
keywords = {Cloud computing, Energy efficiency, Situation awareness},
abstract = {Issues related to energy consumption and its efficiency in large-scale computing environments have emerged as critical points in the development of modern computer systems. This article presents the Samsara architecture, which aims to manage energy-efficient computational clouds. The architecture was developed exploring situation awareness strategies, operating autonomously and with minimal human intervention, essential aspects in massive data processing centers. Samsara has been implemented as one of the modules of the OpenStack cloud platform and considers the maximum allocation capacity of each physical machine, to consolidate the workloads. In the evaluations carried out with synthetic loads, reductions of up to 12.3% in the energy consumption in the managed computational infrastructure were achieved, demonstrating the potential of Samsara for the operation of computational clouds.}
}
@article{ZHANG2024116,
title = {Cloud-edge-end-based aircraft assembly production quality monitoring system framework and applications},
journal = {Journal of Manufacturing Systems},
volume = {75},
pages = {116-131},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001304},
author = {Qiang Zhang and Yifan Zhang and Qun Luo and Cijun Yu and Ningdong Yu and Qing Wang and Yinglin Ke},
keywords = {Cloud-Edge-End, Edge computing, Aircraft assembly, Online monitoring, Machine learning},
abstract = {In recent years, the increasing complexity of quality control in aircraft assembly has necessitated the development of innovative and integrated solutions. This paper introduces a novel framework based on cloud-edge-end architecture to enhance quality monitoring systems in aircraft assembly production. The framework consists of five parts: cloud, edge, end, network, and the trained model. By leveraging the advantages of cloud computing, edge computing, and terminal devices, this framework aims to provide a comprehensive, real-time, and efficient quality monitoring solution. Key technologies within the cloud-edge-end framework are proposed, including data collection, storage, and transmission, deep learning methods based on Channel Attention Convolutional Neural Network (CACNN), and feedback mechanisms from the cloud layer to the lower layers. These innovations enable effective information sharing and utilization at various stages of the aircraft assembly process, providing operational managers with timely and accurate insights into assembly quality. A prototype system application has been implemented at an aircraft assembly site, facilitating process data monitoring and assembly status analysis. Experimental validation of the proposed CACNN algorithm has demonstrated its capability to monitor hole diameters online. Various case studies confirm the framework's effectiveness, showcasing substantial improvements in monitoring accuracy and operational efficiency.}
}
@article{GUPTA2024268,
title = {Neural network inspired efficient scalable task scheduling for cloud infrastructure},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {4},
pages = {268-279},
year = {2024},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667345224000051},
author = {Punit Gupta and Arnaav Anand and Pratyush Agarwal and Gavin McArdle},
keywords = {HS, ANN, Task scheduling, Metaheuristic, Genetic algorithm, Cloud infrastructure},
abstract = {The rapid development of Cloud Computing in the 21st Century is landmark occasion, not only in the field of technology, but also in the field of engineering and services. The development in cloud architecture and services has enabled fast and easy transfer of data from one unit of a network to other. Cloud services support the latest transport services like smart cars, smart aviation services and many others. In the current trend, smart transport services depend on the performance of cloud Infrastructure and its services. Smart cloud services derive real time computing and allows it to make smart decision. For further improvement in cloud services, cloud resource optimization is a vital cog that defines the performance of cloud. Cloud services have certainly aimed to make the optimum use of all available resources to the become as cost efficient and time efficient as possible. One of the issues that still occur in multiple Cloud Environments is a failure in task execution. While there exist multiple methods to tackle this problem in task scheduling, in the recent times, the use of smart scheduling techniques has come to prominence. In this work, we aim to use the Harmony Search Algorithm and neural networks to create a fault aware system for optimal usage of cloud resources. Cloud environments are in general expected to be free of any errors or faults but with time and experience, we know that no system can be faultless. With our approach, we are looking to create the best possible time-efficient system for faulty environments, Where the result shows that the proposed harmony search-inspired ANN model provides least execution time, number of task failures, power consumption and high resource utilization as compared to recent Red fox and Crow search inspired models.}
}
@article{MENDES202416,
title = {MAS-Cloud+: A novel multi-agent architecture with reasoning models for resource management in multiple providers},
journal = {Future Generation Computer Systems},
volume = {154},
pages = {16-34},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004776},
author = {Aldo H.D. Mendes and Michel J.F. Rosa and Marcelo A. Marotta and Aleteia Araujo and Alba C.M.A. Melo and Célia Ghedini Ralha},
keywords = {Cloud computing, Heuristics, Optimization, Metaheuristics, Multi-agent system, Resource provisioning},
abstract = {Nowadays, scientific and commercial applications are often deployed to cloud environments requiring multiple resource types. This scenario increases the necessity for efficient resource management. However, efficient resource management remains challenging due to the complex nature of modern cloud-distributed systems since resources involve different characteristics, technologies, and financial costs. Thus, optimized cloud resource management to support the heterogeneous nature of applications balancing cost, time, and waste remains a challenge. Multi-agent technologies can offer noticeable improvements for resource management, with intelligent agents deciding on Virtual Machine (VM) resources. This article proposes MAS-Cloud+, a novel agent-based architecture for predicting, provisioning, and monitoring optimized cloud computing resources. MAS-Cloud+ implements agents with three reasoning models including heuristic, formal optimization, and metaheuristic. MAS-Cloud+ instantiates VMs considering Service Level Agreement (SLA) on cloud platforms, prioritizing user needs considering time, cost, and waste of resources providing appropriate selection for evaluated workloads. To validate MAS-Cloud+, we use a DNA sequence comparison application subjected to different workload sizes and a comparative study with state-of-the-art work with Apache Spark benchmark applications executed on the AWS EC2. Our results show that to execute the sequence comparison application, the best performance was obtained by the optimization model, whereas the heuristic model presented the best cost. By providing the choice among multiple reasoning models, our results show that MAS-Cloud+ could provide a more cost-effective selection of the instances reducing ≈58% of execution average cost of WorkdCount, Sort, and PageRank BigDataBench benchmarking workloads. As for the execution time, the WorkdCount and PageRank present reduction, the latter with ≈58%. The results indicate a promising solution for efficient cloud resource management.}
}
@incollection{BASU2024445,
title = {Chapter 16 - Discussions on the IIoT, Industrie 4.0, and digital transformation},
editor = {Swapan Basu},
booktitle = {Plant Intelligent Automation and Digital Transformation},
publisher = {Academic Press},
pages = {445-479},
year = {2024},
isbn = {978-0-12-824457-9},
doi = {https://doi.org/10.1016/B978-0-12-824457-9.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244579000030},
author = {Swapan Basu},
keywords = {AI, AR, Big data, Cloud, CPS, Cultural role, Customer interaction, Digital transformation, Digital twin, Digitalization, Edge, IoT, ML, MQTT, Predictive maintenance, RPA},
abstract = {Discussions in this chapter start with the basics. Conceptual discussions on digital transformation include: explanation and comparison of digitization, digitalization digital transformation (DT), DT path and business issues, DT framework, decision-making; advanced analytics, IIoT and Industrie 4.0, edge and cloud computing with individual characteristics, and comparisons between the two, including benefits of both along with the associated selection process. Discussions also cover inventory optimization, predictive demand and maintenance, robotic process automation, augmented reality, artificial intelligence, machine learning, and digital twins. Under DT and Industrie 4.0 philosophy issues such as various driving technologies, DT in manufacturing, and manufacturing edges have been included. As a part of DT, cultural role, customer interaction, predictive maintenance, product quality improvement, MQTT, DT requirements, DT-driven changes, decision-making, automation pyramid, automation impact, secured data delivery, successful road map, strategic decision-making, phases of DT, IIoT ecosystems, etc. have been included. Discussions also focus on Industrie 4.0, including: integration of OT and IT. As part of Industrie 4.0 discussions also include issues like: smart system, new paradigm, data analysis, CPS horizontal and vertical integration, etc., which have been discussed at length. The discussions also cover a reference architectural model for Industrie 4.0 (RAMI) including the concept, its characteristics, and structural details of a 3D model. Also, discussions on a new business model concept, new ecosystem for services, and production system are included in the chapter.}
}
@article{ZHENG202088,
title = {5G network-oriented hierarchical distributed cloud computing system resource optimization scheduling and allocation},
journal = {Computer Communications},
volume = {164},
pages = {88-99},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420319447},
author = {Guang Zheng and Hao Zhang and Yanling Li and Lei Xi},
keywords = {5G network, Cloud computing, Dynamic resource scheduling, Resource allocation},
abstract = {As the core technology of the next generation mobile communication system, the development of 5G key technologies needs to be able to efficiently and effectively support massive data services. Aiming at the impact of massive data traffic on mobile communication networks in 5G communication systems, this paper proposes a 5G-oriented hierarchical distributed cloud service mobile communication system architecture. The model consists of a cloud access layer, a distributed micro-cloud system, and a core cloud data center. The distributed micro cloud system consists of multiple micro clouds that are deployed to the edge of the network. The service content in the core cloud data center can be deployed and cached to the local micro cloud server in advance to reduce repeated redundant transmission of user requested content in the network. Aiming at the problem of how to determine the migration object when dynamically optimizing the resource structure, a heuristic function-based dynamic optimization algorithm for cloud resources is proposed. The experimental results show that the dynamic expansion algorithm of cloud resources based on dynamic programming ideas can better improve the performance of virtual resources, and the dynamic optimization algorithm of cloud resources based on heuristic functions can effectively and quickly optimize the resource structure, thereby improving the operating efficiency of user virtual machine groups. An efficient resource allocation scheme based on cooperative Q (Quality) learning is proposed. The environmental knowledge obtained by the base station learning and exchanging information is used for distributed resource block allocation. This resource allocation scheme can obtain the optimal resource allocation strategy in a short learning time, and can terminate the learning process at any time according to the delay requirements of different services. Compared with traditional resource allocation schemes, it can effectively improve system throughput.}
}
@article{CHEN2024113388,
title = {Edge–cloud collaborative estimation lithium-ion battery SOH based on MEWOA-VMD and Transformer},
journal = {Journal of Energy Storage},
volume = {99},
pages = {113388},
year = {2024},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2024.113388},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X24029748},
author = {Yuan Chen and Xiaohe Huang and Yigang He and Siyuan Zhang and Yujing Cai},
keywords = {Lithium-ion battery, State of health, Multi-population evolution whale optimization algorithm, Variational mode decomposition, Transformer},
abstract = {The State of Health (SOH) of lithium-ion batteries significantly impacts the performance, safety, and reliability of the battery, making it a crucial component of the battery management system. Addressing the issues of inadequate accuracy and lack of robustness in current SOH estimation methods, this study introduces a novel methodology for estimating SOH in lithium-ion batteries. It leverages the multi-population evolution whale optimization algorithm optimized variational mode decomposition (MEWOA-VMD) in conjunction with Transformer architecture. This framework enhances the efficiency and accuracy of SOH estimation by leveraging the computational capabilities of edge devices for real-time data processing, as well as the robust data processing power and model training advantages offered by cloud computing. Specifically, MEWOA is utilized to optimize VMD parameters, enabling MEWOA-VMD to fully decompose the capacity signal of lithium-ion batteries. This results in a component showing a global attenuation trend and a set of fluctuating components that represent capacity regeneration, thereby minimizing the impact of capacity regeneration on SOH estimation. Subsequently, all components are collectively input into the Transformer, marking the first application of this method for input. To enhance convergence speed and training efficiency, the layer normalization (LN) layer within the neural network architecture is proactively advanced. Finally, various artificial neural networks are compared and validated on three publicly available datasets. Furthermore, Gaussian noise is introduced into the original data to validate robustness. To confirm the practical applicability of the proposed method, real-world vehicle data is used for SOH estimation. The results indicate that the proposed method achieves a maximum MSE of no more than 0.009% across three publicly available datasets, showcasing improved accuracy and stability in SOH estimation. The practical applicability is further validated using real-world vehicle data, proving the proposed method’s potential for application in edge cloud-based battery management systems.}
}
@article{GARCIA202254,
title = {An ultra-compact and high-speed FFT-based large-integer multiplier for fully homomorphic encryption using a dual spike-based arithmetic circuit over GF(p)},
journal = {Neurocomputing},
volume = {507},
pages = {54-66},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222009948},
author = {Luis Garcia and Eduardo Vazquez and Gabriel Sanchez and Juan-Gerardo Avalos and Giovanny Sanchez},
keywords = {Finite-field adder, Finite-field adder multiplier, Spiking neural P system, Dendritic delay, Dendritic growth, Dendritic pruning, Dendritic delays, Dendritic feedback, Astrocyte-like control, FPGA, Fast fourier transform},
abstract = {During last years, fully homomorphic encryption (FHE) has attracted great interest since enables computation on encrypted data and thus can be considered as a potential solution in advanced applications, such as privacy-preserving cloud computing, genomics, electronic voting and bio-metric authentication. Nowadays, software simulations of the FHE schemes on general purpose computers are extremely slow. Therefore, the development of specific hardware architectures open up new horizons in efficient simulation of FHE schemes. Until date, the implementation of FHE schemes in embedded devices remain impractical due to very high processing time and area consumption required to process very large-integer numbers. Specifically, the development of efficient very large-integer finite-field adder and multiplier potentially allows the throughput and area consumption to be improved since these circuits are the most used components in the computation of FHE schemes. This work presents, for the first time, the development of a compact and highly dual finite-field circuit based on spiking neural P systems (SN P), dendritic growth, dendritic pruning, communication on request and structural plasticity. Specifically, this circuit performs either the finite-field addition or multiplication of two variable integers by only reconnecting their synapses dynamically. Hence, the proposed circuit performs both operations employing the same neural network. In this way, we achieve a significant improvement in terms of area consumption and throughput. Since the computation of FHE schemes requires the multiplication of very large-integer numbers, we use the proposed dual finite-field circuit as the basic processing unit to create a very large-integer multiplier based on fast Fourier transform (FFT). Specifically, the creation of the proposed very large-integer multiplier has allowed us to accelerate the key generation and encryption processes involved in the computation of FHE algorithm. This multiplier was implemented in scalable compact neuromorphic architecture, which mimic the dynamic dendritic phenomena, such as dendritic growth and dendritic pruning. To achieve this, we propose a dynamic multiplexing mechanism based on simple multiplexers and an optimized control unit. This have allowed us to significantly improve the area consumption compared with previously reported solution.}
}
@incollection{MARINESCU202341,
title = {Chapter 3 - Parallel processing and distributed computing},
editor = {Dan C. Marinescu},
booktitle = {Cloud Computing (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {41-94},
year = {2023},
isbn = {978-0-323-85277-7},
doi = {https://doi.org/10.1016/B978-0-32-385277-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323852777000105},
author = {Dan C. Marinescu},
keywords = {fine-grained parallelism, coarse-grained parallelism, bit-level and instruction-level parallelism, CPI—cycles per instruction, pipelined execution, structural hazards, dynamic instruction scheduling, chaining of vector operations, the roofline performance model},
abstract = {Computer clouds are large-scale distributed systems that are collections of autonomous and heterogeneous systems. Cloud organization is based on the experience accumulated since the first electronic computer was used to solve computationally challenging problems. This chapter overviews the concepts in parallel and distributed systems important for understanding the basic challenges of the design and use of computer clouds. Hardware parallelism is critical for the performance of single-core processors, multicore processors, systems on a chip, multiprocessor systems, clusters, and warehouse-scale computers, which are the backbone of computer clouds. The first sections cover parallel and distributed system hardware, stressing the quantitative rather than qualitative aspects of computer architecture. Basic architectural concepts of modern computer systems, optimization of computer architecture including caching, out-of-order instruction execution, dynamic scheduling, branch predictions, and ARM architecture are discussed in Sections 3.1, 3.2, and 3.3, respectively. Sections 3.4, 3.5, 3.6, and 3.7 cover SIMD architectures, GPUs (Graphics Processing Units), TPUs (Tensor Processing Units), and SoCs (Systems On a Chip) and edge cloud computing. Section 3.8 covers data, task, and thread-level parallelism. Extracting parallelism depends on the application; Sections 3.9 and 3.10 analyze the speedup limits given by Amdahl's law, Amdahl's law for multicore processors, and scaled speedup. Section 3.11 reviews the evolution of most powerful computing systems, from supercomputers to large-scale distributed systems. Organization principles for distributed systems, such as modularity and layering, presented in Sections 3.12 and 3.13, are applied to the design of the peer-to-peer and large-scale systems discussed in Sections 3.14 and 3.15, respectively. Section 3.16 presents composability bounds and scalability, and Section 3.17 surveys fallacies concerning distributed computing. Finally, Section 3.18 discusses blockchain technologies and applications. History notes, further readings, and exercises and problems in Sections 3.19 and 3.20 conclude the chapter.}
}
@article{CHEN2024120502,
title = {Fast multi-type resource allocation in local-edge-cloud computing for energy-efficient service provision},
journal = {Information Sciences},
volume = {668},
pages = {120502},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120502},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524004158},
author = {Yishan Chen and Shumei Ye and Jianqing Wu and Bi Wang and Hui Wang and Wei Li},
keywords = {Local-edge-cloud, Multi-type, Latency, Resource, Energy, Differential evolution},
abstract = {With the advancement of information technology, the concept of local-edge-cloud computing has gained prominence. Operating on a collaborative model, heterogeneous computing nodes converge to deliver a spectrum of multi-type services, including calculation-intensive, latency-sensitive, and privacy-requiring services. This collaborative approach fosters high-quality development in power economy. However, the proliferation of heterogeneous computing nodes, while beneficial, introduces challenges. The intricate connections and limited energy supply may lead to interruptions in the service processes of nodes. In this study, we present an energy-efficient resource allocation scheme designed for low-latency multi-type service provision within a local-edge-cloud collaboration. Our methodology focuses on optimizing the performance of multi-type service provision in a local-edge-cloud network, taking into account considerations such as latency, resource allocation, and energy consumption. To accomplish this, we employ the Alopex-based Differential Evolution algorithm. Initially, we construct three sub-models to analyze latency and energy aspects across various computing modes. Subsequently, we formulate a constrained optimization problem aimed at minimizing both latency and energy consumption in multi-type service provisioning. These models seek to derive optimal resource allocation decisions for the given scenario. To address this optimization problem, we introduce the hybrid differential evolution algorithm, Alopex-DE. A formal analysis is conducted to showcase its near-optimal performance in comparison to three state-of-the-art algorithms. Additionally, extensive simulations are carried out to validate the superior effectiveness of our proposed approach.}
}
@article{WANG2023384,
title = {Design of intelligent water transport logistics management system based on cloud computing},
journal = {Desalination and Water Treatment},
volume = {314},
pages = {384-394},
year = {2023},
issn = {1944-3986},
doi = {https://doi.org/10.5004/dwt.2023.30052},
url = {https://www.sciencedirect.com/science/article/pii/S1944398624015212},
author = {Jin Wang},
keywords = {Intelligent water transport logistics, System design, Cloud computing platform, User experience evaluation, System optimization},
abstract = {ABSTRACT
With the rapid development of science and technology, intelligent logistics plays an increasingly important role in various fields. This study focuses on the design, application and optimization of intelligent logistics system. Through empirical analysis and model construction, the design and division of system architecture are deeply discussed, and the efficient operation of the system is realized with the support of cloud computing platform. By collecting and preprocessing a large amount of actual data, a complete system modeling scheme is constructed in this study. Then, relying on user feedback, the system is deeply evaluated and continuously optimized to achieve a more user-friendly intelligent logistics service. The research results show that through scientific system design and model construction, combined with advanced cloud computing technology and user experience evaluation, the performance and application value of intelligent logistics system can be significantly improved.}
}
@article{SUN2022108119,
title = {Design of cross-cultural teaching management system for international students based on cloud service platform},
journal = {Computers and Electrical Engineering},
volume = {102},
pages = {108119},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108119},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200372X},
author = {Taiwei Sun},
keywords = {Cloud services, Sensor design, Secure shell, Video internet of things, Teaching resource management},
abstract = {Recently online teaching resources management has become an interesting domain in China due to emerging technologies like Video Internet of Things (VIoT). We propose a novel integrated framework for efficient teaching resources management using VIoT and cloud computing services. The proposed model designs a VIoT system to analyze the need for front-end portals. We employ a cloud platform for international students based on business goals and functional requirements to optimize sensor design. The proposed model first builds the VIoT front-end web portal to elaborate the design schemes, architecture, and various design ideas. Then, we integrate the VIoT system with other cloud-based modules such as the teaching management module, application management module, and system-object module. We represent the proposed model from three perspectives like Secure Shell (SSH) framework implementation, interfaces, and certain function modules. The experimental outcomes demonstrate that the system developed in this study has a specific influence.}
}
@article{SU2023154562,
title = {Reconfigurable multi-core array architecture and mapping method for RNS-based homomophic encryption},
journal = {AEU - International Journal of Electronics and Communications},
volume = {161},
pages = {154562},
year = {2023},
issn = {1434-8411},
doi = {https://doi.org/10.1016/j.aeue.2023.154562},
url = {https://www.sciencedirect.com/science/article/pii/S1434841123000365},
author = {Yang Su and Bailong Yang and Jianfei Wang and Fahong Zhang and Chen Yang},
keywords = {Full homomorphic Encryption, Reconfigurable architecture, Multi-core array, RNS-based CKKS, NTT/INTT},
abstract = {Fully homomorphic encryption (FHE) plays a vital role in privacy-preserving outsourcing computing and cloud computing security. However, efficiency is still the main factor limiting the actual use of FHE. This paper presents an area-efficient reconfigurable multi-core array architecture (named RMCA) and mapping method for the RNS variant of CKKS scheme. To accelerate the time-consuming polynomial multiplication, we present an improved NTT/INTT algorithm without pre/post-processing and a reconfigurable processing element (PE) unit that can be configured as NTT, INTT and modular multiplier. Also, a memory-saving NTT architecture and memory organization of the twiddle factor are introduced to reduce the data and twiddle factor memory overhead by 25 % and 50 %, respectively. Furthermore, targeting the computational requirements of RNS-CKKS, a unified computational model and distributed on-chip memory organization are presented for RMCA. Lastly, all the computational units involved in the homomorphic evaluation of RNS-CKKS are optimized and mapped on RMCA reasonably. When evaluated on Virtex UltraScale XCVU190 FPGA at 300 MHz, RMCA can perform 9154, 4308 and 1743 homomorphic multiplications per second for N = 4096, 8192 and 16384, respectively, and the area-time-products (ATPs) are improved by 1.11×∼8.60 × for a wide range of parameter sets.}
}
@article{MOREIRA2024110434,
title = {5G and edge: A reinforcement learning approach for Virtual Network Embedding with cost optimization and improved acceptance rate},
journal = {Computer Networks},
volume = {247},
pages = {110434},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110434},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624002664},
author = {Cristiano L. Moreira and Carlos A. Kamienski and Reinaldo A.C. Bianchi},
keywords = {Network virtualization, 5G network slicing, Cloud computing, Edge computing, Virtual Network Embedding, Artificial Intelligence, Reinforcement Learning},
abstract = {5G technologies are fueling a revolution across numerous industries, including manufacturing, healthcare, and entertainment, by enabling the development and deployment of novel applications at the network’s edge. To meet the demanding service level agreements of these industries, a dynamic and adaptable infrastructure strategy that combines cloud and edge computing models is needed. This hybrid approach offers the benefits of both centralized cloud processing and decentralized edge computing, optimized for responsiveness and efficiency. A key element for success is an orchestration mechanism that dynamically allocates resources to ensure the infrastructure can adapt to fluctuating demands in real time, optimizing resource utilization and meeting SLA requirements. Among these mechanisms, virtual network embedding (VNE) and dynamic resource management (DRM) have emerged as tools for defining where and how edge technology should be used. However, current VNE approaches struggle to adapt to real-time fluctuations in demand across geographically distributed edge resources. This work introduces a novel resource allocation algorithm, the VNE-CRS, which uses an Artificial Intelligence technique called Reinforcement Learning to orchestrate resources across multiple domains. This approach benefits from the strength of Reinforcement Learning: its ability to consider the entire problem from beginning to end while incorporating various aspects of 5G Quality of Service Indicators for optimal decision-making. Experiments were conducted to evaluate the performance of VNE-CRS against state-of-the-art algorithms for multi-domain edge environments. Results have shown that employing Reinforcement Learning techniques for VNE resource allocation yields performance gains of 12.32 percentage points in comparison with the GRC algorithm and 28.80 percentage points in comparison with the base edge environment, presenting an acceptability rate closer to the Public Cloud environment with all benefits of edge environment. In conclusion, VNE-CRS offers an efficient solution for resource allocation in 5G environments, achieving superior performance and transforming the VNE architecture into a comprehensive orchestration system that optimizes infrastructure utilization for strategic long-term benefits.}
}
@article{SHENG2022108254,
title = {Learning to schedule multi-NUMA virtual machines via reinforcement learning},
journal = {Pattern Recognition},
volume = {121},
pages = {108254},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108254},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004349},
author = {Junjie Sheng and Yiqiu Hu and Wenli Zhou and Lei Zhu and Bo Jin and Jun Wang and Xiangfeng Wang},
keywords = {Dynamic virtual machine scheduling, Multi-NUMA, Reinforcement learning, Cloud computing},
abstract = {With the rapid development of cloud computing, the importance of dynamic virtual machine scheduling is increasing. Existing works formulate the VM scheduling as a bin-packing problem and design greedy methods to solve it. However, cloud service providers widely adopt multi-NUMA architecture servers in recent years, and existing methods do not consider the architecture. This paper formulates the multi-NUMA VM scheduling into a novel structured combinatorial optimization and transforms it into a reinforcement learning problem. We propose a reinforcement learning algorithm called SchedRL with a delta reward scheme and an episodic guided sampling strategy to solve the problem efficiently. Evaluating on a public dataset of Azure under two different scenarios, our SchedRL outperforms FirstFit and BestFit on the fulfill number and allocation rate.}
}
@article{MA2020123155,
title = {Data-driven sustainable intelligent manufacturing based on demand response for energy-intensive industries},
journal = {Journal of Cleaner Production},
volume = {274},
pages = {123155},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123155},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620332005},
author = {Shuaiyin Ma and Yingfeng Zhang and Yang Liu and Haidong Yang and Jingxiang Lv and Shan Ren},
keywords = {Data-driven, Sustainable intelligent manufacturing, Demand response, Particle swarm optimisation, Energy-intensive industries, Circular economy},
abstract = {The circular economy plays an important role in energy-intensive industries, aiming to contribute to ethical sustainable societal development. Energy demand response is a key actor for cleaner production and circular economy strategy. In the Industry 4.0 context, the advanced technologies (e.g. cloud computing, Internet of things, cyber-physical system, digital twin and big data analytics) provide numerous opportunities for the implementation of a cleaner production strategy and the development of intelligent manufacturing. This paper presented a framework of data-driven sustainable intelligent/smart manufacturing based on demand response for energy-intensive industries. The technological architecture was designed to implement the proposed framework, and multi-level demand response models were developed based on machine, shop-floor and factory to save energy cost. Finally, an application of ball mills in a slurry shop-floor of a partner company was presented to demonstrate the proposed framework and models. Results showed that the energy efficiency of ball mills can be greatly improved. The energy cost of the slurry shop-floor saved approximately 19.33% by considering electricity demand response using particle swarm optimisation. This study provides a practical approach to make effective and energy-efficient decisions for energy-intensive manufacturing enterprises.}
}
@article{BASAHEL20224319,
title = {Enhanced Coyote Optimization with Deep Learning Based Cloud-Intrusion Detection System},
journal = {Computers, Materials and Continua},
volume = {74},
number = {2},
pages = {4319-4336},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.033497},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822005161},
author = {Abdullah M. Basahel and Mohammad Yamin and Sulafah M. Basahel and E. Laxmi Lydia},
keywords = {Intrusion detection system, cloud security, coyote optimization algorithm, class imbalance data, deep learning},
abstract = {Cloud Computing (CC) is the preference of all information technology (IT) organizations as it offers pay-per-use based and flexible services to its users. But the privacy and security become the main hindrances in its achievement due to distributed and open architecture that is prone to intruders. Intrusion Detection System (IDS) refers to one of the commonly utilized system for detecting attacks on cloud. IDS proves to be an effective and promising technique, that identifies malicious activities and known threats by observing traffic data in computers, and warnings are given when such threats were identified. The current mainstream IDS are assisted with machine learning (ML) but have issues of low detection rates and demanded wide feature engineering. This article devises an Enhanced Coyote Optimization with Deep Learning based Intrusion Detection System for Cloud Security (ECODL-IDSCS) model. The ECODL-IDSCS model initially addresses the class imbalance data problem by the use of Adaptive Synthetic (ADASYN) technique. For detecting and classification of intrusions, long short term memory (LSTM) model is exploited. In addition, ECO algorithm is derived to optimally fine tune the hyperparameters related to the LSTM model to enhance its detection efficiency in the cloud environment. Once the presented ECODL-IDSCS model is tested on benchmark dataset, the experimental results show the promising performance of the ECODL-IDSCS model over the existing IDS models.}
}
@article{DU2024103001,
title = {Joint optimization of offloading strategy and resource allocation for multi-user in dynamic vehicular edge computing systems},
journal = {Simulation Modelling Practice and Theory},
volume = {136},
pages = {103001},
year = {2024},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2024.103001},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X24001151},
author = {Zhuocheng Du and Yuanzhi Ni and Hongfeng Tao and Mingfeng Yin},
keywords = {Internet of vehicle, Mobile edge computing, Cloud computing, Resource allocation, Offloading strategy},
abstract = {Internet of Vehicles (IoV) relies heavily on its computing capability to facilitate various vehicular applications. Since the cloud computing or mobile edge computing (MEC) only cannot satisfy the latency requirement due to the limitation of the resource coverage, the cloud–edge-end cooperative computing has become an emerging paradigm. A comprehensive IoV architecture is considered and a joint optimization problem is formulated to minimize the system function value. To optimize the resource allocation and the task offloading strategy, the simulated spring system algorithm (SSSA) is designed where the initial problem is decoupled into two sub-problems with priority. The first one is to allocate computing resources based on KKT conditions, thus the individual optimal solution is achieved. The second one is solved based on the idea of simulated spring system such that the task offloading strategy is obtained. Two sub-problems iterate mutually to update each other until finishing the binary tree traversal. Thus, the proposed solution adapts to various conditions and the computational complexity is also reduced compared with traditional methods. Simulation verifies that the proposed algorithm reduces the maximum system function value by about 31% compared with the benchmark methods and performs efficiently in various road conditions.}
}
@article{JOSEG2024103773,
title = {Self-Attention conditional generative adversarial network optimised with crayfish optimization algorithm for improving cyber security in cloud computing},
journal = {Computers & Security},
volume = {140},
pages = {103773},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103773},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824000749},
author = {Sahaya Stalin {Jose. G} and G. Sugitha and Ayshwarya {Lakshmi. S} and Preethi Bangalore Chaluvaraj},
keywords = {Cloud computing, Crayfish optimization algorithm, NSL-KDD benchmark dataset, Reformed phase conserving vibrant range compression filter, Manta ray foraging optimization algorithm and self-attention conditional generative adversarial network},
abstract = {The decentralized and distributed architecture of cloud computing promotes adoption and growth in various societal domains, including education, government, information technology, business, entertainment. Cloud computing (CC) makes a broad range of information technologies available. Security and privacy are key challenges in storing big data in the cloud. To overcome this challenge, a Self-Attention Conditional Generative Adversarial Network Optimised with Crayfish Optimization Algorithm for Improving Cyber Security in Cloud Computing (CybS-CC-SACGANCOA) is proposed in this paper to enhance the safety of CC environment. The input data is amassed from NSL-KDD database. After that, the data is fed to pre-processing segment. The segment of pre-processing eliminates Cloud data termination and missing value replacement using Reformed Phase Conserving Vibrant Range Compression filter. The results of pre-processing serves for feature selection. The optimal features are selected by means of Manta Ray Foraging Optimization Algorithm (MRFOA). Cloud data is categorized as normal and abnormal data, like Denial-of-Service (DoS), Probe, Remote to local attack (R2L), User to root attack (U2R) by the help of SACGAN. Crayfish Optimization Algorithm (COA) is proposed to optimize the SACGAN classifier that classifies anomaly data precisely. The proposed CSCC-SACGANCOA technique is activated in Python under some metrics. The proposed CSCC-SACGANCOA approach has attained higher detection accuracy, lower computation time, higher AUC, higher scalability and lower detection error rate compared to the existing methods.}
}
@article{GARCIAVALDEZ2021234,
title = {A container-based cloud-native architecture for the reproducible execution of multi-population optimization algorithms},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {234-252},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330235},
author = {Mario {García Valdez} and Juan J. {Merelo Guervós}},
keywords = {Multi-population, Nature-inspired algorithm, Parallel genetic algorithms, Cloud-computing, Event-driven architecture},
abstract = {Splitting a population into multiple instances is a technique used extensively in recent years to help improve the performance of nature-inspired optimization algorithms. Work on those populations can be done in parallel, and they can interact asynchronously, a fact that can be leveraged to create scalable implementations based on, among other methods, distributed, multi-threaded, parallel, and cloud-native computing. However, the design of these cloud-native, distributed, multi-population algorithms is not a trivial task. Using as a foundation monolithic (single-instance) solutions, adaptations at several levels, from the algorithmic to the functional, must be made to leverage the scalability, elasticity, (limited) fault-tolerance, reproducibility, and cost-effectiveness of cloud systems while, at the same time, conserving the intended functionality. Instead of an evolutive approach, in this paper, we propose a cloud-native optimization framework created from scratch, that can include multiple (population-based) algorithms without increasing the number of parameters that need tuning. This solution goes beyond the current state of the art, since it can support different algorithms at the same time, work asynchronously, and also be readily deployable to any cloud platform. We evaluate this solution’s performance and scalability, together with the effect other design parameters had on it, particularly the number and the size of populations with respect to problem size. The implemented platform is an excellent alternative for running locally or in the cloud, thus proving that cloud-native bioinspired algorithms perform better in their “natural” environment than other algorithms, and set a new baseline for scaling and performance of this kind of algorithms in the cloud.}
}
@article{VNREDDY2022100468,
title = {On optimization efficiency of scalability and availability of cloud-based software services using scale rate limiting algorithm},
journal = {Measurement: Sensors},
volume = {24},
pages = {100468},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100468},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422001027},
author = {Annapareddy {V N Reddy} and A. Arun Kumar and Nookala Venu and R. {Vijaya Kumar Reddy}},
keywords = {Cloud computing, Optimized network, TCP, Scaling, Synchronized monitoring},
abstract = {Internet applications nowadays combine globally shared resources into a single software platform. It's a difficult technology issue to supply reports for the resource consumption among those World Wide Web applications. The formulation and simulation of spread levels were introduced in this report. Spread frequency operates together again to impose a worldwide speed restriction over revenue aggregated at several locations allowing for the synchronized monitoring of an internet company's activity. Assures that traffic delays mass transit streams act as passing through a unique, common limitation network. We describe two models overall and the other TCP-optimized—that permits network operators to expressly balance away transmission cost with network correctness, speed, and scaling. All these approaches could speed restrict 1000s of streams with little expense (less than 3% in the tested configuration). We show that with us TCP-centric architecture could scale to hundreds of servers whilst remaining resilient towards both outages and transmission postponement, suitable for countrywide telecom operators.}
}
@article{ARJONA2023436,
title = {Transparent serverless execution of Python multiprocessing applications},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {436-449},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003612},
author = {Aitor Arjona and Gerard Finol and Pedro García López},
keywords = {Transparency, Access transparency, Serverless, FaaS, Multiprocessing, Parallel programming},
abstract = {Access transparency means that both local and remote resources are accessed using identical operations. With transparency, unmodified single-machine applications could run over disaggregated compute, storage, and memory resources. Hiding the complexity of distributed systems through transparency would have great benefits, like scaling-out local-parallel scientific applications over flexible disaggregated resources in the Cloud. This paper presents a performance evaluation where we assess the feasibility of access transparency over state-of-the-art Cloud disaggregated resources for Python multiprocessing applications. We have interfaced the multiprocessing module with an implementation that transparently runs processes on serverless functions and uses an in-memory data store for shared state. To evaluate transparency, we run in the Cloud four unmodified applications: Uber Research’s Evolution Strategies, Baselines-AI’s Proximal Policy Optimization, Pandaral.lel’s dataframe, and Scikit Learn’s Hyperparameter tuning. We compare execution time and scalability of the same application running over disaggregated resources using our library, with the single-machine Python multiprocessing libraries in a large VM. For equal resources, applications efficiently using message-passing abstractions achieve comparable results despite the significant overheads of remote communication. Other shared-memory intensive applications do not perform due to high remote memory latency. The results show that Python’s multiprocessing library design is an enabler towards transparency: legacy applications using efficient disaggregated abstractions can transparently scale beyond VM limited resources for increased parallelism without changing the underlying code or architecture.}
}
@incollection{HE2022101,
title = {Chapter 6 - AI-driven BIM on the cloud},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {101-117},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000093},
author = {Wanyu He and Jackie Yong Leong Shong and Chuyu Wang},
keywords = {Artificial intelligence (AI), Building information modeling (BIM), Cloud computing, AI-driven BIM on cloud, XKool},
abstract = {Recent changes in China's land supply and real estate-related policies have brought new demands and pressures to major stakeholders of construction projects, especially planners and designers. The application of building information modeling (BIM) in the initial investment and planning stages of the project may help solve this problem, but its high cost of adoption, the limitations of BIM software, and the lack of awareness of the owners have restricted the adoption of BIM in Chinese urban and architectural projects—even for the earlier phases of the project. Therefore, a new BIM paradigm AI-driven BIM on Cloud (ABC) attempts to solve these emerging problems by integrating BIM with big data, artificial intelligence, and cloud computing. The integration of these advanced technologies realizes the generation of planning or design schemes through multidimensional data consideration, multiobjective optimization, and more cost-effective computing resources. In this chapter, we illustrate several ABC-assisted construction projects in China.}
}
@article{DISTEFANO202291,
title = {Improving QoS through network isolation in PaaS},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {91-105},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000188},
author = {Alessandro {Di Stefano} and Antonella {Di Stefano} and Giovanni Morana},
keywords = {Cloud computing, Platform as a service, Kubernetes orchestration, Container placement and scheduling, Network optimization, Micro-services architecture},
abstract = {The lack of knowledge about the architecture and dynamics of the hosted applications represents a significant limit for the QoS management, both on IaaS- and PaaS-based clouds. The interference among virtual machines, the heterogeneity of cloud clusters, the different resource requirements of micro-services based applications, and the network asymmetry make it difficult to find an effective mapping between the applications and the clouds’ resources. This paper aims to show how to improve the estimation of an applications’ dynamics within the cloud, combining the exploitation of the inherent flexibility of the containerization architecture with the availability of information about the structure of workflows. The authors propose two parameters, isolation index and closeness, to measure the degree of mutual influence among the components of distributed applications. These parameters will be the base for a deployment strategy aimed at minimizing the mutual interference among the applications and making their performance more predictable. The problem will be formulated through a IQP-based algorithm and its effectiveness will be shown by empirical tests and numerical simulations.}
}
@article{NIE2023102543,
title = {A multi-agent and cloud-edge orchestration framework of digital twin for distributed production control},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {82},
pages = {102543},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102543},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000194},
author = {Qingwei Nie and Dunbing Tang and Changchun Liu and Liping Wang and Jiaye Song},
keywords = {Heterogeneous multi-agent system, Digital twin, Cloud production line, Cloud-edge orchestration, Long short-term memory},
abstract = {The demands for mass individualization and networked collaborative manufacturing are increasing, bringing significant challenges to effectively organizing idle distributed manufacturing resources. To improve production efficiency and applicability in the distributed manufacturing environment, this paper proposes a multi-agent and cloud-edge orchestration framework for production control. A multi-agent system is established both at the cloud and the edge to achieve the operation mechanism of cloud-edge orchestration. By leveraging Digital Twin (DT) technology and Industrial Internet of Things (IIoT), real-time status data of the distributed manufacturing resources are collected and processed to perform the decision-making and manufacturing execution by the corresponding agent with permission. Based on the generated data of distributed shop floors and factories, the cloud production line model is established to support the optimal configuration of the distributed idle manufacturing resources by applying a systematic evaluation method and digital twin technology, which reflects the actual manufacturing scenario of the whole production process. In addition, a rescheduling decision prediction model for distributed control adjustment on the cloud is developed, which is driven by Convolutional Neural Network (CNN) combined with Bi-directional Long Short-Term Memory (BiLSTM) and attention mechanism. A self-adaptive strategy that makes the real-time exceptions results available on the cloud production line for holistic rescheduling decisions is brought to make the distributed manufacturing resources intelligent enough to address the influences of different degrees of exceptions at the edge. The applicability and efficiency of the proposed framework are verified through a design case.}
}
@article{OLARIU20233649,
title = {Challenges In Optimizing Migration Costs From On-Premises To Microsoft Azure},
journal = {Procedia Computer Science},
volume = {225},
pages = {3649-3659},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.360},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015181},
author = {Florin Olariu and Lenuța Alboaie},
keywords = {Cloud Migration, Modular monolith, Lift and Shift, Clean Architecture, Optimizing costs, Optimizing resources, Azure Pricing},
abstract = {The current paper analyzes the feasibility of a modular web application's migration procedure from on-premises to the cloud. Our focus is on identifying cost savings and options for hosting the application in the cloud. The research specifically examines the impact of architectural decisions records (ADR) on a modular monolith use case, utilizing .NET Core for the backend and Angular for the front end, with Clean Architecture as the design pattern. We investigate different cloud models (IaaS, PaaS, Serverless) considering project management's triple constraints (time, cost, performance). The study demonstrates that a modular monolith can be migrated with varying effort and costs depending on the chosen cloud model and technology stack. We explore optimizations such as resource utilization, licensing fees, and cost reduction through infrastructure reservations. Our findings show that the migration cost can range from a 20% increase with IaaS to approximately 70% cost reduction with the Serverless strategy compared to the on-premises environment, using equivalent resources. We also explore methods to lower expenses for each model, including resource modifications, Linux operating systems, and longer resource reservations. Considering limitations, we propose a two-stage migration strategy: initially lifting and shifting the application to IaaS with cost optimization, and subsequently migrating to PaaS for scalability and simplified resource management in the long term.}
}
@article{SOTO202326,
title = {JACC-FPGA: A hardware accelerator for Jaccard similarity estimation using FPGAs in the cloud},
journal = {Future Generation Computer Systems},
volume = {138},
pages = {26-42},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002643},
author = {Javier E. Soto and Cecilia Hernández and Miguel Figueroa},
keywords = {Hardware acceleration, Cloud computing, Field-programmable gate arrays, Streaming algorithms, Sketches, Genomics},
abstract = {Genomic similarity is a key metric in genomics, used in important tasks such as genome clustering and metagenomic profiling. One commonly-used approach is to treat each genome as a set of k-mers and to compute the Jaccard coefficient between each genome pair. However, computing the Jaccard coefficient between genomes in a large dataset is a computationally-challenging task. In this paper, we present an algorithm and accelerator architecture that uses an FPGA-as-a-service paradigm to compute the Jaccard similarity between pairs of genomes in large datasets using sketches and hardware acceleration in the cloud. The algorithm can compute the similarity between all genome pairs in the dataset, or it can use a selection criterion to reduce the amount of computation when only genome pairs with a Jaccard coefficient above a user-supplied threshold are of interest. After building the sketches, our heterogeneous accelerator can compute more than 96 million Jaccard coefficients per second running on an AWS EC2 f1.2xlarge instance with a Xilinx XCVU9P FPGA, which is 58 times faster than a state-of-the art software implementation that exploits SIMD instructions and thread-level parallelism on a compute-optimized EC2 c5.9xlarge instance with 36 hardware threads. The accelerator also computes similarities 27 times faster than a straightforward GPU-accelerated implementation of the Jaccard coefficient using sketches, and 4 times faster than an optimized GPU implementation of our algorithm, both running on an EC2 g5.4xlarge instance tailored with an NVIDIA A10G GPU. Furthermore, using a Jaccard coefficient threshold of 0.8 reduces the execution time of similarity computation to approximately one third in the hardware-accelerated and parallel software implementations, compared to computing the complete similarity matrix.}
}
@article{MORARIU2020103244,
title = {Machine learning for predictive scheduling and resource allocation in large scale manufacturing systems},
journal = {Computers in Industry},
volume = {120},
pages = {103244},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103244},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519311595},
author = {Cristina Morariu and Octavian Morariu and Silviu Răileanu and Theodor Borangiu},
keywords = {Large scale manufacturing system, Big data streaming, Cloud manufacturing, Machine learning, Long short-term memory neural network, Prediction, Classifier, Anomaly detection},
abstract = {The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud. Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data.}
}
@incollection{MARINESCU2018365,
title = {Chapter 10 - Cloud Resource Virtualization},
editor = {Dan C. Marinescu},
booktitle = {Cloud Computing (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {365-402},
year = {2018},
isbn = {978-0-12-812810-7},
doi = {https://doi.org/10.1016/B978-0-12-812810-7.00013-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128107000133},
author = {Dan C. Marinescu},
keywords = {Hypervisor, Hypercalls, Virtual machine, Virtual Machine Control Structure (VMCS), Address space compression, Interrupt virtualization, Virtual-Machine Based Rootkit (VMBR), Zero-copy semantics},
abstract = {Virtualization, a basic tenet of cloud computing, simplifies some of the resource management tasks. For example, the state of a virtual machine (VM) running under a hypervisor can be saved and migrated to another server to balance the load. At the same time, virtualization allows users to operate in environments they are familiar with, rather than forcing them to work in idiosyncratic environments. This chapter starts with a discussion of virtualization principles, and motivation for virtualization focused on performance and security isolation, and of alternatives for the implementation of virtualization. Two distinct approaches for processor virtualization, full virtualization and paravirtualization are presented. Full virtualization is feasible when the hardware abstraction provided by the hypervisor is an exact replica of the physical hardware while paravirtualization requires modifications of the guest operating system. The x86 processor architecture was extended to provide hardware support for virtualization. Next the Xen hypervisor and an optimization of its network performance are analyzed. KVM, a virtualization infrastructure of the Linux kernel and nested virtualization is followed by the presentation of a trusted kernel virtualization. Nested virtualization allows hypervisors to run inside a VM complicating even further the virtualization landscape. Itanium paravirtualization, performance degradation in a VM environment due to cache misses, open source software platforms for virtualization, the potential risks of virtualization, and an overview of the virtualization software are the other topics covered in this chapter.}
}
@article{KUMAR20181037,
title = {Analysis and design of an optimized secure auditing protocol for storing data dynamically in cloud computing},
journal = {Materials Today: Proceedings},
volume = {5},
number = {1, Part 1},
pages = {1037-1047},
year = {2018},
note = {International Conference on Processing of Materials, Minerals and Energy (July 29th – 30th) 2016, Ongole, Andhra Pradesh, India},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2017.11.180},
url = {https://www.sciencedirect.com/science/article/pii/S2214785317324446},
author = {Raman Kumar and Gurpreet Singh},
keywords = {Cloud Computing, Communication overhead, Time cost of individual client, Packet delivery ratio, Energy level, Average delay, Packet delivery time and Throughput},
abstract = {The remote server (Cloud Service Provider (CSP)) store their data on cloud servers and users can access their data from cloud servers while implementing the concept of cloud computing. Because of some security constraints in data outsourcing, the latest concept of data hosting service also arises new security challenges, those challenges can be handled by third party auditing service to check the data integrity in the cloud server. There are are few existing remote integrity checking methods those can serve for static stored data but not able to work dynamically. In this paper, we develop a three-tier security architecture for storing multimedia files which include role base access control, encryption, and signature verification. Therefore, an enhanced secure dynamic auditing protocol is proposed, which can store data correctly in the cloud. In the proposed scheme, both the combiner and the third party auditor (TPA) can verify the integrity of the information that they are receiving from each other. Therefore, the proposed an optimized secure dynamic auditing protocol is secure and efficient against various conspiracy attacks.}
}
@article{CHHABRA2018683,
title = {A Probabilistic Model for Finding an Optimal Host Framework and Load Distribution in Cloud Environment},
journal = {Procedia Computer Science},
volume = {125},
pages = {683-690},
year = {2018},
note = {The 6th International Conference on Smart Computing and Communications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.12.088},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917328569},
author = {Sakshi Chhabra and Ashutosh Kumar Singh},
keywords = {Cloud computing, load balancing, task deployment, probability, performance, optimality},
abstract = {Cloud Computing is being widely accepted in this computing world and comprises of many hardware and software resources. It becomes critical to meet their client’s needs on time if the cloud data centers are so overloaded. In this regard, if the utilization of resources becomes an intelligent way so that we focused on the selection problem of the physical hosts for deploying the tasks. So, OPH-LB (Optimal Physical Host with effective Load Balancing) framework is proposed to model the service of client’s requests in this IaaS architecture with heterogeneous virtual machines. It proposes the thought of accomplishing load balancing in this dynamic environment. Firstly, this OPH-LB approach is filtering the qualified hosts among all which accomplishes the requirement of deploying tasks. Then out of those qualified sets, we apply probablistic model which helps to find the most optimal host in terms of its computing capability and its performance function. Further, the performance is analyzed using Cloudsim simulation tool and compared with existing approaches. The results demonstrate that our model has improved the throughput, reduced the failure rate and optimized the attainment of cloud data centers.}
}
@article{CASALICCHIO2018211,
title = {Research challenges in legal-rule and QoS-aware cloud service brokerage},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {211-223},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16306641},
author = {Emiliano Casalicchio and Valeria Cardellini and Gianluca Interino and Monica Palmirani},
keywords = {Cloud computing, Autonomic computing, Legislation compliance checking, Optimization, Quality of service, Monitoring, Service migration, Service portability},
abstract = {The ICT industry and specifically critical sectors, such as healthcare, transportation, energy and government, require as mandatory the compliance of ICT systems and services with legislation and regulation, as well as with standards. In the era of cloud computing, this compliance management issue is exacerbated by the distributed nature of the system and by the limited control that customers have on the services. Today, the cloud industry is aware of this problem (as evidenced by the compliance program of many cloud service providers), and the research community is addressing the many facets of the legal-rule compliance checking and quality assurance problem. Cloud service brokerage plays an important role in legislation compliance and QoS management of cloud services. In this paper we discuss our experience in designing a legal-rule and QoS-aware cloud service broker, and we explore relate research issues. Specifically we provide three main contributions to the literature: first, we describe the detailed design architecture of the legal-rule and QoS-aware broker. Second, we discuss our design choices which rely on the state of the art solutions available in literature. We cover four main research areas: cloud broker service deployment, seamless cloud service migration, cloud service monitoring, and legal rule compliance checking. Finally, from the literature review in these research areas, we identify and discuss research challenges.}
}
@article{SHAHRYARI202018,
title = {An SDN based framework for maximizing throughput and balanced load distribution in a Cloudlet network},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {18-32},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19322447},
author = {Shirzad Shahryari and Seyed-Amin Hosseini-Seno and Farzad Tashtarian},
keywords = {SDN, Mobile Cloud Computing (MCC), Cloudlet, Load balancing, Mobile Edge computing (MEC)},
abstract = {Although mobile devices have experienced voluminous proliferation throughout the last decade, there are limited resources in terms of their portable size. Such limitations could be mitigated by remote execution of the computation-intensive tasks to the cloud. By creating a cluster of servers (a.k.a. “Cloudlets”) to the network edge and close to the mobile devices, task offloading could be performed with a more acceptable delay in comparison with a cloud-based solution. Nevertheless, once the user requests mount, the resource constraints in a Cloudlet will lead to resource shortages. However, this challenge can be obviated using a network of Cloudlets for sharing their resources. This paper proposes a novel framework to optimally manage the resources and balance an equitable load across a network of Cloudlets via software-defined networking (SDN) techniques. To achieve this, firstly, the problem is considered as a mixed-integer linear programming (MILP) optimization model in order to balance the distribution of independent tasks offloaded from the mobile devices along with optimal use of resources. The MILP model guarantees meeting the tasks’ deadlines and maximizes overall system throughput. Secondly, by showing that the addressed problem is NP-hard, an LP-relaxation model is proposed to enable the SDN controller on a large-scale network. Finally, we conduct experiments by emulating the proposed framework in Mininet-WiFi, with the Floodlight usage as the SDN controller. The simulation results indicate that the proposed architecture can achieve a significant throughput maximization of a system, which satisfactorily performs load balancing, and offers adequate proof, as well.}
}
@article{JENA20151219,
title = {Multi Objective Task Scheduling in Cloud Environment Using Nested PSO Framework},
journal = {Procedia Computer Science},
volume = {57},
pages = {1219-1227},
year = {2015},
note = {3rd International Conference on Recent Trends in Computing 2015 (ICRTC-2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.419},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915019481},
author = {R.K. Jena},
keywords = {Task Scheduling, Cloud Computing, Multi-Objective optimisation, CloudSim, PSO},
abstract = {Cloud computing is an emerging computing paradigm with a large collection of heterogeneous autonomous systems with flexible computational architecture. Task scheduling is an important step to improve the overall performance of the cloud computing. Task scheduling is also essential to reduce power consumption and improve the profit of service providers by reducing processing time. This paper focuses on task scheduling using a multi-objective nested Particle Swarm Optimization(TSPSO) to optimize energy and processing time. The result obtained by TSPSO was simulated by an open source cloud platform (CloudSim). Finally, the results were compared to existing scheduling algorithms and found that the proposed algorithm (TSPSO) provide an optimal balance results for multiple objectives.}
}
@article{BUI2017103,
title = {Energy efficiency for cloud computing system based on predictive optimization},
journal = {Journal of Parallel and Distributed Computing},
volume = {102},
pages = {103-114},
year = {2017},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2016.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0743731516301708},
author = {Dinh-Mao Bui and YongIk Yoon and Eui-Nam Huh and SungIk Jun and Sungyoung Lee},
keywords = {Energy efficiency, IaaS cloud computing, Predictive analysis, Convex optimization, Gaussian process},
abstract = {In recent years, power consumption has become one of the hottest research trends in computer science and industry. Most of the reasons are related to the operational budget and the environmental issues. In this paper, we would like to propose an energy-efficient solution for orchestrating the resource in cloud computing. In nature, the proposed approach firstly predicts the resource utilization of the upcoming period based on the Gaussian process regression method. Subsequently, the convex optimization technique is engaged to compute an appropriate quantity of physical servers for each monitoring window. This quantity of interest is calculated to ensure that a minimum number of servers can still provide an acceptable quality of service. Finally, a corresponding migrating instruction is issued to stack the virtual machines and turn off the idle physical servers to achieve the objective of energy savings. In order to evaluate the proposed method, we conduct the experiments using synthetic data from 29-day period of Google traces and real workload from the Montage open-source toolkit. Through the evaluation, we show that the proposed approach can achieve a significant result in reducing the energy consumption as well as maintaining the system performance.}
}
@article{SUN2016146,
title = {ROAR: A QoS-oriented modeling framework for automated cloud resource allocation and optimization},
journal = {Journal of Systems and Software},
volume = {116},
pages = {146-161},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001715},
author = {Yu Sun and Jules White and Sean Eade and Douglas C. Schmidt},
keywords = {Cloud computing, Resource optimization, Load testing and benchmarking},
abstract = {Cloud computing offers a fast, easy and cost-effective way to configure and allocate computing resources for web applications, such as consoles for smart grid applications, medical records systems, and security management platforms. Although a diverse collection of cloud resources (e.g., servers) is available, choosing the most optimized and cost-effective set of cloud resources for a given web application and set of quality of service (QoS) goals is not a straightforward task. Optimizing cloud resource allocation is a critical task for offering web applications using a software as a service model in the cloud, where minimizing operational cost while ensuring QoS goals are met is critical to meeting customer demands and maximizing profit. Manual load testing with different sets of cloud resources, followed by comparison of test results to QoS goals is tedious and inaccurate due to the limitations of the load testing tools, challenges characterizing resource utilization, significant manual test orchestration effort, and challenges identifying resource bottlenecks. This paper introduces our work using a modeling framework – ROAR (Resource Optimization, Allocation and Recommendation System) to simplify, optimize, and automate cloud resource allocation decisions to meet QoS goals for web applications, including complex multi-tier application distributed in different server groups. ROAR uses a domain-specific language to describe the configuration of the web application, the APIs to benchmark and the expected QoS requirements (e.g., throughput and latency), and the resource optimization engine uses model-based analysis and code generation to automatically deploy and load test the application in multiple resource configurations in order to derive a cost-optimal resource configuration that meets the QoS goals.}
}
@article{NVHATKAR20221906,
title = {Optimal container resource allocation in cloud architecture: A new hybrid model},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {5},
pages = {1906-1918},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819307190},
author = {Kapil {N. Vhatkar} and Girish P. Bhole},
keywords = {Cloud computing, Container resource allocation, Microservices, Optimization, Lion algorithm, Whale optimization algorithm},
abstract = {A huge variety of fields and industries depend upon cloud computing based microservice due to its high-performance capability. Also, the merit of container usage is enormous; it enable larger portability, easier and faster deployment and restricted overheads. However, the rapid evolution causes issues in terms of container automation and management, Till now, a number of research works has concentrated on solving the open issues in container automation and management. In fact, container resource allocation is the major key hole for cloud providers since it directly influences the resource consumption and system performance. In this manner, this paper introduces a new optimized container resource allocation model by proposing a new optimization concept. To make the possibility of optimal container resource allocation, a new hybridized algorithm is implanted; namely, Whale Random update assisted Lion Algorithm (WR-LA), which is the hybrid form of Lion Algorithm (LA) and Whale Optimization Algorithm (WOA) is introduced. Moreover, the solution of optimized resource allocation is made by considering objectives like Threshold Distance, Balanced Cluster Use, System Failure, and Total Network Distance, respectively. Finally, the performance of the proposed model is compared over other conventional models and proves its superiority.}
}
@article{YU201951,
title = {Components and Development in Big Data System: A Surveyb},
journal = {Journal of Electronic Science and Technology},
volume = {17},
number = {1},
pages = {51-72},
year = {2019},
issn = {1674-862X},
doi = {https://doi.org/10.11989/JEST.1674-862X.80926105},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300060},
author = {Jing-Huan Yu and Zi-Meng Zhou},
keywords = {Big Data, cloud computing, data analysis, optimization, system architecture},
abstract = {With the growth of distributed computing systems, the modern Big Data analysis platform products often have diversified characteristics. It is hard for users to make decisions when they are in early contact with Big Data platforms. In this paper, we discussed the design principles and research directions of modern Big Data platforms by presenting research in modern Big Data products. We provided a detailed review and comparison of several state-of-the-art frameworks and concluded into a typical structure with five horizontal and one vertical. According to this structure, this paper presents the components and modern optimization technologies developed for Big Data, which helps to choose the most suitable components and architecture from various Big Data technologies based on requirements.}
}
@article{YANG202051,
title = {Task offloading for directed acyclic graph applications based on edge computing in Industrial Internet},
journal = {Information Sciences},
volume = {540},
pages = {51-68},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520305685},
author = {Lei Yang and Changyi Zhong and Qiuhui Yang and Wanrong Zou and Ahmed Fathalla},
keywords = {Edge computing, Cloud computing, Industrial Internet, Task offloading, Multi-objective optimization},
abstract = {With an increase in the number of devices involved in the Industrial Internet, effectively combining the characteristics of industrial scenarios with an edge computing methodology for computation-intensive applications poses a critical challenge. This paper proposes an integrated architecture that allows industrial devices to offload tasks to cloud or edge servers. An offloading problem is also formulated into an energy-cost (EC) minimization problem while satisfying the deadline constraint. To solve the optimization problem, two types of offloading algorithms, namely ASO and Pro-ITGO, are proposed based on the integrated architecture. The ASO algorithm is a lightweight linear programming algorithm that includes subdeadline allocation, topology sorting, and task offloading sub-algorithms. The Pro-ITGO algorithm is a group intelligence heuristic algorithm that is derived from the original ITGO algorithm adapting the offloading scenarios of the Industrial Internet. Experimental results demonstrate that compared with state-of-the-art heuristic algorithms, the proposed algorithms can effectively reduce the energy consumption of industrial devices and cloud computing costs.}
}
@article{RAEI2019103,
title = {SeCARA: A security and cost-aware resource allocation method for mobile cloudlet systems},
journal = {Ad Hoc Networks},
volume = {86},
pages = {103-118},
year = {2019},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S157087051830859X},
author = {Hassan Raei and Ensieh Ilkhani and Morteza Nikooghadam},
keywords = {Mobile cloud computing, Cloudlet, Resource allocation, Security-aware, Cost optimization problem, Analytical performance model},
abstract = {In the cloudlet architecture of Mobile Cloud Computing (MCC), a number of heavy jobs are offloaded to a private and local cloud, known as cloudlet, to overcome the lack of battery and computation resources of mobile devices. In this architecture, the cloudlet with the help of some public clouds provides the computational resources in the form of virtual machines (VMs) on physical machines (PMs). The users’ requests, based on their security requirements, are classified into different levels which require a certain number of PMs and VMs. On the other hand, the financial budgets of the cloudlet to afford these computational resources are restricted. In addition to the mentioned trade-off between security and cost, other trade-offs emerge among some components of the cost. For example, utilizing a large number of PMs of the cloudlet and public clouds decreases the penalty cost of request rejection; however, other associated costs, such as infrastructure, power consumption, and cooling, augment significantly. Therefore, to minimize the overall cost and to guarantee the security requirements, finding the optimal number of required PMs in all participant clouds, including the cloudlet and public clouds, is recognized as an important issue. To deal with this issue, in the current paper, a security and cost-aware resource allocation (SeCARA) method is proposed. Specifically, the proposed method converts this issue into two cost optimization problems. To compute the components of the cost, the proposed method exploits an analytical performance-security model of a cloudlet to obtain some necessary performance measures, such as request rejection probability. To achieve these significant measures, Markov reward model (MRM) is appropriately used by the performance-security model. Moreover, simulated annealing algorithms are applied to solve the optimization problems. Numerical results demonstrate the strength of the proposed method to identify the optimal number of PMs, for different security types of requests, within an acceptable duration of the time.}
}
@incollection{LI2016215,
title = {Chapter 9 - System Optimization for Big Data Processing},
editor = {Rajkumar Buyya and Rodrigo N. Calheiros and Amir Vahid Dastjerdi},
booktitle = {Big Data},
publisher = {Morgan Kaufmann},
pages = {215-238},
year = {2016},
isbn = {978-0-12-805394-2},
doi = {https://doi.org/10.1016/B978-0-12-805394-2.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805394200009X},
author = {R. Li and X. Dong and X. Gu and Z. Xue and K. Li},
keywords = {Big Data, Hadoop, Performance optimization, MapReduce, Job scheduling, HDFS},
abstract = {With the development and popularization of cloud computing, the Internet-of-Things, and mobile communications, the amount of data grows faster than ever before, which drives people to step into the age of Big Data. Hadoop and MapReduce programming frameworks have powerful data processing capability, and become more mature solutions in the field of text analysis, natural language processing, and business data processing. Therefore, Hadoop has become a key component of the Big Data processing system. Although Hadoop can support large-scale parallel data processing, there are still drawbacks to its underlying architecture and processing model in product environments, which caused the bottleneck in processing efficiency and performance. In terms of these shortcomings, various optimization techniques of Hadoop are proposed accordingly. In this chapter, the basic framework of the Hadoop ecosystem is described first. Next, we discuss the optimization of the parallel computing framework MapReduce, along with job scheduling, HDFS, HBase, etc. Finally, the directions of future optimization are discussed.}
}
@article{JAYASENA2017135,
title = {Multi-modal Multimedia Big Data Analyzing Architecture and Resource Allocation on Cloud Platform},
journal = {Neurocomputing},
volume = {253},
pages = {135-143},
year = {2017},
note = {Learning Multimodal Data},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.11.077},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217304411},
author = {K.P.N. Jayasena and Lin Li and Qing Xie},
keywords = {Multi-modal, Multimedia big data, Resource allocation, Cloud computing, Hadoop & MapReduce, ACO},
abstract = {Multimedia big data analyzing is the new topic that focus on all features of distributed computing systems that contains of a combination of text, visual and audio modalities. The traditional method to transcoding multi-modal multimedia big data needs expensive hardware and the amount of data increases transcoding executes a significant burden on the computing infrastructure. Therefore we illustrate a novel implementation for multimedia big data analyzing and data distribution. Our proposed architecture contains three layers such as service layer, platform layer and infrastructure layer. We design and implement the platform layer of the system by using a MapReduce framework running on a hadoop distributed file system (HDFS) and the media processing libraries Xuggler. In this way, our proposed system reduces the time for transcoding large amounts of data into specific formats depending on the user requirements. It provides flexible multimedia record/write interface and we can build large scale multimedia big data analytic applications based on Hadoop cloud platform. Moreover, we proposed the ant colony optimization (ACO) algorithm for efficient resource allocation in infrastructure layer. The simulation results demonstrate that the proposed algorithm can optimally allocate VM to achieve a minimal response time.}
}
@article{ELSHERBINY201833,
title = {An extended Intelligent Water Drops algorithm for workflow scheduling in cloud computing environment},
journal = {Egyptian Informatics Journal},
volume = {19},
number = {1},
pages = {33-55},
year = {2018},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866517302219},
author = {Shaymaa Elsherbiny and Eman Eldaydamony and Mohammed Alrahmawy and Alaa Eldin Reyad},
keywords = {Cloud computing, Scheduling algorithms, Resource management, Intelligent Water Drops, Workflow scheduling, Natural-based algorithms},
abstract = {Cloud computing is emerging as a high performance computing environment with a large scale, heterogeneous collection of autonomous systems and flexible computational architecture. Many resource management methods may enhance the efficiency of the whole cloud computing system. The key part of cloud computing resource management is resource scheduling. Optimized scheduling of tasks on the cloud virtual machines is an NP-hard problem and many algorithms have been presented to solve it. The variations among these schedulers are due to the fact that the scheduling strategies of the schedulers are adapted to the changing environment and the types of tasks. The focus of this paper is on workflows scheduling in cloud computing, which is gaining a lot of attention recently because workflows have emerged as a paradigm to represent complex computing problems. We proposed a novel algorithm extending the natural-based Intelligent Water Drops (IWD) algorithm that optimizes the scheduling of workflows on the cloud. The proposed algorithm is implemented and embedded within the workflows simulation toolkit and tested in different simulated cloud environments with different cost models. Our algorithm showed noticeable enhancements over the classical workflow scheduling algorithms. We made a comparison between the proposed IWD-based algorithm with other well-known scheduling algorithms, including MIN-MIN, MAX-MIN, Round Robin, FCFS, and MCT, PSO and C-PSO, where the proposed algorithm presented noticeable enhancements in the performance and cost in most situations.}
}
@article{LOPEZRIQUELME2017123,
title = {A software architecture based on FIWARE cloud for Precision Agriculture},
journal = {Agricultural Water Management},
volume = {183},
pages = {123-135},
year = {2017},
note = {Special Issue: Advances on ICTs for Water Management in Agriculture},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2016.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S0378377416304061},
author = {J.A. López-Riquelme and N. Pavón-Pulido and H. Navarro-Hellín and F. Soto-Valles and R. Torres-Sánchez},
keywords = {Precision Agriculture, FIWARE, Cloud Computing, WSN},
abstract = {Using suitable information storage, management and processing resources is essential when Precision Agriculture-based applications are developed. Nowadays, traditional client-server paradigm is useful but it might not be enough for this purpose. The amount of data that could be stored and processed, and the need of generating complex knowledge and rules that allow stakeholders to take appropriate decisions related to crop optimization are leading researchers to pay attention to new solutions based on designing software architectures in the Cloud. This paper demonstrates that using cloud services in the agronomic context could be considered as highly beneficial. In particular, the used cloud provider is FIWARE, since it provides open source and free development modules, and even, several enablers for agriculture. An application has been developed by using the FIWARE components, and it has been validated in real crops located in a semiarid area of the South of Spain with the aim of reducing the amount of water necessary for irrigation tasks. The advantages of using FIWARE, opposite to the use of traditional systems, are properly analysed and highlighted. In addition, a discussion that emphasizes the advantages of using FIWARE instead of other well-known cloud providers is also presented.}
}
@article{CAMPA2014354,
title = {Parallel patterns for heterogeneous CPU/GPU architectures: Structured parallelism from cluster to cloud},
journal = {Future Generation Computer Systems},
volume = {37},
pages = {354-366},
year = {2014},
note = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.12.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X14000041},
author = {Sonia Campa and Marco Danelutto and Mehdi Goli and Horacio González-Vélez and Alina Madalina Popescu and Massimo Torquati},
keywords = {Parallel patterns, Algorithmic skeletons, Structured parallelism, Heterogeneous architectures, GPU, Performance models, Cluster computing, Cloud computing},
abstract = {The widespread adoption of traditional heterogeneous systems has substantially improved the computing power available and, in the meantime, raised optimisation issues related to the processing of task streams across both CPU and GPU cores in heterogeneous systems. Similar to the heterogeneous improvement gained in traditional systems, cloud computing has started to add heterogeneity support, typically through GPU instances, to the conventional CPU-based cloud resources. This optimisation of cloud resources will arguably have a real impact when running on-demand computationally-intensive applications. In this work, we investigate the scaling of pattern-based parallel applications from physical, “local” mixed CPU/GPU-clusters to a public cloud CPU/GPU infrastructure. Specifically, such parallel patterns are deployed via algorithmic skeletons to exploit a peculiar parallel behaviour while hiding implementation details. We propose a systematic methodology to exploit approximated analytical performance/cost models, and an integrated programming framework that is suitable for targeting both local and remote resources to support the offloading of computations from structured parallel applications to heterogeneous cloud resources, such that performance values not available on local resources may be actually achieved with the remote resources. The amount of remote resources necessary to achieve a given performance target is calculated through the performance models in order to allow any user to hire the amount of cloud resources needed to achieve a given target performance value. Thus, it is therefore expected that such models can be used to devise the optimal proportion of computations to be allocated on different remote nodes for Big Data computations. We present different experiments run with a proof-of-concept implementation based on FastFlow  on small departmental clusters as well as on a public cloud infrastructure with CPU and GPU using the Amazon Elastic Compute Cloud. In particular, we show how CPU-only and mixed CPU/GPU computations can be offloaded to remote cloud resources with predictable performances and how data intensive applications can be mapped to a mix of local and remote resources to guarantee optimal performances.}
}
@article{JIANG2020158,
title = {The construction of smart city information system based on the Internet of Things and cloud computing},
journal = {Computer Communications},
volume = {150},
pages = {158-166},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.10.035},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419312034},
author = {Dingfu Jiang},
keywords = {Cloud computing, Internet of things, Smart city, Information isolated island},
abstract = {With the rapid development and deep application and cooperation of new concepts and technologies brought by the Internet of Things and cloud computing all over the world, all walks of life have gradually moved towards a ”smart” modern society. These technologies have gradually penetrated into the field of smart cities. The traditional urban system, the system that has been handed down since ancient times, has a very inefficient and cumbersome mode of operation, and the information between the systems has not been effectively shared and interconnected. In order to solve this series of problems, this study first studies the development of the Internet of Things, cloud computing-related technologies and smart cities, and then focuses on the key technologies of the Internet of Things and cloud computing in the field of structure and application. Under the support of these two technologies, proposed a smart city system based on Internet and cloud computing. System architecture, application system design, application support platform, various transmission networks and typical sensors are studied in detail and on different levels. In smart city systems based on the Internet of Things, sensor networks are often placed in unreliable communication environments, and this usually causes the transmission of information to fail. Whether the sensor chooses to transmit again after the information transmission fails is an optimized problem. This research study proposes a data aggregation algorithm based on Markov chain to solve the problem of transmitting such data again. The experimental results show that the system can realize information sharing, exchange and fusion between various sensing subsystems, solve the previous information island phenomenon and meet the actual needs of smart cities.}
}
@article{LEWIS2015158,
title = {Architectural tactics for cyber-foraging: Results of a systematic literature review},
journal = {Journal of Systems and Software},
volume = {107},
pages = {158-186},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001211},
author = {Grace Lewis and Patricia Lago},
keywords = {Mobile cloud computing, Cyber-foraging, Software architecture},
abstract = {Mobile devices have become for many the preferred way of interacting with the Internet, social media and the enterprise. However, mobile devices still do not have the computing power and battery life that will allow them to perform effectively over long periods of time, or for executing applications that require extensive communication, computation, or low latency. Cyber-foraging is a technique to enable mobile devices to extend their computing power and storage by offloading computation or data to more powerful servers located in the cloud or in single-hop proximity. This article presents the results of a systematic literature review (SLR) on architectures that support cyber-foraging. Elements of the identified architectures were codified in the form of Architectural Tactics for Cyber-Foraging. These tactics will help architects extend their design reasoning toward cyber-foraging as a way to support the mobile applications of the present and the future.}
}
@article{EVANGELINOU201567,
title = {A Joint Benchmark-Analytic Approach For Design-Time Assessment of Multi-Cloud Applications},
journal = {Procedia Computer Science},
volume = {68},
pages = {67-77},
year = {2015},
note = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.224},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030690},
author = {Athanasia Evangelinou and Michele Ciavotta and George Kousiouris and Danilo Ardagna},
keywords = {Benchmarking, Cloud applications, QoS, Model Driven Design.},
abstract = {Verifying that a software system shows certain non-functional properties is a primary concern for cloud applications. Given the heterogeneous technology offer and the pricing models currently available in the cloud market it is extremely complex to find the deployment that fits the application requirements and provides the best Quality of Service (QoS) and cost trade-offs. This task can be very challenging, even infeasible if performed manually, since the number of solutions may become extremely large depending on the number of possible providers and available technology stacks. Furthermore, with the increasing adoption of cloud computing, there is a need for fair evaluation of cloud systems. Today's cloud services differ among others by cost, performance, consistency guarantees, load-balancing, caching, fault tolerance, and SLAs. Moreover, cloud systems are inherently multi-tenant and their performance can vary over time, depending on the congestion level, provider policies, and the competition among running applications. System architects and developers are challenged with this variety of services and trade-offs. Hence, the purpose of a cloud benchmark should be to help developers when choosing the right architecture and services for their applications. In this paper we propose a joint benchmarking and optimization methodology to support the design and migration of legacy applications to Cloud. Our approach is effective in identifying the deployment of minimum costs, which provide also QoS guarantees.}
}
@article{ASSIS201651,
title = {A survey on cloud federation architectures: Identifying functional and non-functional properties},
journal = {Journal of Network and Computer Applications},
volume = {72},
pages = {51-71},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301436},
author = {M.R.M. Assis and L.F. Bittencourt},
keywords = {Cloud computing, Interconnected-clouds, Cloud federation},
abstract = {The cloud computing paradigm as originally conceived has reached a plateau of evolution, exposing several limitations that compromise the main features of the paradigm: resource contention, interruption of services, lack of interoperability in data representation, quality of service degradation, and others. Consequently, several new approaches to its use and optimization have been implemented to maintain continuity of technology. In this way, multiple clouds organizations have been formed with the objective of maximizing the use of cloud computing, in particular small- and medium-sized cloud providers who present difficulties to maintain all properties of the paradigm have mobilized themselves into organizations to maximize their revenues. Such organizations, formally called inter-clouds, have been gaining attention, where solutions like hybrid clouds, multi-clouds, and cloud federations are the main elements in the academic–scientific and industrial world. In particular, cloud federations are well behaved because organizations governed by a contract can be interesting and useful in many critical environments. However, there is a lack of works dedicated only to clouds federations. In addition, the existing works are not able to describe federations as unique inter-cloud entities to highlight specific properties and characteristics. In this paper, we present the desired functional and non-functional properties for cloud federations through the identification of the main architectures in the literature and we evaluate these architectures based on the described properties.}
}
@article{KRITIKOS2017206,
title = {Towards a security-enhanced PaaS platform for multi-cloud applications},
journal = {Future Generation Computer Systems},
volume = {67},
pages = {206-226},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16303880},
author = {Kyriakos Kritikos and Tom Kirkham and Bartosz Kryza and Philippe Massonet},
keywords = {Cloud, Security, Meta-model, Policies, Roles, SAML},
abstract = {Multi-cloud adaptive application provisioning can solve the vendor lock-in problem and allows optimising user requirements by selecting the best from the multitude of services offered by different cloud providers. To this end, such provisioning type is increasingly supported by new or existing research prototypes and platforms. One major concern, actually preventing users from moving to the cloud, comes with respect to security, which becomes more complex in multi-cloud settings. Such a concern spans two main aspects: (a) suitable access control on user personal data, VMs and platform services and (b) planning and adapting application deployments based on security requirements. As such, this paper addresses both security aspects by proposing a novel model-driven approach and architecture which secures multi-cloud platforms, enables users to have their own private space and guarantees that application deployments are not only constructed based on but can also maintain a certain user-required security level. Such a solution exploits state-of-the-art security standards, security software and secure model management technology. Moreover, it covers different access control scenarios involving external, web-based and programmatic user authentication.}
}
@article{NAWROCKI20171,
title = {Resource usage optimization in Mobile Cloud Computing},
journal = {Computer Communications},
volume = {99},
pages = {1-12},
year = {2017},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2016.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0140366416306673},
author = {Piotr Nawrocki and Wojciech Reszelewski},
keywords = {Mobile cloud computing, Resource usage, Load balancing, Multi-tenancy, Asynchronous processing},
abstract = {The majority of Mobile Cloud Computing architectures do not currently take into account resource usage or the costs of running the system in the Cloud, and an assumption is usually made that there is only one virtual machine per user. This approach blocks the large-scale adoption of Mobile Cloud Computing. We apply architectural patterns that are common in traditional Cloud Computing in order to decrease resource demand in Mobile Cloud Computing. We present and compare three architectures: the reference one, which represents the current common approach, and two architectures that leverage the multi-tenancy and asynchrony concepts in order to reduce cloud resource usage. The evaluation conducted has demonstrated that architectures using common Cloud Computing patterns can reduce the resources used by the current approach by 23 while serving 23 times more tasks. This proves that applying common Cloud Computing patterns to the Mobile Cloud Computing environment is a good way of constraining resource demand.}
}
@article{ALZAMIL201591,
title = {Energy-Aware Profiling for Cloud Computing Environments},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {318},
pages = {91-108},
year = {2015},
note = {Twenty-ninth and thirtieth Annual UK Performance Engineering Workshops (UKPEW)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2015.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S1571066115000626},
author = {Ibrahim Alzamil and Karim Djemame and Django Armstrong and Richard Kavanagh},
keywords = {Cloud Computing, Energy Efficiency, Energy-Aware Profiling, Energy Efficiency Metrics},
abstract = {Cloud Computing has changed the way in which people use the IT resources today. Now, instead of buying their own IT resources, they can use the services offered by Cloud Computing with reasonable costs based on a “pay-per-use” model. However, with the wide adoption of Cloud Computing, the costs for maintaining the Cloud infrastructure have become a vital issue for the providers, especially with the large input of energy costs to underpin these resources. Thus, this paper proposes a system architecture that can be used for profiling the resources usage in terms of the energy consumption. From the profiled data, the application developers can enhance their energy-aware decisions when creating or optimising the applications to be more energy efficient. This paper also presents an adapted existing Cloud architecture to enable energy-aware profiling based on the proposed system. The results of the conducted experiments show energy-awareness at physical host and virtual machine levels.}
}
@article{LOUATI201851,
title = {LXCloudFT: Towards high availability, fault tolerant Cloud system based Linux Containers},
journal = {Journal of Parallel and Distributed Computing},
volume = {122},
pages = {51-69},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518305136},
author = {Thouraya Louati and Heithem Abbes and Christophe Cérin},
keywords = {Cloud computing, Containers, Virtualization, Fault tolerance, Replication, Versioning, Grid’5000},
abstract = {Infrastructure-as-a-Service container-based virtualization is gaining interest as a platform for running distributed applications. With increasing scale of Cloud architectures, faults are becoming a frequent occurrence, which makes availability a challenge. LXCloudFT is a fault tolerant Cloud system, which is composed of LXCloud-CR, a Checkpoint–Restart model and GC-CR, a garbage collector component that eliminates old snapshots of containers. LXCloudFT is designed, originally, for scientific applications and all its components are decentralized. We want to adapt it to serve stateless loosely coupled applications such as web applications. Replication is a method to survive failures for such applications. This paper addresses the issue of replication and contributes with a novel replication model, LXCloud-Rep, in LXCloudFT. LXCloud-Rep is a replication model with versioning and garbage collection, which is able to replicate Linux Container instances on several nodes in a decentralized manner. Following a node failure, LXCloud-Rep restarts failed containers on a new node from distributed images of containers not from snapshots. It optimizes the use of storage space. Large-scale experiments on Grid’5000 improve the performance of applications.}
}
@article{HEYDARIBENI201936,
title = {Infracomposer: Policy-driven adaptive and reflective middleware for the cloudification of simulation & optimization workflows},
journal = {Journal of Systems Architecture},
volume = {95},
pages = {36-46},
year = {2019},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1383762118300985},
author = {Emad {Heydari Beni} and Bert Lagaisse and Wouter Joosen},
keywords = {Workflow, Cloud computing, Middleware, Adaptability, Reflection, Parallel and distributed architectures},
abstract = {The simulation and optimization of complex engineering designs in automotive or aerospace involves multiple mathematical tools, long-running workflows and resource-intensive computations on distributed infrastructures. Finding the optimal deployment in terms of task distribution, parallelization, collocation and resource assignment for each execution is a step-wise process involving both human input with domain-specific knowledge about the tools as well as the acquisition of new knowledge based on the actual execution history. In this paper, we present a policy-driven adaptive and reflective middleware that supports smart cloud-based deployment and execution of engineering workflows. This middleware supports deep inspection of the workflow task structure and execution, as well as of the very specific mathematical tools, their executions and used parameters. The reflective capabilities are based on multiple meta-models to reflect workflow structure, deployment, execution and resources. Adaptive deployment is driven by both human input as meta-data annotations as well as adaptation policies that reason over the actual execution history of the workflows. We validate and evaluate this middleware in real-life application cases and scenarios in the domain of aeronautics.}
}
@article{JOSEPH2020101785,
title = {IntMA: Dynamic Interaction-aware resource allocation for containerized microservices in cloud environments},
journal = {Journal of Systems Architecture},
volume = {111},
pages = {101785},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101785},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120300758},
author = {Christina Terese Joseph and K. Chandrasekaran},
keywords = {Microservice placement, Container orchestration, Service oriented computing, Cloud computing, Performance optimization},
abstract = {The Information Technology sector has undergone tremendous changes arising due to the emergence and prevalence of Cloud Computing. Microservice Architectures have also been attracting attention from several industries and researchers. Due to the suitability of microservices for the Cloud environments, an increasing number of Cloud applications are now provided as microservices. However, this transition to microservices brings a wide range of infrastructural orchestration challenges. Though several research works have discussed the engineering of microservice-based applications, there is an inevitable need for research on handling the operational phases of the microservice components. Microservice application deployment in containerized datacenters must be optimized to enhance the overall system performance. In this research work, the deployment of microservice application modules on the Cloud infrastructure is first modelled as a Binary Quadratic Programming Problem. In order to reduce the adverse impact of communication latencies on the response time, the interaction pattern between the microservice components is modelled as an undirected doubly weighted complete Interaction Graph. A novel, robust heuristic approach IntMA is also proposed for deploying the microservices in an interaction-aware manner with the aid of the interaction information obtained from the Interaction Graph. The proposed allocation policies are implemented in Kubernetes. The effectiveness of the proposed approach is evaluated on the Google Cloud Platform, using different microservice reference applications. Experimental results indicate that the proposed approach improves the response time and throughput of the microservice-based systems.}
}
@article{WANG2017117,
title = {An energy-efficient system on a programmable chip platform for cloud applications},
journal = {Journal of Systems Architecture},
volume = {76},
pages = {117-132},
year = {2017},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2016.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1383762116302247},
author = {Xu Wang and Yongxin Zhu and Yajun Ha and Meikang Qiu and Tian Huang and Xueming Si and Jiangxing Wu},
keywords = {Cloud computing, ECG classification, FPGA, Performance analysis, Reconfigurable architectures, Web server},
abstract = {Traditional cloud service providers build large data-centers with a huge number of connected commodity computers to meet the ever-growing demand on performance. However, the growth potential of these data-centers is limited by their corresponding energy consumption and thermal issues. Energy efficiency becomes a key issue of building large-scale cloud computing centers. To solve this issue, we propose a standalone SOPC (System on a Programmable Chip) based platform for cloud applications. We improve the energy efficiency for cloud computing platforms with two techniques. First, we propose a massive-sessions optimized TCP/IP hardware stack using a macro-pipeline architecture. It enables the hardware acceleration of pipelining execution of network packet offloading and application level data processing. This achieves higher energy efficiency while maintaining peak performance. Second, we propose a online dynamic scheduling strategy. It can reconfigure or shut down FPGA nodes according to workload variance to reduce the runtime energy consumption in a standalone SOPC based reconfigurable cluster system. Two case studies including a webserver application and a cloud based ECG (electrocardiogram) classification application are developed to validate the effectiveness of the proposed platform. Evaluation results show that our SOPC based cloud computing platform can achieve up to 418X improvement in terms of energy efficiency over commercial cloud systems.}
}
@article{KRITIKOS2018155,
title = {Reprint of “Towards a security-enhanced PaaS platform for multi-cloud applications”},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {155-175},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16306343},
author = {Kyriakos Kritikos and Tom Kirkham and Bartosz Kryza and Philippe Massonet},
keywords = {Cloud, Security, Meta-model, Policies, Roles, SAML},
abstract = {Multi-cloud adaptive application provisioning can solve the vendor lock-in problem and allows optimising user requirements by selecting the best from the multitude of services offered by different cloud providers. To this end, such provisioning type is increasingly supported by new or existing research prototypes and platforms. One major concern, actually preventing users from moving to the cloud, comes with respect to security, which becomes more complex in multi-cloud settings. Such a concern spans two main aspects: (a) suitable access control on user personal data, VMs and platform services and (b) planning and adapting application deployments based on security requirements. As such, this paper addresses both security aspects by proposing a novel model-driven approach and architecture which secures multi-cloud platforms, enables users to have their own private space and guarantees that application deployments are not only constructed based on but can also maintain a certain user-required security level. Such a solution exploits state-of-the-art security standards, security software and secure model management technology. Moreover, it covers different access control scenarios involving external, web-based and programmatic user authentication.}
}
@article{CHEN2020779,
title = {Stochastic scheduling for variation-aware virtual machine placement in a cloud computing CPS},
journal = {Future Generation Computer Systems},
volume = {105},
pages = {779-788},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17320101},
author = {Yunliang Chen and Xiaodao Chen and Wangyang Liu and Yuchen Zhou and Albert Y. Zomaya and Rajiv Ranjan and Shiyan Hu},
keywords = {Virtual machine, Cloud computing, Cyber-Physical Systems (CPSs), Variation-aware, Optimization},
abstract = {As the most promising computing paradigm, cloud computing opens up new horizons for the area of high-performance distributed computing. Cyber-physical Systems (CPSs) present novel digital systems, which integrate computation, communication and the control of physical resource. Applied CPSs architecture in cloud computing can provide real-time and scalable resource monitoring and offer time-critical applications. With unrivaled scalability and flexibility, the CPSs based cloud services brings significant convenience to customers in need of elastic computing power. The quality of CPSs based cloud services is, to an large extent, determined by the performance of Virtual Machine (VM) placement algorithm for the data center. VM placement also effect the communication between applications and physical resource distribution in cloud computing CPSs. The traditional VM placement algorithm is built upon the two-tier architecture. With the presence of multi-media applications, the application level controller cannot accurately quantify the varying amount computing resources required by VMs at runtime. Consequently, lacking accurate resource demand for each VM, controller at the data center level cannot generate the VM placement with satisfactory feasibility. This architecture no longer fits the modern data centers. In this paper, the two tier VM placement framework is proposed to resolve this technical challenge. Our LP-based variation-unaware VM placement algorithm generates the VM placement with minimized energy consumption. On the other hand, our feasibility driven stochastic VM placement (FDSP) algorithm works seamlessly with the LP-based algorithm to achieve desirable feasibility of the placement. Our experimental results show that the LP-based variation unaware VM placement algorithm improves the energy consumption by 15.3% on average from the baseline algorithm. For test cases with resource request variations, the FDSP algorithm saves 15.7% energy cost compared to the “worst case scenario” of the traditional VM placement paradigm. On the other hand, it improves the feasibility by 50.0% compared to the “best case scenario”.}
}
@article{GUEROUT2014225,
title = {Quality of service modeling for green scheduling in Clouds},
journal = {Sustainable Computing: Informatics and Systems},
volume = {4},
number = {4},
pages = {225-240},
year = {2014},
note = {Special Issue on Energy Aware Resource Management and Scheduling (EARMS)},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2014.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S221053791400047X},
author = {Tom Guérout and Samir Medjiah and Georges {Da Costa} and Thierry Monteil},
keywords = {Cloud computing, Quality of service, Energy-efficiency, Dynamic Voltage and Frequency Scaling, Multi-objective optimization, Simulation},
abstract = {Most Cloud providers support services under constraints of Service Level Agreement (SLA) definitions. The SLAs are composed of different quality of service (QoS) rules promised by the provider. Thus, the QoS in Clouds becomes more and more important. Precise definitions and metrics have to be explained. This article proposes an overview of Cloud QoS parameters as well as their classification, but also it defines usable metrics to evaluate QoS parameters. Moreover, the defined QoS metrics are measurable and reusable in any scheduling approach for Clouds. The use of these QoS models is done through the performance analysis of three scheduling approaches considering four QoS parameters. In addition to the energy consumption and the Response Time, two other QoS parameters are taken into account in different virtual machines scheduling approaches. These parameters are dynamism and robustness, which are usually not easily measurable. The evaluation is done through simulations, using two common scheduling algorithms and a Genetic Algorithm (GA) for virtual machines (VMs) reallocation, allowing us to analyze the QoS parameters evolution in time. Simulation results have shown that including various and antagonist QoS parameters allows a deeper analysis of the intrinsic behavior and insight of these three algorithms. Also, it is shown that the multi-objective optimization allows the service provider to seek the best trade-off between service performances and end user's experience.}
}
@article{ABDENNADHER2017751,
title = {Towards an Optimized Scheme for Mobile Subscribers Based on Cloud Computing},
journal = {Procedia Computer Science},
volume = {112},
pages = {751-760},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.165},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917315223},
author = {Fatma Abdennadher and Maher Ben Jemaa},
keywords = {Mobile Computing, Peer-to-Peer Systems, Wireless Networks, Cloud Computing, Publish/Subscribe},
abstract = {Recently, deploying the publish/subscribe model over Peer-to-Peer networks have been widely used for mobile applications. However, transfer delay and network load remain the challenging issues to consider for mobile subscribers because of their dynamic nature. In this paper, we propose an approach handling mobile subscribers over a Peer-to-Peer architecture, and we illustrate its extension to cloud federation. This extension achieves the realization of a large number of virtual computing clusters, running as into a single cloud organization. Extensive simulation results revealed how our approach can significantly improve transfer delay, and network load compared to previous approaches while applying a wide range of workload parameters, such as frequency of movements, and publication rate.}
}
@article{MARIOTTI2018718,
title = {Strategies and systems towards grids and clouds integration:A DBMS-based solution},
journal = {Future Generation Computer Systems},
volume = {88},
pages = {718-729},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.02.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17302996},
author = {Mirko Mariotti and Osvaldo Gervasi and Flavio Vella and Alfredo Cuzzocrea and Alessandro Costantini},
keywords = {Resource integration, Cloud computing, Grid computing, Distributed environments, Heterogeneous environments, Multi/many core computing, GPGPU computing},
abstract = {Cloud and Grid computing share some essential driving ideas although the computing and economic models are very different. In this paper, we propose different strategies for the Batch-oriented and Service-oriented computing models interoperability. In particular, we describe an innovative approach to connect together Computational Grids and IaaS providers. This is achieved via introducing a simple and powerful DBMS-based system of deploying VM images from a Cloud environment in order to fulfill particular requests of task execution coming from a Grid environment. From a user point of view, resource authorization and access are kept unchanged, thus preserving the user experience related to the Grid. From the accounting point of view, in order to inform the Grid sites that a certain resource is available on a given Cloud-enabled Grid site, the information is published on the Grid information system. In this so-delineated scenario, we are able of using the powerful capability of distributing jobs of the Grid in order to allocate resources not only belonging to Grid clusters, but also with different architectures like GPUs, FPGAs and other systems. The target DBMS-based system has been designed for orchestrate a set of computing systems able to provide physical and virtual resources, creating a unified system, in which the various users-submitted computing tasks are managed and optimized. The goodness of the proposed system is demonstrated by a series of experiments highlighting the benefits of our approach.}
}
@article{FIOCCOLA201662,
title = {A PCE-based architecture for green management of virtual infrastructures},
journal = {Computer Communications},
volume = {91-92},
pages = {62-75},
year = {2016},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0140366416302614},
author = {Giovanni B. Fioccola and Pasquale Donadio and Roberto Canonico and Giorgio Ventre},
keywords = {Cloud computing, Optical networks, Path computation element},
abstract = {Recent evolutions of virtualization technologies allow carriers to optimize and monetize their network infrastructures in new ways, acting as virtual infrastructure providers. By assuming an underlying GMPLS-enabled network infrastructure connecting a number of geographically dispersed data centers, in this paper we define an architectural framework that allows infrastructure providers to optimally use their resources to provide Virtual Infrastructures on demand. The architecture we propose is designed as an extension of the standard Path Computation Element (PCE) architecture. A centralized entity, named VRO, is responsible for optimally allocating the physical resources needed to deploy a requested Virtual Infrastructure. In the paper, we also present how it is possible to apply our framework to pursue green management objectives so that OPEX expenditures can be reduced, while preserving contractual SLAs. We also describe a prototype of our framework that is able to configure GMPLS-enabled network nodes and Cloud-enabled data centers in order to create Virtual Infrastructures.}
}
@article{GHOLIPOUR2020102127,
title = {A novel energy-aware resource management technique using joint VM and container consolidation approach for green computing in cloud data centers},
journal = {Simulation Modelling Practice and Theory},
volume = {104},
pages = {102127},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2020.102127},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X20300666},
author = {Niloofar Gholipour and Ehsan Arianyan and Rajkumar Buyya},
keywords = {Cloud computing, Consolidation, Containerization, Datacenter, Energy consumption, Resource management},
abstract = {Cloud computing is being rapidly adopted for managing IT services as a notable solution due to diverse beneficiaries such as automatically optimized resource management as well as modern service delivery models. The container as a service has been recently introduced by cloud providers as a new service apart from traditional cloud services. Containers enable applications to run and deploy on isolated virtual space, and the operating system kernel is shared among them. Also, containerization has some attributes such as scalability, highly portable properties, and lightweight, for those reasons, it is applied for running isolated applications. Reducing energy consumption, as well as their CO2 emissions, are great deals for cloud providers. In this direction, consolidation is recommended as a vital energy-aware approach in cloud data centers. Previously, independent virtual machine migration or container migration was proposed in the literature for green computing in cloud data centers. However, this paper proposes a new cloud resource management procedure based on a multi-criteria decision-making method that takes advantage of a joint virtual machine and container migration approach concurrently. The results of simulations using ContainerCloudsim simulator validates the applicability of the proposed approach which shows notable reductions in energy consumption, SLA violation, and number of migrations in comparison with the state-of-the-art algorithms.}
}
@article{MONGA2020181,
title = {Software-Defined Network for End-to-end Networked Science at the Exascale},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {181-201},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19305618},
author = {Inder Monga and Chin Guok and John MacAuley and Alex Sim and Harvey Newman and Justas Balcas and Phil DeMar and Linda Winkler and Tom Lehman and Xi Yang},
keywords = {Intent based networking, End-to-end orchestration, Intelligent network services, Distributed infrastructure, Resource modeling, Software defined networking, Real-time, Interactive},
abstract = {Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence.}
}
@article{SIGWELE2020107302,
title = {Energy-efficient 5G cloud RAN with virtual BBU server consolidation and base station sleeping},
journal = {Computer Networks},
volume = {177},
pages = {107302},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107302},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620301742},
author = {Tshiamo Sigwele and Yim Fun Hu and Misfa Susanto},
keywords = {Cloud computing, C-RAN, 5G, Virtual machine placement, Simulated annealing, Genetic algorithm, Energy-efficiency},
abstract = {Heterogeneous network (HetNet) deployment where macro cells are overlaid by small cells is considered a de-facto solution for meeting the ever increasing mobile traffic demand in fifth generation (5G) networks. However, deployment of a large number of small cell base stations (BSs) result in considerable increase in HetNet energy consumption. Cloud radio access networks (C-RAN) has been proposed as an energy-efficient architecture that leverages cloud computing technology where baseband processing is performed in virtual baseband units (vBBU) in the BS cloud. In this paper, we address the energy efficiency (EE) optimization problem in the downlink for two-tier heterogeneous C-RAN (H-CRAN) comprising of macro and pico-cells. At the radio side of H-CRAN, a dynamic pico BS switching OFF algorithm based on a utility function is proposed while maintaining coverage and quality of service (QoS). In the cloud side, heuristic approximation algorithms are proposed including simulated annealing (H-CRAN SA) and genetic algorithm (H-CRAN GA) to minimize energy consumption by reducing the number of BBU servers used through vBBU placement. The proposed scheme is compared with distributed long term evolution advanced (LTE-A) Hetnet system and simulation results show that the proposed H-CRAN SA and H-CRAN GA schemes save 48% and 45% of energy on a daily average, respectively while maintaining the required QoS.}
}
@article{WU2015161,
title = {CAD-based Monte Carlo program for integrated simulation of nuclear system SuperMC},
journal = {Annals of Nuclear Energy},
volume = {82},
pages = {161-168},
year = {2015},
note = {Joint International Conference on Supercomputing in Nuclear Applications and Monte Carlo 2013, SNA + MC 2013. Pluri- and Trans-disciplinarity, Towards New Modeling and Numerical Simulation Paradigms},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2014.08.058},
url = {https://www.sciencedirect.com/science/article/pii/S0306454914004587},
author = {Yican Wu and Jing Song and Huaqing Zheng and Guangyao Sun and Lijuan Hao and Pengcheng Long and Liqin Hu},
keywords = {Monte Carlo simulation, CAD-based, Multi-physics, Nuclear system, SuperMC},
abstract = {Monte Carlo (MC) method has distinct advantages to simulate complicated nuclear systems and is envisioned as a routine method for nuclear design and analysis in the future. High-fidelity simulation with MC method coupled with multi-physics phenomena simulation has significant impact on safety, economy and sustainability of nuclear systems. However, great challenges to current MC methods and codes prevent its application in real engineering projects. SuperMC, developed by the FDS Team in China, is a CAD-based Monte Carlo program for integrated simulation of nuclear systems by making use of hybrid MC and deterministic methods and advanced computer technologies. The design objective, architecture and main methodology of SuperMC are presented in this paper. SuperMC2.1, the latest version, can perform neutron, photon and coupled neutron and photon transport calculation, geometry and physics modeling, results and process visualization. It has been developed and verified by using a series of benchmarking cases such as the fusion reactor ITER model and the fast reactor BN-600 model. SuperMC is still in its evolution process toward a general and routine tool for the simulation of nuclear systems.}
}
@article{DIALLO201484,
title = {Real-time query processing optimization for cloud-based wireless body area networks},
journal = {Information Sciences},
volume = {284},
pages = {84-94},
year = {2014},
note = {Special issue on Cloud-assisted Wireless Body Area Networks},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.03.081},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514003727},
author = {Ousmane Diallo and Joel J.P.C. Rodrigues and Mbaye Sene and Jianwei Niu},
keywords = {Wireless body area network, WBAN, Cloud computing, Real-time database management, Query estimation, Query optimization},
abstract = {Wireless body area networks (WBANs) have received a lot of attention from both academia and industry due to the increasing need of ubiquitous computing for eHealth applications, the continuous advances in miniaturization of electronic devices, and the ultra-low-power wireless technologies. In these networks, various sensors are attached either on clothes, on human body or even implanted under the skin for real-time health monitoring of patients in order to improve their independent daily lives. The energy constraints of sensors, the vital and large amount of data collected by WBAN nodes require powerful and secure storage, and a query processing mechanism that takes into account both real-time and energy constraints. This paper addresses these challenges and proposes a new architecture that combines a cloud-based WBANs with statistical modeling techniques in order to provide a secure storage infrastructure and optimize the real-time user query processing in terms of energy minimization and query latency. Such statistical model provides good approximate answers to queries with a given probabilistic confidence. Furthermore, the combination of the model with the cloud-based WBAN allows performing a query processing algorithm that uses the error tolerance and the probabilistic confidence interval as query execution criterions. The performance analysis and the experiments based on both real and synthetic data sets demonstrate that the new architecture and its underlying proposed algorithm optimize the real-time query processing to achieve minimal energy consumption and query latency, and provide secure and powerful storage infrastructure.}
}
@article{CASALICCHIO2015136,
title = {A Cloud Service Broker with Legal-Rule Compliance Checking and Quality Assurance Capabilities},
journal = {Procedia Computer Science},
volume = {68},
pages = {136-150},
year = {2015},
note = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.230},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030756},
author = {Emiliano Casalicchio and Monica Palmirani},
keywords = {Cloud Computing, Autonomic Computing, Legislation compliance checking, optimisation, Quality of Service ;},
abstract = {The ICT industry, and specifically critical sectors such as healthcare, transportation, energy and government require as mandatory the compliance of the ICT systems and services with legislation and regulation, as well as with standards. In the era of cloud computing, and particularly in a public cloud scenario, this compliance management issue is exacerbated by the distributed nature of the system and by the limited control of the customer on the infrastructure/services. Also if the cloud industry is aware of this legislation/regulation compliance issue (e.g. the compliance program of Amazon, Google and Microsoft Azure), right now, there are nor reference architectures neither mechanisms capable to check and to assure, off-line and at run-time, that the compliance is guaranteed during the whole life cycle of a cloud service. Cloud service brokerage can play an important role in law/regulation compliance management of cloud services. In this paper we propose a broker-based solution for the management of law/regulation compliance. In the specific first we define a reference architecture for a legislation-aware cloud service broker, and second we propose an autonomic manager that integrate the MAPE-K control loop with the LegEx framework for the management of the legal compliance checking lifecycle.}
}
@article{PEREZ201850,
title = {Serverless computing for container-based architectures},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {50-59},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316485},
author = {Alfonso Pérez and Germán Moltó and Miguel Caballer and Amanda Calatrava},
keywords = {Cloud computing, Serverless, Docker, Elasticity, AWS lambda},
abstract = {New architectural patterns (e.g. microservices), the massive adoption of Linux containers (e.g. Docker containers), and improvements in key features of Cloud computing such as auto-scaling, have helped developers to decouple complex and monolithic systems into smaller stateless services. In turn, Cloud providers have introduced serverless computing, where applications can be defined as a workflow of event-triggered functions. However, serverless services, such as AWS Lambda, impose serious restrictions for these applications (e.g. using a predefined set of programming languages or difficulting the installation and deployment of external libraries). This paper addresses such issues by introducing a framework and a methodology to create Serverless Container-aware ARchitectures (SCAR). The SCAR framework can be used to create highly-parallel event-driven serverless applications that run on customized runtime environments defined as Docker images on top of AWS Lambda. This paper describes the architecture of SCAR together with the cache-based optimizations applied to minimize cost, exemplified on a massive image processing use case. The results show that, by means of SCAR, AWS Lambda becomes a convenient platform for High Throughput Computing, specially for highly-parallel bursty workloads of short stateless jobs.}
}
@incollection{AKHGAR2015295,
title = {Chapter 15 - Cloud Computing, Sustainability, and Risk: Case Study: A Quantitative Fuzzy Optimization Model for Determining Cloud Inexperienced Risks’ Appetite},
editor = {Mohammad Dastbaz and Colin Pattinson and Babak Akhgar},
booktitle = {Green Information Technology},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {295-311},
year = {2015},
isbn = {978-0-12-801379-3},
doi = {https://doi.org/10.1016/B978-0-12-801379-3.00015-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128013793000152},
author = {Babak Akhgar and Ashkan Tafaghodi and Konstantinos Domdouzis},
keywords = {Cloud computing, Cloud architecture, Risk appetite, Linear programming, Fuzzy logic},
abstract = {Cloud computing is considered a growing trend in the information and communication technology (ICT) industry, and, of course, risk issues of leveraging this computing model are still in focus. In this chapter, we develop a novel quantitative model for cloud security risk management regarding optimal cloud risk appetite. We focus most on determining cloud risk appetite, which must be considered in advance to make an applicable enterprise risk management (ERM) and to make any plan regarding strategic risk mitigation. In this approach, we mention primarily some of the main risk examples in terms of cloud architecture layers and the developing optimization model of cloud risk appetite. We use a risk map that indicates the severity of each individual risk, Confidentiality, Integrity, Availability (CIA) as risk constraints and evaluation criteria, and linear programming for final fuzzy optimization calculation to gain risk acceptance amount for a given organization in regard to each cloud’s predefined risk. Finally, the applicability and effectiveness of our model is demonstrated through a case study.}
}
@article{QIAN2019114,
title = {LG-RAM: Load-aware global resource affinity management for virtualized multicore systems},
journal = {Journal of Systems Architecture},
volume = {98},
pages = {114-125},
year = {2019},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1383762118306544},
author = {Jianmin Qian and Jian Li and Ruhui Ma and Liwei Lin and Haibing Guan},
keywords = {Virtualization, Resource management, NUMA Optimization},
abstract = {Server consolidation and virtualization technologies enable multiple end-users to share a single physical server, substantially improving hardware utilization and reducing energy consumption. However, as modern multicore server architectures shift to non-uniform memory access (NUMA), the complex interplay between data access affinity and shared resource overhead continues to pose challenges to consolidation efficiency. In this paper, we first systematically characterize the performance impacts of server consolidation on NUMA systems with various cloud applications. We find that the virtual machine (VM) memory and network I/O access could both affect the cloud applications performance. Moreover, as consolidation density continues to grow, conventional approaches cannot manage the system loads and thus result in overall system performance degradation. Motivated by these two findings, we then propose a load-aware global resource affinity management framework (LG-RAM) that aims to optimize VM consolidation performance on NUMA systems. LG-RAM consists of three components: the VM resource access monitor quantifies the VM resource access behaviors, the shared resource load detector models the load on shared hardware resources, and the VM resource scheduler makes the scheduling decision according to the information from the above two components. Our evaluations on the two different systems indicate that, compared with state-of-the-art approaches, LG-RAM can exhibit an average throughput improvement of 41.5% and 54.2% on Intel and AMD NUMA machines, respectively. Additionally, LG-RAM only incurs an extra CPU usage of no more than 7% on average when consolidating 32 VMs.}
}
@article{MONDAL201927,
title = {Cost-optimal cloudlet placement frameworks over fiber-wireless access networks for low-latency applications},
journal = {Journal of Network and Computer Applications},
volume = {138},
pages = {27-38},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519301389},
author = {Sourav Mondal and Goutam Das and Elaine Wong},
keywords = {Cloud computing, Cloudlet, Cost-minimization, Low-latency, Non-linear mixed-integer programming, Fiber-wireless access networks},
abstract = {In the recent past, we have observed a trend that edge devices are increasingly accessing computational resources through the Internet, preferably via cloud computing technologies. However, cloud servers fail to meet the desired latency requirements of low-latency applications like augmented reality, cognitive assistance, and Tactile Internet applications, primarily due to the high-transmission latency from edge devices. This issue is resolved by augmenting cloud networks with a cloudlet enabled edge computing solution that creates a three-tier network topology. In this paper, we primarily focus on static cloudlet network planning problem and propose a hybrid cost-optimization framework for optimal cloudlet placement over existing passive optical access networks, subject to capacity and latency constraints. We formulate a mixed integer non-linear program to identify ideal cloudlet placement locations over urban, suburban and rural deployment scenarios. We show that the target latency requirement and the type of deployment scenario plays a major role in determining the amount of computational resources to be installed in the cloudlets. We also show that the percentage of the incremental energy budget of the access networks due to active cloudlet installation is under 18%. Overall, we show that the newly proposed hybrid cloudlet placement framework achieves the most cost optimal solution as compared to the field cloudlet placement framework.}
}
@article{ALIMI2017653,
title = {Analysis of multiuser mixed RF/FSO relay networks for performance improvements in Cloud Computing-Based Radio Access Networks (CC-RANs)},
journal = {Optics Communications},
volume = {402},
pages = {653-661},
year = {2017},
issn = {0030-4018},
doi = {https://doi.org/10.1016/j.optcom.2017.06.097},
url = {https://www.sciencedirect.com/science/article/pii/S0030401817305734},
author = {Isiaka A. Alimi and Paulo P. Monteiro and António L. Teixeira},
keywords = {Atmospheric turbulence, Gamma–gamma fading, Cloud computing-based radio access networks (CC-RANs), Multiuser mixed RF/FSO relay networks, Pointing error, Rayleigh fading},
abstract = {The key paths toward the fifth generation (5G) network requirements are towards centralized processing and small-cell densification systems that are implemented on the cloud computing-based radio access networks (CC-RANs). The increasing recognitions of the CC-RANs can be attributed to their valuable features regarding system performance optimization and cost-effectiveness. Nevertheless, realization of the stringent requirements of the fronthaul that connects the network elements is highly demanding. In this paper, considering the small-cell network architectures, we present multiuser mixed radio-frequency/free-space optical (RF/FSO) relay networks as feasible technologies for the alleviation of the stringent requirements in the CC-RANs. In this study, we use the end-to-end (e2e) outage probability, average symbol error probability (ASEP), and ergodic channel capacity as the performance metrics in our analysis. Simulation results show the suitability of deployment of mixed RF/FSO schemes in the real-life scenarios.}
}
@article{AN2014757,
title = {A cloud middleware for assuring performance and high availability of soft real-time applications},
journal = {Journal of Systems Architecture},
volume = {60},
number = {9},
pages = {757-769},
year = {2014},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2014.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1383762114000253},
author = {Kyoungho An and Shashank Shekhar and Faruk Caglar and Aniruddha Gokhale and Shivakumar Sastry},
keywords = {High availability, Real-time, Quality of service, Cloud computing, Middleware, Framework, Publish/Subscribe},
abstract = {Applications are increasingly being deployed in the cloud due to benefits stemming from economy of scale, scalability, flexibility and utility-based pricing model. Although most cloud-based applications have hitherto been enterprise-style, there is an emerging need for hosting real-time streaming applications in the cloud that demand both high availability and low latency. Contemporary cloud computing research has seldom focused on solutions that provide both high availability and real-time assurance to these applications in a way that also optimizes resource consumption in data centers, which is a key consideration for cloud providers. This paper makes three contributions to address this dual challenge. First, it describes an architecture for a fault-tolerant framework that can be used to automatically deploy replicas of virtual machines in data centers in a way that optimizes resources while assuring availability and responsiveness. Second, it describes the design of a pluggable framework within the fault-tolerant architecture that enables plugging in different placement algorithms for VM replica deployment. Third, it illustrates the design of a framework for real-time dissemination of resource utilization information using a real-time publish/subscribe framework, which is required by the replica selection and placement framework. Experimental results using a case study that involves a specific replica placement algorithm are presented to evaluate the effectiveness of our architecture.}
}
@article{JIANG201790,
title = {Fault-tolerant system design on cloud logistics by greener standbys deployment with Petri net model},
journal = {Neurocomputing},
volume = {256},
pages = {90-100},
year = {2017},
note = {Fuzzy Neuro Theory and Technologies for Cloud Computing},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.08.134},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217304149},
author = {Fuu-Cheng Jiang and Ching-Hsien Hsu},
keywords = {Cloud computing, Fault-tolerant system, Petri nets, Cost optimization},
abstract = {The cost-aware exploration on enhancing fault-tolerant becomes an important issue of service quality from cloud platform. To approach this goal with greener design, a novel server backup strategy is adopted with two types of standby server with warm standby and cold standby configurations. On such two-level standby scheme, cost elaboration has been explored in terms of deployment ratio between warm standbys and cold standbys. The cold standbys provide a greener power solution than those of conventional warm standbys. The optimal cost policy has been proposed to maintain regulated quality of service for the cloud customers. On qualitative study, a Petri net is developed and designed to visualize the whole system operational flow. On quantitative research for decision support, the theory of finite source queue is elaborated and relevant comprehensive mathematical analysis on cost pattern has been made in detail. Relevant simulations have been conducted to validate the proposed cost optimization model as well. On green contribution, the saving of power consumption has been estimated on the basis of switching warm standbys into cold standbys, which amounts for the reduction of CO2 emission. Hence the proposed approach indeed provides a feasibly standby architecture to meet cloud logistic economy with greener deployment.}
}
@article{KAVVADIA2015435,
title = {Elastic virtual machine placement in cloud computing network environments},
journal = {Computer Networks},
volume = {93},
pages = {435-447},
year = {2015},
note = {Cloud Networking and Communications II},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2015.09.038},
url = {https://www.sciencedirect.com/science/article/pii/S1389128615003631},
author = {Eleni Kavvadia and Spyros Sagiadinos and Konstantinos Oikonomou and Giorgos Tsioutsiouliklis and Sonia Aïssa},
keywords = {Virtual machine, Elastic placement, Cloud computing, Network architecture, Facility location},
abstract = {The growth of cloud computing and the need to support the ever increasing number of applications introduces new challenges and gives rise to various optimization problems, such as calculating the number and location of virtual machines instantiating cloud services to minimize a well-defined cost function. This paper introduces a novel cloud computing network architecture that allows for the formulation of the optimization as an Uncapacitated Facility Location (UFL) problem, where a facility corresponds to an instantiation of a particular service (e.g. a virtual machine). Since UFL is not only difficult (NP-hard and requires global information), but also its centralized solution is non-scalable, the approach followed here is distributed and elastic, and relays local information to improve scalability. In particular, virtual machine replication and merging are proposed and analyzed ensuring overall cost reduction. In addition, a policy that employs virtual machine replication and merging along with migration is proposed to reduce the overall cost for using a service. The efficiency of this policy and its limitations are analyzed and discussed, with simulation results supporting the analytical findings and demonstrating a significant overall cost reduction when the proposed policy is implemented.}
}
@article{ABUSHARKH2017199,
title = {An evergreen cloud: Optimizing energy efficiency in heterogeneous cloud computing architectures},
journal = {Vehicular Communications},
volume = {9},
pages = {199-210},
year = {2017},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214209616300717},
author = {Mohamed {Abu Sharkh} and Abdallah Shami},
keywords = {Cloud computing, Energy efficiency, Scalability, Virtualization, Network and systems monitoring and measurements},
abstract = {In a heterogeneous Cloud network scenario where a Cloud computing data center serves mobile Cloud computing requests, Cloud providers are expected to implement more innovative and effective solutions for a list of long standing challenges. Energy efficiency in the Cloud data center is one of the more pressing issues near the top of that list. Cloud providers are in constant pursuit of a system that satisfies client demands for resources, maximizes availability and other service level agreement metrics while minimizing energy consumption and, in turn, minimizing Cloud providers' cost. In this work, we introduce a novel mathematical optimization model to solve the problem of energy efficiency in a cloud data center. Next, we offer a solution based on VM migration that tackles this problem and minimizes energy efficiency in comparison to other common solutions. This solution includes a novel proposed technique to be integrated in any consolidation-based energy efficiency solution. This technique depends on dynamic idleness prediction (DIP) using machine learning classifiers. Moreover, we offer a robust energy efficiency scheduling solution that does not depend on live migration. This technique, termed Smart VM Over Provision (SVOP), offers a major advantage to cloud providers in the cases when live migration of VMs is not preferred due to its effects on performance. We evaluate the aforementioned solutions in terms of a number of critical metrics, namely, energy used per server, energy used per served request, acceptance rate, and the number of migrations performed.}
}
@article{BAKTIR2019959,
title = {SLA-aware optimal resource allocation for service-oriented networks},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {959-974},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19306430},
author = {Ahmet Cihat Baktır and Betül Ahat and Necati Aras and Atay Özgövde and Cem Ersoy},
keywords = {Edge computing, Cloud computing, Service-centric networks, Network optimization, Task offloading},
abstract = {The popularity of traditional network services and web content is succeeded by the recent trend in customized services proliferated by the smart devices and gadgets. Fall-risk assessment, augmented reality, ECG (electrocardiography) monitoring, virtual reality-based gaming and similar services are driven through data generated by multi-modal sensors embedded in the end-user equipment. These services may possess varying characteristics and requirements represented with performance metrics and Quality of Service (QoS) parameters. Even though the small form-factor end-user gadgets are getting powerful in terms of resource capacity, they are still incapable of executing complex routines, and thus these tasks should be offloaded to a remote machine. Service-Centric Networks (SCN) focus on delivering customized services to the users in a location-independent fashion. This is in parallel with previous vision put forward by the Information-Centric Networks (ICN) and Content Delivery Networks (CDN), which aim to enhance the end-user experience. The novel set of services for complementing the daily activities of the end-users mostly depicts a latency-intolerant attribute which ultimately calls for a full-fledged resource allocation scheme. Within this context, both computation and networking resources should be allocated optimally, and task assignments should be handled precisely for following the requirements specified by the Service Level Agreements (SLAs). This paper initially presents and discusses problem definitions that should be addressed by the service-centric multi-tier computing architecture that is composed of edge, metro, and cloud servers. In order to achieve this objective, an SLA-aware optimal resource allocation and task assignment model for service-oriented networks is proposed. This optimization model is based on a nonlinear delay formulation for accommodating service-centric network scenarios under various conditions. It is then reshaped as a mixed-integer linear model through piecewise linear approximation. Additionally, a heuristic implementation is presented to address the time and space complexities of the problem for which the aforementioned optimization models remain ineffective. Performance evaluation results show that the proposed solutions are able to find a good allocation of resources while taking the requirements of the services into account.}
}
@article{FILELISPAPADOPOULOS2019100,
title = {Simulating large vCDN networks: A parallel approach},
journal = {Simulation Modelling Practice and Theory},
volume = {92},
pages = {100-114},
year = {2019},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19300012},
author = {Christos K. Filelis-Papadopoulos and Konstantinos M. Giannoutakis and George A. Gravvanis and Patricia Takako Endo and Dimitrios Tzovaras and Sergej Svorobej and Theo Lynn},
keywords = {vCDN, Simulation, VM placement, OpenMP},
abstract = {Virtualization and cloud computing are being used by Communication Service Providers to deploy and utilize virtual Content Distribution Networks (vCDNs) to reduce costs and increase elasticity thereby avoiding performance, quality, reliability and availability limitations that characterize traditional CDNs. As cache placement is based on both the content type and geographic location of a user request, it has a significant impact on service delivery and network congestion. To study the effectiveness of cache placements and hierarchical network architectures composed of sites, a novel parallel simulation framework is proposed utilizing a discrete-time approach. Unlike other simulation approaches, the proposed simulation framework can update, in parallel, the state of sites and their resource utilization with respect to incoming requests in a significantly faster manner at hyperscale. It allows for simulations with multiple types of content, different virtual machine distributions, probabilistic caching, and forwarding of requests. In addition, power consumption models allow the estimation of energy consumption of the physical resources that host virtual machines. The results of simulations conducted to assess the performance and applicability of the proposed simulation framework are presented. Results are promising for the potential of this simulation framework in the study of vCDNs and optimization of network infrastructure.}
}
@article{HOSNI2016540,
title = {OaaS Based on Temporal Partitioning with Minimum Energy Consumption},
journal = {Procedia Computer Science},
volume = {96},
pages = {540-549},
year = {2016},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 20th International Conference KES-2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.232},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916320427},
author = {Emna Hosni and Zaki Brahmi},
keywords = {OaaS, BPEL process, Dynamic energy consumption, Temporal partitioning, Buffers.},
abstract = {Cloud computing is an economical solution for industry which is highly scalable and useful of virtualized resources that can be used on demand. It will have a significant impact on companies with the introduction of orchestration platforms as a Service (OaaS) to perform the services that support a variety of business processes such as BPEL. It's becoming an adoptable technology for many of the organizations, thanks to its flexibility and because it reduces total cost of ownership. Thus, an effective OaaS must meet several requests simultaneously; ensuring scalability and optimizing the use of shared resources in order to minimize energy consumption. In this paper, we will investigate three issues i) exploiting the minimum of resources to execute a maximum number of processes, ii) Preventing possible overload to the server, and iii) minimizing dynamic energy consumption which becomes one of the main challenges for large-scale computing, such as in cloud data center. As a solution for these challenges, we propose to use Workflow partitioning technique and this based on temporal dynamic reconfiguration approach. Our work aims to reduce the dynamic energy consumption; especially in communication buffers between partitions of BPEL process during partitioning. The proposed approach is based on two main steps: 1) Estimate the energy consumption of BPEL processes 2) Temporal and dynamic partitioning of BPEL process based on reconfigurable architecture in order to minimize overall energy consumption on each BPEL process.}
}
@article{MIAO2020925,
title = {Intelligent task prediction and computation offloading based on mobile-edge cloud computing},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {925-931},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19320862},
author = {Yiming Miao and Gaoxiang Wu and Miao Li and Ahmed Ghoneim and Mabrook Al-Rakhami and M. Shamim Hossain},
keywords = {Artificial intelligence, Computation offloading, Edge computing, LSTM, Task migration},
abstract = {Edge computing overcomes the high communication delay shortcoming of traditional cloud computing and provides computing services with high reliability and high bandwidth for mobile devices. At present, edge computing has become the forefront and hotspot of mobile-edge cloud computing (MEC) research. However, with the increasing requirements and services of mobile users, offloading strategy of simple edge computing is no longer applicable to MEC architecture. This paper puts forward a new intelligent computation offloading based MEC architecture in combination with artificial intelligence (AI) technology. According to the data size of computation task from mobile users and the performance features of edge computing nodes, a computation offloading and task migration algorithm based on task prediction is proposed. The computation task prediction based on LSTM algorithm, computation offloading strategy for mobile device based on task prediction, and task migration for edge cloud scheduling scheme are used to assist in optimizing the edge computing offloading model. Experiments show that our proposed architecture and algorithm can effectively reduce the total task delay with the increasing data and subtasks.}
}
@article{KNIGHT2019542,
title = {Towards extending the SWITCH platform for time-critical, cloud-based CUDA applications: Job scheduling parameters influencing performance},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {542-556},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18311014},
author = {Louise Knight and Polona Štefanič and Matej Cigale and Andrew C. Jones and Ian Taylor},
keywords = {Time-critical applications, CUDA, Distributed cloud computing},
abstract = {SWITCH (Software Workbench for Interactive, Time Critical and Highly self-adaptive cloud applications) allows for the development and deployment of real-time applications in the cloud, but it does not yet support instances backed by Graphics Processing Units (GPUs). Wanting to explore how SWITCH might support CUDA (a GPU architecture) in the future, we have undertaken a review of time-critical CUDA applications, discovering that run-time requirements (which we call ‘wall time’) are in many cases regarded as the most important. We have performed experiments to investigate which parameters have the greatest impact on wall time when running multiple Amazon Web Services GPU-backed instances. Although a maximum of 8 single-GPU instances can be launched in a single Amazon Region, launching just 2 instances rather than 1 gives a 42% decrease in wall time. Also, instances are often wasted doing nothing, and there is a moderately-strong relationship between how problems are distributed across instances and wall time. These findings can be used to enhance the SWITCH provision for specifying Non-Functional Requirements (NFRs); in the future, GPU-backed instances could be supported. These findings can also be used more generally, to optimise the balance between the computational resources needed and the resulting wall time to obtain results.}
}
@article{RASHIDI2017331,
title = {A hybrid heuristic queue based algorithm for task assignment in mobile cloud},
journal = {Future Generation Computer Systems},
volume = {68},
pages = {331-345},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16304022},
author = {Shima Rashidi and Saeed Sharifian},
keywords = {Mobile cloud computing, Task assignment, Load balancing, Offloading, Ant Colony Optimization, Genetic algorithm, Queue theory},
abstract = {This paper presents a novel algorithm for task assignment in mobile cloud computing environments in order to reduce offload duration time while balancing the cloudlets’ loads. The algorithm is proposed for a two-level mobile cloud architecture, including public cloud and cloudlets. The algorithm models each cloud and cloudlet as a queue to consider cloudlets’ limited resources and study response time more accurately. Performance factors and resource limitations of cloudlets such as waiting time for clients in cloudlets can be determined using queue models. We propose a hybrid genetic algorithm (GA) - Ant Colony Optimization (ACO) algorithm to minimize mean completion time of offloaded tasks for the whole system. Simulation results confirm that the proposed hybrid heuristic algorithm has significant improvements in terms of decreasing mean completion time, total energy consumption of the mobile devices, number of dropped tasks over Queue based Random, Queue based Round Robin and Queue based weighted Round Robin assignment algorithms. Also, to prove the superiority of our queue based algorithm, it is compared with a dynamic application scheduling algorithm, HACAS, which has not considered queue in cloudlets.}
}
@article{CIAVOTTA201764,
title = {A mixed integer linear programming optimization approach for multi-cloud capacity allocation},
journal = {Journal of Systems and Software},
volume = {123},
pages = {64-78},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216301996},
author = {Michele Ciavotta and Danilo Ardagna and Giovanni Paolo Gibilisco},
keywords = {Multi-cloud capacity allocation, Optimization, MILP},
abstract = {The large success of the Cloud computing, its strong impact on the ICT world and on everyday life testifies the maturity and effectiveness this paradigm achieved in the last few years. Presently, the Cloud market offers a multitude of heterogeneous solutions. However, despite the undeniable advantages, Cloud computing introduced new issues and challenges. In particular, the heterogeneity of the available Cloud services and their pricing models makes the identification of a configuration that minimizes the operating costs of a Cloud application, guaranteeing at the same time the Quality of Service, a challenging task. This situation requires new processes and models to design software architectures and predict costs and performance considering together the large variability in price models and the intrinsic dynamism and multi-tenancy of the Cloud environments. This work aims at providing a novel mathematical approach to this problem presenting a queuing theory based Mixed Integer Linear Program (MILP) to find a promising multi-cloud configuration for a given software architecture. The effectiveness of the proposed model has been favorably evaluated against first principle heuristics currently adopted by practitioners. Furthermore, the configuration returned by the model has been also used as initial solution for a local-search based optimization engine, which exploits more accurate but time-consuming performance models. This combined approach has been shown to improve the quality of the returned solutions by 37% on average and reducing the overall search time by 50% with respect to state-of-the-art heuristics based on tiers utilization thresholds.}
}
@article{ISHAKIAN201714,
title = {AngelCast: Cloud-based peer-assisted live streaming using optimized multi-tree construction},
journal = {Computer Communications},
volume = {111},
pages = {14-28},
year = {2017},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2017.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0140366416305035},
author = {Vatche Ishakian and Raymond Sweha and Azer Bestavros},
keywords = {P2P, Live streaming, Cloud computing, Content delivery},
abstract = {Increasingly, commercial content providers (CPs) offer streaming solutions using peer-to-peer (P2P) architectures, which promises significant scalability by leveraging clients’ upstream capacity. A major limitation of P2P live streaming is that playout rates are constrained by clients’ upstream capacities – typically much lower than downstream capacities – which limit the quality of the delivered stream. To leverage P2P architectures without sacrificing quality, CPs must commit additional resources to complement clients’ resources. In this work, we propose a cloud-based service AngelCast that enables CPs to complement P2P streaming. By subscribing to AngelCast, a CP is able to deploy extra resources (angel), on-demand from the cloud, to maintain a desirable stream quality. Angels do not download the whole stream, nor are they in possession of it. Rather, angels only relay the minimal fraction of the stream necessary to achieve the desired quality. We provide a lower bound on the minimum angel capacity needed to maintain a desired client bit-rate, and develop a fluid model construction to achieve it. Realizing the limitations of the fluid model construction, we design a practical multi-tree construction that captures the spirit of the optimal construction, and avoids its limitations. We present a prototype implementation of AngelCast, along with experimental results confirming the feasibility of our service.}
}
@article{LIAO20181318,
title = {Energy Consumption Optimization Scheme of Cloud Data Center Based on SDN},
journal = {Procedia Computer Science},
volume = {131},
pages = {1318-1327},
year = {2018},
note = {Recent Advancement in Information and Communication Technology:},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.327},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918307075},
author = {Qing Liao and Zun Wang},
keywords = {Cloud data center, software-defined network, energy consumption control, VM migration integration},
abstract = {In response to the mobile Internet, Internet of things and cloud computing technology fast development, the operation of large-scale distributed computing cloud data center construction in the world at the same time, the network data calculation of energy consumption problem is causing extensive concern. And the software-defined Network (SDN) provides a feasible solution to the rigid problem of traditional Network architecture. Based on SDN technology, this paper proposes a method of energy consumption control based on cloud data center. Aiming at server energy saving, the multi-objective optimization model of virtual machine migration integration is established, and the integration mechanism of virtual machine migration based on mixed single-parent genetic algorithm is proposed. The experimental results show that the algorithm can be used to solve the problem of virtual machine migration and integration in data center.}
}
@article{ODONNCHA2016199,
title = {On the Efficiency of Executing Hydro-environmental Models on Cloud},
journal = {Procedia Engineering},
volume = {154},
pages = {199-206},
year = {2016},
note = {12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816318367},
author = {Fearghal O’Donncha and Emanuele Ragnoli and Srikumar Venugopal and Scott C. James and Kostas Katrinis},
keywords = {Cloud, HPC, Numerical modelling, Containers, Hydrodynamic model},
abstract = {Optimizing high-performance computing applications requires understanding of both the application and its parallelization approach, the system software stack and the target architecture. Traditionally, performance tuning of parallel applications involves consideration of the underlying machine architecture, including floating point performance, memory hierarchies and bandwidth, interconnect architecture, data placement – among others. The shift to the utility computing model through cloud has created tempting economies of scale across IT and domains, not leaving HPC as an exception as a candidate beneficiary. Nevertheless, the infrastructure abstraction and multi-tenancy inherent to cloud offerings poses great challenges to HPC workloads, requiring a dedicated study of applicability of cloud computing as a viable time-to-solution and efficiency platform. In this paper, we present the evaluation of a widely used hydro-environmental code, EFDC, on a cloud platform. Specifically, we evaluate the target parallel application on Linux containers managed by Docker. Unlike virtualization- based solutions that have been widely used for HPC cloud explorations, containers are more fit-for-purpose, sporting among others native execution and lightweight resource consumption. Many-core capability is provided by the OpenMP library in a hybrid configuration with MPI for cross-node data movement, and we explore the combination of these in the target setup. For the MPI part, the work flow is implemented as a data-parallel execution model, with all processing elements performing the same computation, on different sub-domains with thread-level, fine-grain parallelism provided by OpenMP. Optimizing performance requires consideration of the overheads introduced by the OpenMP paradigm such as thread initialization and synchronization. Features of the application make it an ideal test case for deployment on modern cloud architectures, including that it: 1) is legacy code written in Fortran 77, 2) has an implicit solver requiring non-local communication that poses a challenge to traditional partitioning methods, communication optimization and scaling and, 3) is a legacy code across academia, research organizations, governmental agencies, and consulting firms. These technical and practical considerations make this study a representative assessment of migrating legacy codes from traditional HPC systems to the cloud. We finally discuss challenges that stem from the containerized nature of the platform; the latter forms another novel contribution of this paper.}
}
@article{DZIURZANSKI2020101069,
title = {Scalable distributed evolutionary algorithm orchestration using Docker containers},
journal = {Journal of Computational Science},
volume = {40},
pages = {101069},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2019.101069},
url = {https://www.sciencedirect.com/science/article/pii/S1877750319303333},
author = {Piotr Dziurzanski and Shuai Zhao and Michal Przewozniczek and Marcin Komarnicki and Leandro Soares Indrusiak},
keywords = {Smart factory, Industry 4.0, Evolutionary algorithms, Distributed optimisation, Multi-objective optimisation, Integrated process planning and scheduling},
abstract = {In smart factories, integrated optimisation of manufacturing process planning and scheduling leads to better results than a traditional sequential approach but is computationally more expensive and thus difficult to be applied to real-world manufacturing scenarios. In this paper, a working approach for cloud-based distributed optimisation for process planning and scheduling is presented. Three managers dynamically governing the creation and deletion of subpopulations (islands) evolved by a multi-objective genetic algorithm are proposed, compared and contrasted. A number of test cases based on two real-world manufacturing scenarios are used to show the applicability of the proposed solution.}
}
@article{SHUJA2017407,
title = {Case of ARM emulation optimization for offloading mechanisms in Mobile Cloud Computing},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {407-417},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16301558},
author = {Junaid Shuja and Abdullah Gani and Anjum Naveed and Ejaz Ahmed and Ching-Hsien Hsu},
keywords = {ARM emulation, Virtualization, Benchmark, MCC, Offloading},
abstract = {The Mobile Cloud Computing (MCC) paradigm depends on efficient offloading of computation from the resource constrained mobile device to the resource rich cloud server. The computational offloading is assisted by system virtualization, application virtualization, and process state migration. However, system and application virtualization techniques force unnecessary overhead on applications that require offloading to the cloud and applications that do not. Moreover, smartphones and cloud data centers are based on heterogeneous processor architectures, such as, ARM and x86. As a result, process migrated from a smartphone needs translation or emulation on the cloud server. Therefore, instruction emulation is a necessary criterion for a comprehensive MCC framework. In this paper, we evaluate the overhead of the system and application virtualization techniques and emulation frameworks that enable MCC offloading mechanisms. We find that the overhead of system and application virtualization can be as high as 4.51% and 55.18% respectively for the SciMark benchmark. Moreover, ARM to Intel device emulation overhead can be as high as 55.53%. We provide a proof of concept of emulation speedup by utilizing efficient Single Instruction, Multiple Data (SIMD) translations. We conclude that the overhead of virtualization and emulation techniques need to be reduced for efficient MCC offloading frameworks.}
}
@article{TOSNER201479,
title = {Computer-intensive simulation of solid-state NMR experiments using SIMPSON},
journal = {Journal of Magnetic Resonance},
volume = {246},
pages = {79-93},
year = {2014},
issn = {1090-7807},
doi = {https://doi.org/10.1016/j.jmr.2014.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090780714001931},
author = {Zdeněk Tošner and Rasmus Andersen and Baltzar Stevensson and Mattias Edén and Niels Chr. Nielsen and Thomas Vosegaard},
keywords = {SIMPSON, Simulation of solid-state NMR experiments, Fast powder averaging, Cloud computing, Optimal control pulse sequence optimization},
abstract = {Conducting large-scale solid-state NMR simulations requires fast computer software potentially in combination with efficient computational resources to complete within a reasonable time frame. Such simulations may involve large spin systems, multiple-parameter fitting of experimental spectra, or multiple-pulse experiment design using parameter scan, non-linear optimization, or optimal control procedures. To efficiently accommodate such simulations, we here present an improved version of the widely distributed open-source SIMPSON NMR simulation software package adapted to contemporary high performance hardware setups. The software is optimized for fast performance on standard stand-alone computers, multi-core processors, and large clusters of identical nodes. We describe the novel features for fast computation including internal matrix manipulations, propagator setups and acquisition strategies. For efficient calculation of powder averages, we implemented interpolation method of Alderman, Solum, and Grant, as well as recently introduced fast Wigner transform interpolation technique. The potential of the optimal control toolbox is greatly enhanced by higher precision gradients in combination with the efficient optimization algorithm known as limited memory Broyden–Fletcher–Goldfarb–Shanno. In addition, advanced parallelization can be used in all types of calculations, providing significant time reductions. SIMPSON is thus reflecting current knowledge in the field of numerical simulations of solid-state NMR experiments. The efficiency and novel features are demonstrated on the representative simulations.}
}
@article{XIN201737,
title = {A load balance oriented cost efficient scheduling method for parallel tasks},
journal = {Journal of Network and Computer Applications},
volume = {81},
pages = {37-46},
year = {2017},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516303496},
author = {Yu Xin and Zhi-Qiang Xie and Jing Yang},
keywords = {Load balance, Scheduling method, Multiple schedulers, Execution cost},
abstract = {With the development of Internet technology, distributed task processing has become the key to solve the problems in big data computing, cloud computing, and collaborative computing. At the aspect of distributed task scheduling optimization, it is needed to establish the scheduling architecture with multiple schedulers, to meet the requirement of minimizing the cost of large scale parallel tasks. However the schedulers would give rise to the issue of high device load, intensive resource competition, and the inefficient collaboration. For this, we proposed the CESM (Cost Efficient Scheduling Method) method, which utilizes the weighted random schedule policy to assign the devices to the tasks, to reduce the competition of the task on the efficient low-cost devices. The weights in the random schedule process dependent on the scheduling environment, such as communication time, the busy state, the execution time and the cost. The efficient low-cost device tends to get a higher weight, implying it has a higher possibility to be assigned. That makes the scheduling results have a better rationality on execution time and cost. For this reason, we designed the weight model based on the communication time, the busy state, the execution time and the cost, and adopted the experimental method to analyze the values of the parameters. Finally, we gave four experiments on the arrival time test, device dependence test, task structure test, device set test, respectively, to verify the effectiveness and rationality of the proposed CESM.}
}
@article{ZHANG201732,
title = {Chunk mode VM migration in XIA and triple-way pipeline for performance optimization},
journal = {Future Generation Computer Systems},
volume = {74},
pages = {32-40},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17306660},
author = {Dalu Zhang and Dejiang Zhou and Xiang Jin},
keywords = {DAG, XIA, Virtual machine migration, Optimization},
abstract = {Traditional TCP/IP network is showing lots of shortages and research for future networks is becoming a hotspot. FIA (Future Internet Architecture) and FIA-NP (Next Phase) are supported by US NSF for future Internet designing. Moreover, virtual machine migration is a significant technique in cloud computing. As a network application, it should also be supported in XIA (eXpressive Internet Architecture), which is in both FIA and FIA-NP projects. Current research just achieved to conduct VM migration in XIA, but the performance is not satisfactory. In this paper, we firstly present three methods including DAG design and routing table modification to maintain VM’s connectivity and communication states, concerning performance and overhead. VM migration experiments are conducted intra-AD, inter-AD and across IP network with KVM instances. The procedure is achieved by a migration control protocol which is suitable for the characters of XIA. Secondly, we propose a triple-way pipeline to improve the performance of chunk mode data transfer and greatly reduce total migration time and downtime. Evaluation results show that our solutions can well supports full live VM migration over XIA network respectively, keeping services seamless and performance improved. Furthermore, the optimization scheme can greatly improve the speed of VM migration, especially on XIA testbed.}
}
@article{TANKOVIC201762,
title = {ElaClo: A framework for optimizing software application topology in the cloud environment},
journal = {Expert Systems with Applications},
volume = {90},
pages = {62-86},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417304700},
author = {Nikola Tanković and Tihana {Galinac Grbac} and Mario Žagar},
keywords = {Software architecture, Cloud computing, Service-oriented computing, Application topology, Service deployment, Evolutionary optimization},
abstract = {Application architectures in the cloud employ elastic components, and achieve lower operating costs without sacrificing quality. Software architects strive to provide efficient services by deciding on software topology: a set of structural architectural decisions. For a given application, there can be numerous software topology alternatives creating the need for automated optimization methods. Current optimization approaches rely on experts providing application performance models built upfront, based on their experience and the requirements provided. While such techniques are effective and valuable, they require additional maintenance effort as the software evolves. This paper introduces ElaClo, a framework for optimizing application topologies in a cloud environment. ElaClo’s main contribution is in providing optimization in the software assembly phase from automatically extracted application models. ElaClo provides workload generation, monitoring, topology management, elasticity mechanisms, and algorithms to support the optimization process. We have implemented ElaClo as an expert tool and evaluated it on a real-life cloud application from the retailing business domain. ElaClo was used to select optimal topologies with regards to service response time objectives and infrastructure costs. The efficiency of the optimization process and the quality of optimization results were validated quantitatively on a set of optimization runs. Results demonstrate the effectiveness of the suggested framework in yielding optimal topologies.}
}
@article{GOMES2017148,
title = {Edge caching with mobility prediction in virtualized LTE mobile networks},
journal = {Future Generation Computer Systems},
volume = {70},
pages = {148-162},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16302072},
author = {Andre S. Gomes and Bruno Sousa and David Palma and Vitor Fonseca and Zhongliang Zhao and Edmundo Monteiro and Torsten Braun and Paulo Simoes and Luis Cordeiro},
keywords = {Information-Centric Networking, Content migration, Edge caching, Mobility prediction, LTE, Follow-Me Cloud},
abstract = {Mobile Edge Computing enables the deployment of services, applications, content storage and processing in close proximity to mobile end users. This highly distributed computing environment can be used to provide ultra-low latency, precise positional awareness and agile applications, which could significantly improve user experience. In order to achieve this, it is necessary to consider next-generation paradigms such as Information-Centric Networking and Cloud Computing, integrated with the upcoming 5th Generation networking access. A cohesive end-to-end architecture is proposed, fully exploiting Information-Centric Networking together with the Mobile Follow-Me Cloud approach, for enhancing the migration of content-caches located at the edge of cloudified mobile networks. The chosen content-relocation algorithm attains content-availability improvements of up to 500% when a mobile user performs a request and compared against other existing solutions. The performed evaluation considers a realistic core-network, with functional and non-functional measurements, including the deployment of the entire system, computation and allocation/migration of resources. The achieved results reveal that the proposed architecture is beneficial not only from the users’ perspective but also from the providers point-of-view, which may be able to optimize their resources and reach significant bandwidth savings.}
}
@article{CHIBA2019291,
title = {Intelligent approach to build a Deep Neural Network based IDS for cloud environment using combination of machine learning algorithms},
journal = {Computers & Security},
volume = {86},
pages = {291-317},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819301221},
author = {Zouhair Chiba and Noreddine Abghour and Khalid Moussaid and Amina {El omri} and Mohamed Rida},
keywords = {Cloud computing, Network intrusion detection system, Deep Neural Network, Genetic algorithm, Simulated Annealing Algorithm, CICIDS dataset 2017, NSL-KDD dataset, CIDDS-001 dataset},
abstract = {The appealing features of Cloud Computing continue to fuel its adoption and its integration in many sectors such industry, governments, education and entertainment. Nevertheless, uploading sensitive data to public cloud storage services poses security risks such as integrity, availability and confidentiality to organizations. Moreover, the open and distributed (decentralized) structure of the cloud has resulted this class of computing, prone to cyber attackers and intruders. Thereby, it is imperative to develop an anomaly network intrusion system to detect and prevent both inside and outside assaults in cloud environment with high detection precision and low false warnings. In this work, we propose an intelligent approach to build automatically an efficient and effective Deep Neural Network (DNN) based anomaly Network IDS using a hybrid optimization framework (IGASAA) based on Improved Genetic Algorithm (IGA) and Simulated Annealing Algorithm (SAA). The IDS resulted is called “MLIDS” (Machine Learning based Intrusion Detection System). Genetic Algorithm (GA) is improved through optimization strategies, namely Parallel Processing and Fitness Value Hashing, which reduce execution time, convergence time and save processing power. Moreover, SAA was incorporated to IGA with the aim to optimize its heuristic search. Our approach consists of using IGASAA in order to search the optimal or near-optimal combination of most relevant values of the parameters included in construction of DNN based IDS or impacting its performance, like feature selection, data normalization, architecture of DNN, activation function, learning rate and Momentum term, which ensure high detection rate, high accuracy and low false alarm rate. For simulation and validation of the proposed method, CloudSim 4.0 simulator platform and three benchmark IDS datasets were used, namely CICIDS2017, NSL-KDD version 2015 and CIDDS-001. The implementation results of our model demonstrate its ability to detect intrusions with high detection accuracy and low false alarm rate, and indicate its superiority in comparison with state-of-the-art methods.}
}
@article{CALLEGATI2018277,
title = {Cloud-of-Things meets Mobility-as-a-Service: An insider threat perspective},
journal = {Computers & Security},
volume = {74},
pages = {277-295},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817302134},
author = {Franco Callegati and Saverio Giallorenzo and Andrea Melis and Marco Prandini},
keywords = {Mobility-as-a-Service, Federated platforms, Insider threat, Cloud-of-Things, Internet-of-Things},
abstract = {Mobility-as-a-Service (MaaS) applies the everything-as-a-service paradigm of Cloud Computing to transportation: a MaaS provider offers to its users the dynamic composition of solutions of different travel agencies into a single, consistent interface. Traditionally, transits and data on mobility belong to a scattered plethora of operators. Thus, we argue that the economic model of MaaS is that of federations of providers, each trading its resources to coordinate multi-modal solutions for mobility. Such flexibility comes with many security and privacy concerns, of which insider threat is one of the most prominent. In this paper, we revise and extend previous work where we classified the potential threats of individual operators and markets of federated MaaS providers, proposing appropriate countermeasures to mitigate the problems. In addition, we consider the emerging case of Cloud-of-Things (CoT) for mobility, i.e., networks of ubiquitous, pervasive devices that provide real-time data on objects and people. Automation and pervasiveness of CoT make an additional attack surface for insiders. In an effort to limit such phenomenon, we present an overlay networking architecture, based on gossip protocols, that lets users share information on mobility with each other. A peculiarity of the architecture is that it both constrains the quality and quantity of data obtainable by insiders, optimizing the routing of requests to involve only users that are able to answer them.}
}
@article{SHEIKHI201642,
title = {Dynamic load management for a residential customer; Reinforcement Learning approach},
journal = {Sustainable Cities and Society},
volume = {24},
pages = {42-51},
year = {2016},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716300543},
author = {A. Sheikhi and M. Rayati and A.M. Ranjbar},
keywords = {Smart Energy Hub (SEH), Smart grids (SG), Reinforcement Learning (RL), Energy efficiency, Optimization},
abstract = {United Nation aims to double the global rate of improvement in energy efficiency as one of the sustainable development goals. It means researchers should focus on energy systems to enhance their overall efficiency. One of the effective solution to move from suboptimal energy systems to optimal ones is analyzing energy system in Energy Hub (EH) framework. In EH framework, interactions between different energy carriers are considered in supplying the required loads. The couplings and selecting proper combinations of inputs energy carriers lead to more optimized and intelligent consumption. The appropriate combination is found by solving an optimization problem at each time step. Utilizing intelligent technologies such as Advanced Metering Infrastructures (AMIs) inevitably facilitate the decision making processes. This paper modifies the classic Energy Hub model to present an upgraded model in the smart environment entitling “Smart Energy Hub” and optimizes the operation of a residential customer equipped with combined heat and power (CHP), auxiliary boiler, electricity storage and heating storage in this framework. Supporting real time, two-way communication between utility companies and smart energy hubs, and allowing AMIs at both ends to manage power consumption necessitates large-scale real-time computing capabilities to handle the communication and the storage of huge transferable data. To address this concern and reduce the amount of calculations, Reinforcement Learning (RL) method is employed to find a near optimal solution, which does not need massive computations. Finally, communications to large numbers of endpoints in a secure, scalable, and highly-available environment, in this paper, we propose a cloud computing (CC) architecture. Simulation results show that by applying RL technique in smart energy hub framework for a residential customer, efficiency of the energy system is increased substantially and leads to decrease energy bills and electricity peak load.}
}
@article{YEH201668,
title = {CloudBook: Implementing an E-book reader on a cloud platform by an optimized vector graphic library},
journal = {Computer Standards & Interfaces},
volume = {43},
pages = {68-78},
year = {2016},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920548915000902},
author = {Hsiu-lien Yeh and Tseng-Yi Chen and Hsin-Wen Wei and Nien-I Hsu and Jenq-Shiou Leu and Heng-Yin Chen},
keywords = {Vector graphic, E-book, Embedded GPU, Hadoop platform, Cloud computing},
abstract = {Problems of the E-book application include different file formats, performance inefficiency and image distortion; those may cause a slow development of E-book marketing in the industry of information technology. This paper proposes an innovative E-book system called CloudBook, which utilizes an embedded Graphics Processing Unit (GPU) and a data locality aware Hadoop system to resolve the problems. The results of the experiments show that the CloudBook system can increase the performance of the OpenVG library by 73%, reduce the execution time of file conversion by 50%–75%, and improve the data hit ratio in cloud platform by 10%.}
}
@article{BOUKADI2019397,
title = {Business process outsourcing to cloud containers: How to find the optimal deployment?},
journal = {Future Generation Computer Systems},
volume = {97},
pages = {397-408},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.069},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18303467},
author = {Khouloud Boukadi and Rima Grati and Molka Rekik and Hanêne Ben-Abdallah},
keywords = {Business process, Cloud, CaaS, Linear program, Optimal deployment},
abstract = {Containers are a new service model that empowers cloud computing by offering horizontally scalable systems while bypassing high-performance challenges of traditional hypervisors. In the business process management context, Containers-as-a-Service can be used to outsource business processes to the cloud and allow an enterprise to bundle its processes and data in a simpler and more performance-oriented manner. To profit from containers, an enterprise must however have a means to identify the optimal resource allocation. Towards this end, we propose a system architecture for optimal containers-based deployment of business processes. The proposed system architecture relies on our extension of ContainerCloudSim simulator to estimate the execution time of business processes deployed according to the CaaS model. In addition, it encloses a business process deployment optimizer. To develop this latter, we examine a linear program and a genetic algorithm to find out the optimal deployment of a business process on cloud containers. We show experimentally the effective performance of containers-based versus VM-based deployment, and linear program versus the First-Fit container strategy and the genetic algorithm.}
}
@article{LI201545,
title = {Transformer: Run-time reprogrammable heterogeneous architecture for transparent acceleration of dynamic workloads},
journal = {Journal of Parallel and Distributed Computing},
volume = {86},
pages = {45-61},
year = {2015},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731515001409},
author = {Peilong Li and Yan Luo and Jun Yang},
keywords = {Accelerator, Transparent acceleration, Coarse-grain, Heterogeneous architecture, FPGA},
abstract = {Heterogeneous architectures face challenges regarding transparent acceleration as well as the allocation of resources to cores and accelerators. The “Transformer”, a run-time reprogrammable, heterogeneous architecture consisting of cores and reconfigurable logic with support for coarse-grained acceleration of the dynamic, unpredictable workloads present in mobile and cloud computing environments, is proposed as a solution. The architecture allows for the run-time instantiation of one or more acceleration functions, present in an on-chip reconfigurable logic, which responds to the demands of compute-intensive software libraries. The hardware controller and software wrapper functions are designed to profile workloads, reprogram the internal logic, and invoke the appropriate acceleration functions. Novel heuristics are derived with respect to the accelerator function scheduling. In order to optimize performance and power efficiency, the appropriate system parameters are explored, including the L1 and L2 cache sizes, the accelerator local buffer sizes, as well as the allocation of resources to the cores and accelerators. The simulation results indicate that the Transformer provides significant improvements in terms of performance, up to 14× for single-type workloads and 2.3× for dynamic workloads, as well as energy efficiency, up to 6.9× for various workloads.}
}
@article{GONZALEZBRIONES2019971,
title = {Intelligent multi-agent system for water reduction in automotive irrigation processes},
journal = {Procedia Computer Science},
volume = {151},
pages = {971-976},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.136},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306003},
author = {Alfonso González-Briones and Yeray Mezquita and José A. Castellanos-Garzón and Javier Prieto and Juan M. Corchado},
keywords = {Multi-agent system, smart sprinkling, CBR system, RFID, pivot irrigation},
abstract = {This paper deals with a multi-agent system (MAS) to automate the gathering and managing of information from potato crops in order to provide a precision irrigation system. The proposal and development of a novel MAS is presented based on different agent subsystems with specific objectives to meet the main objective of the global MAS. The proposed MAS has been developed on the Cloud Computing paradigm and is able to gather data from wireless sensor networks (WSNs) located in potato crops for knowledge discovery and decision making. According to the collected information as historical data by the MAS, it can make decision on an actuator set that modify the irrigation system by updating the areas of the crop with most irrigation needs. The use of these intelligent technologies in rural areas provides a considerable saving of resources and improves the efficiency and effectiveness of agricultural production systems. The architecture has been tested in an agricultural environment in order to optimize irrigation in a potato crop. The results showed a significant reduction in comparison to traditional automotive irrigation.}
}
@article{CHAUHAN2019193,
title = {Brokering in interconnected cloud computing environments: A survey},
journal = {Journal of Parallel and Distributed Computing},
volume = {133},
pages = {193-209},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518305719},
author = {Sameer Singh Chauhan and Emmanuel S. Pilli and R.C. Joshi and Girdhari Singh and M.C. Govil},
keywords = {Cloud computing, Cloud broker, Inter-cloud, Federated cloud, Multi-cloud, Hybrid cloud},
abstract = {Cloud computing provides computing platforms and facilitates to optimize utilization of infrastructure resources, reduces deployment time and increases flexibility. The popularity of cloud computing led to development of interconnected cloud computing environments (ICCE) such as hybrid cloud, inter-cloud, multi-cloud, and federated cloud, enabling the possibilities to share resources among individual clouds. However, individual proprietary technologies and access interfaces employed by cloud service providers made it difficult to share resources. Interoperability and portability are two of the major challenges to be addressed to ensure seamless access and sharing of resources and services. Many cloud service providers have similar service offerings but different access patterns. It is difficult and time consuming for a cloud user to select an appropriate cloud service as per the application’s requirement. Cloud user has to gather information from various cloud service providers and analyze them. Cloud broker has been proposed to address the challenge of cloud users to get best out of cloud provider. Cloud broker is an entity which works as an independent third party between cloud users and cloud providers. Cloud broker negotiates with several cloud providers as per user’s requirements and tries to select the best services. Cloud broker coordinates the sharing of resources and provides interoperability and portability with other cloud providers. In this paper, a comprehensive survey of cloud brokering in interconnected cloud computing environments has been provided. The need and importance of cloud broker has been discussed. The existing architectures and frameworks of Cloud Brokering are reviewed. A comprehensive literature survey of various Cloud Brokering techniques is presented. A taxonomy of Cloud Brokering techniques has been presented and analyzed on the basis of their strengths and weaknesses/limitations. The taxonomy includes pricing, multi-criteria, quality of services, optimization and trust techniques. The techniques are analyzed on various performance metrics. Research challenges and open problems are identified from reviewed techniques. A model for cloud broker is proposed to address identified challenges. We hope that our work will enable researchers to launch and dive deep into Cloud Brokering challenges in interconnected cloud computing environments.}
}
@article{GARG201961,
title = {Improved TOPSIS: A multi-criteria decision making for research productivity in cloud security},
journal = {Computer Standards & Interfaces},
volume = {65},
pages = {61-78},
year = {2019},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548918301946},
author = {Deepak Garg and Jagpreet Sidhu and Shalli Rani},
keywords = {Cloud computing security, Effective citation (EC), Effective impact factor (EIF), Vulnerabilities, Threats & attacks, TOPSIS},
abstract = {Cloud Computing (CC) paradigm has become a mainstream solution for deployment of business processes and applications. It provides an ability to utilize geographically distributed resources a pay-as-you-go model. According to industry reports, 76% of prospective cloud clients viewed security as the major barrier in the adaption of CC. During the last decades, the research community has immensely focused on developing and optimizing security techniques to overcome this hurdle in computing. Due to a huge amount of literature, it becomes difficult to comprehend the overall structure and advancements. This domain needs an analytic approach to completely understand the problem domain. Therefore, in accordance to architecture layer of CC is the need of hour. This article represents the same in context to security in CC. Further, a notion on Security Issues (SI's) in CC with respect to vulnerabilities, threats, and attacks are discussed. Paper also proposed two research evaluation parameters that are Effective Citation (EC) and Effective Impact Factor (EIF) for better appreciate and analyse the patterns, trends and other important factors as a basis for directing research activities. A case study is demonstrated based on proposed parameters and generic parameters to rank effective journals by using improved TOPSIS method. Finally, global research trends on security domain are evaluated and analysed.}
}
@article{WANG2020107143,
title = {A multi-service differentiation traffic management strategy in SDN cloud data center},
journal = {Computer Networks},
volume = {171},
pages = {107143},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107143},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619309740},
author = {Yaomin Wang and Xia Wang and Haiyan Li and Yi Dong and Qing Liu and Xinling Shi},
keywords = {Cloud data centers, Traffic management, Multi-service differentiation, Fibonacci tree optimization (FTO), Multi-modal adaptive optimization},
abstract = {The cloud computing technology makes services more diverse, and users have different needs for multi-service experience. The operator cloud data centers support more services and users, resulting in a rapid growth in traffic. Consequently, the traditional network architectures and technologies can no longer meet these traffic management requirements. To address this problem, this paper proposes a multi-service differentiated (MSD) traffic management strategy for the operator SDN cloud data center. The MSD traffic management rules and model are designed according to the operational requirements in cloud data centers. Fibonacci tree optimization algorithm (FTO) is improved for matching the MSD model to optimize the differentiated traffic management solutions. An operator SDN cloud data center test platform is established to verify the effectiveness of the strategy. The results show that the MSD-FTO strategy can obtain multiple differentiated traffic management schemes through one single optimization and it outperforms ECMP. In addition, it can properly manage the multi-service traffic and greatly improve the network performance and service experience in cloud data centers.}
}
@article{TERRAZAS2019204,
title = {A cloud-based framework for shop floor big data management and elastic computing analytics},
journal = {Computers in Industry},
volume = {109},
pages = {204-214},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518306808},
author = {German Terrazas and Nicolas Ferry and Svetan Ratchev},
keywords = {Industry 4.0, Cyber physical systems, Big data, Cloud-based data collection, Cloud-based analytics, Elastic computing},
abstract = {Advanced digitalization together with the rise of disruptive Internet technologies are key enablers of a fundamental paradigm shift observed in industrial production. This is known as the fourth industrial revolution (Industry 4.0) which proposes the integration of the new generation of ICT solutions for the monitoring, adaptation, simulation, and optimisation of factories. With the democratization of sensors and actuators, factories and machine tools can now be sensorized and the data generated by these devices can be exploited, for instance, to optimise the utilization of the machines as well as their operation and maintenance. However, analyzing the vast amount of generated data is resource demanding both in terms of computing power and network bandwidth, thus requiring highly scalable solutions. This paper presents a novel big data approach and analytics framework for the management and analysis of machine generated data in the cloud. It brings together standard open source technologies and the exploitation of elastic computing, which, as a whole, can be adapted to and deployed on different cloud computing platforms. This enables reducing infrastructure costs, minimizing deployment difficulty and providing on-demand access to a virtually infinite set of computing power, storage and network resources.}
}
@article{GUAY201539,
title = {Early experiences with live migration of SR-IOV enabled InfiniBand},
journal = {Journal of Parallel and Distributed Computing},
volume = {78},
pages = {39-52},
year = {2015},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2015.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0743731515000052},
author = {Wei Lin Guay and Sven-Arne Reinemo and Bjørn Dag Johnsen and Chien-Hua Yen and Tor Skeie and Olav Lysne and Ola Tørudbakken},
keywords = {IO virtualization, VM migration, SR-IOV, Architecture},
abstract = {Virtualization is the key to efficient resource utilization and elastic resource allocation in cloud computing. It enables consolidation, the on-demand provisioning of resources, and elasticity through live migration. Live migration makes it possible to optimize resource usage by moving virtual machines (VMs) between physical servers in an application transparent manner. It does, however, require a flexible, high-performance, scalable virtualized I/O architecture to reach its full potential. This is challenging to achieve with high-speed networks such as InfiniBand and remote direct memory access enhanced Ethernet, because these devices usually maintain their connection state in the network device hardware. Fortunately, the single root IO virtualization (SR-IOV) specification addresses the performance and scalability issues. With SR-IOV, each VM has direct access to a hardware assisted virtual device without the overhead introduced by emulation or para-virtualization. However, SR-IOV does not address the migration of the network device state. In this paper we present and evaluate the first available prototype implementation of live migration over SR-IOV enabled InfiniBand devices.}
}
@article{JENA2020382,
title = {Cloud Computing Tools: Inside Views and Analysis},
journal = {Procedia Computer Science},
volume = {173},
pages = {382-391},
year = {2020},
note = {International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.06.045},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031557X},
author = {Soumya Ranjan Jena and Raju Shanmugam and Kavita Saini and Sanjay Kumar},
keywords = {CloudSim, Cloud Analyst, Cloud Reports, Cloudsched, Green Cloud},
abstract = {Cloud computing tools can be segregated into various categories as indicated by their highlights. In this area, we have made an extensive investigation with various classes on the basis of the following parameters such as broadening, correlation and classification. In this paper, we have taken some of the important open-source Cloud tools for comparative study for their better utility. It is very much useful if we can optimize codes in subtleties, growing new calculations for all the users involved in various processes and its design. The five open-source Cloud computing tools, more specifically CloudSim, Cloud Analyst, Cloud Reports, Cloudsched and Green Cloud are illustrative of many related test systems differ from each other on the parameters like architecture design, modeling elements, simulation process, performance metrics and scalability. These tools have regular highlights by their commercial vendors, particularly in design, displaying components, and reproduction process. These are some of the important characteristics, which can be utilized to study the utilities of these tools, for example: concentrating on different layers and with several execution measurements.}
}
@article{WAN201897,
title = {Application deployment using Microservice and Docker containers: Framework and optimization},
journal = {Journal of Network and Computer Applications},
volume = {119},
pages = {97-109},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518302273},
author = {Xili Wan and Xinjie Guan and Tianjing Wang and Guangwei Bai and Baek-Yong Choi},
keywords = {Application deployment, Microservice architecture, Docker container},
abstract = {To improve the scalability and elasticity of application deployment and operation in cloud computing environments, new architectures and techniques are developed and studied, e.g., microservice architecture, and Docker container. Especially, Docker container enables the sharing on operation system and supporting libraries, which is more lightweight, prompt and scalable than Hypervisor based virtualization. These features make it ideally suit for applications deployed in microservice architecture. However, existing models and schemes, which are mostly designed for Hypervisor based virtualization techniques, fall short to be efficiently used for Docker container based application deployment. To take the benefits of microservice architecture and Docker containers, we explore the optimization of application deployment in cloud data centers using microservice and Docker containers. Our goal is to minimize the application deployment cost as well as the operation cost while preserving service delay requirements for applications. In this paper, we first formulate the application deployment problem by examining the features of Docker, the requirements of microservice-based applications, and available resources in cloud data centers. We further propose a communication efficient framework and a suboptimal algorithm to determine the container placement and task assignment. The proposed algorithm works in a distributed and incremental manner, which makes it scalable to massive physical resources and diverse applications under the framework. We validate the efficiency of our solution through comparisons with three existing strategies in Docker Swarm using real traces from Google Cluster. The evaluation results show that the proposed framework and algorithm provide more flexibility and save more cost than existing strategies.}
}
@article{MIDYA201858,
title = {Multi-objective optimization technique for resource allocation and task scheduling in vehicular cloud architecture: A hybrid adaptive nature inspired approach},
journal = {Journal of Network and Computer Applications},
volume = {103},
pages = {58-84},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517303958},
author = {Sadip Midya and Asmita Roy and Koushik Majumder and Santanu Phadikar},
keywords = {Vehicular cloud, Cloudlet, Resource allocation, Task scheduling, Genetic algorithm, Adaptive nature inspired algorithm, Task response time, Virtual machine utilization, Energy consumption},
abstract = {With onset of Intelligent Transport Systems, vehicles are equipped with internet enabled powerful computation units that provide smart driving assistance, along with various infotainment applications. These applications require web assistance and high computation power, which cannot be executed by standalone onboard units of the smart vehicles. Third party infrastructures like centralized cloud and cloudlets are introduced, to meet the requirement for such vehicular, web based, resource hungry applications. Offloading jobs to centralized cloud, exhausts network bandwidth and causes network delay, whereas frequent offloading to cloudlet results in resource starvation due to limited cloudlet resources. These problems lead to the introduction of Vehicular Cloud Computing (VCC) where the onboard units of several local smart vehicles collectively form a cloud. The concept of multi layered cloud brings centralized cloud, cloudlet and vehicular cloud together to coexist and provide on demand services to mobile and vehicular users. In this work, a three tier architecture is proposed consisting of vehicular cloud, roadside cloudlet and centralized cloud. We have developed an optimized resource allocation and task scheduling algorithm to efficiently serve huge number of task requests arriving from on road users, while maintaining improved Quality of Service. These task requests are optimally mapped to cloud resources among the three cloud layers. The optimization process is carried out using the proposed Hybrid Adaptive Particle Swarm Optimization (HAPSO) algorithm which is a combination of Genetic Algorithm and Adaptive Particle Swarm Optimization. Further the proposed model is simulated using SUMO 0.30.0, NS 3.26 and MATLAB R2014a. The results show that for this problem domain, HAPSO converges faster than Standard Particle Swarm Optimization and Self Adaptive Particle Swarm Optimization with ~98.92% reduction in mean square error. The results also show ~34% improvement in task response time and reduced energy consumption upto ~32.5%.}
}
@article{POLTER201745,
title = {Towards an Adaptive Civil Engineering Computation Framework},
journal = {Procedia Engineering},
volume = {196},
pages = {45-51},
year = {2017},
note = {Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.07.171},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817330308},
author = {Michael Polter and Raimar Scherer},
keywords = {bim, cloud, grid, integration platform, soa},
abstract = {Virtual desktops and virtual applications nowadays become popular in civil engineering companies. Current developments of software vendors trend to the outsourcing of the engineer's workstation to the cloud. However, in the foreseeable future companies will rely on hybrid environments where traditional tasks are handled by traditional applications and cloud platforms are used for more dynamical business areas. Furthermore, stakeholders in construction projects are geographically spread and work together only temporary. Traditional applications never were designed for this scenario. In this paper we introduce the BIMgrid, a work in progress to develop an innovative, adaptive infrastructure integration framework for civil engineering applications, based on private grid and public cloud resources. Although some middleware solutions for the efficient combination of hardware already exist, they are mostly based on UNIX and demand advanced technological knowledge to implement. We present our current state of work and give an overview about the architecture of the BIMgrid, the integration into an IT infrastructure, data management and the interfaces to use it's features with common applications. The prototype implementation is designed as a layered, service-oriented architecture (SOA) and offers several interfaces. Data Management in the BIMgrid will be based on vendor specific data formats and the Industry Foundation Classes (IFC) standard and hence supports the interoperability between applications. Our prototype has been developed in several research projects and tested by industry partners under real conditions. We expect to reduce costs of small and medium-sized enterprises (SMEs) through the automation of complex workflows, optimization of data transfer and reduction of the communication effort between stakeholders. Further we want to make benefits of grid and cloud computing accessible to companies, without dissipation of so far made hard- and software investments.}
}
@article{PSYCHAS2020657,
title = {Cloud toolkit for Provider assessment, optimized Application Cloudification and deployment on IaaS},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {657-667},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329357},
author = {A. Psychas and J. Violos and F. Aisopos and A. Evangelinou and G. Kousiouris and I. Bouras and T. Varvarigou and G. Xidas and D. Charilas and Y. Stavroulas},
keywords = {Service oriented architectures, Cloud computing, Application profiling, Interference effects, Benchmarking, Quality of Experience (QoE)},
abstract = {The deployment of a data-intensive application to a Cloud poses a number of serious challenges, mainly concerning the provider and resources selection process, based on the Quality of Service expected, as well as the management of the Virtual Machines in the provider premises. This work attempts to address those issues by providing a sophisticated toolkit assisting both the Cloud adopter and the Cloud Provider, in terms of application profiling and categorization, analyzing and predicting interference effects, benchmarking and exploiting Quality of Experience in Cloud computing infrastructures. Thus, a service oriented computing architecture is presented, realizing this toolkit in the context of the CloudPerfect framework, and a set of experiments is carried out, evaluating those tools in terms of effectiveness and efficiency. Finally, a specific Use Case scenario is analyzed, that involves an Infrastructure as a Service adoption for an EPR/CRM System.}
}
@article{SOIN2014287,
title = {A hybrid mobile environmental and population density management system for smart poultry farms},
journal = {Computers and Electronics in Agriculture},
volume = {109},
pages = {287-301},
year = {2014},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2014.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0168169914002518},
author = {Chakchai So-In and Sarayut Poolsanguan and Kanokmon Rujirakul},
keywords = {Hybrid architecture, Evaporative cooling, Smart farming, Mobile cloud computing, Mobile management system, Wireless sensor network},
abstract = {Due to advances in technology and size reduction of portable mechanical and electronic devices, sensors have been involved in many sectors, including in agriculture, where sensors and their unique functionalities are extremely useful for both productivity gains and reducing operating costs. These benefits are attained by utilizing their real-time sensing capability over environmental stages affecting the animal breed to enable on-time decision support but with some limitations, i.e., mobility, coverage, ubiquitous access, and energy. Thus, this research focuses on the integration of wireless sensor and mobile system networks with a well-known sensor integration platform toward cloud offloading scalability services via a hybrid architecture used to collect sensing data, such as temperature, humidity, light intensity, and population density, for data analytics and then issuing on-time decisions to adjust the environmental behavior accordingly. Based on a smart poultry farm concept for evaporative cooling environments, the instrument and components of the system design are discussed in detail with the experienced selection criteria, including a discussion of practical topology and deployments, enhanced transmission logics, external environmental tuning control logics integrating mobile user management interfaces, and image processing units. Aside from the proposed prototype of the integration of mobile phones, sensors, and controllers, an experimental investigation was also performed on data sensing and transmission procedures regarding power consumption characteristics, especially on high-cost image data transmissions, including an illustration of the outstanding performance, i.e., 80% in accuracy with low computational complexity, of the image filter over other well-known image classification techniques.}
}
@article{TANG2015116,
title = {Efficient hardware implementation of PMI+ for low-resource devices in mobile cloud computing},
journal = {Future Generation Computer Systems},
volume = {52},
pages = {116-124},
year = {2015},
note = {Special Section: Cloud Computing: Security, Privacy and Practice},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2014.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X14002489},
author = {Shaohua Tang and Bo Lv and Guomin Chen and Zhiniang Peng and Adama Diene and Xiaofeng Chen},
keywords = {Multivariate Quadratic (MQ) public key algorithm, PMI+ encryption and decryption, Hardware implementation, Mobile cloud computing, Low-resource devices, Optimized large power operation},
abstract = {With rapid development of cloud computing, security issues have gained more and more attention, especially in mobile cloud computing environment. Smart phones and other mobile devices provide a lot of convenience to us, but due to its intrinsic low-resource limitation, it also causes many security problems. In this paper, we design a hardware that can efficiently implement PMI+, which is a Multivariate Quadratic (MQ) asymmetric cipher, for low-resource devices in mobile cloud computing. Our main contributions are that, firstly, hardware architectures of encryption and decryption of PMI+ are developed, and descriptions of corresponding hardware algorithm are proposed; secondly, basic arithmetic units are implemented with higher efficiency that multiplication, squaring, vector dot product and power operation are implemented in full parallel; and thirdly, optimized implementations for core modules, including optimized large power operation, are achieved. The encryption and decryption hardware of PMI+ is efficiently realized on FPGA by the above optimization and improvement. It is verified by experiments that the designed hardware can complete an encryption operation within 497 clock cycles, and the clock frequency can be up to 145.60 MHz, and the designed hardware can complete a decryption operation within 438 clock cycles wherein the clock frequency can be up to 132.21 MHz. Our experiment results also confirm that our design can be deployed in low-resource devices as thin client of mobile cloud computing.}
}
@article{LEVITIN2020106969,
title = {Optimal early warning defense of N-version programming service against co-resident attacks in cloud system},
journal = {Reliability Engineering & System Safety},
volume = {201},
pages = {106969},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2020.106969},
url = {https://www.sciencedirect.com/science/article/pii/S0951832019311949},
author = {Gregory Levitin and Liudong Xing and Yanping Xiang},
keywords = {Cloud service, Co-resident attack, Early warning, -version programming, Service corruption},
abstract = {Due to the virtual machine co-resident architecture, cloud computing systems are vulnerable to co-resident attacks (CRAs) where a malicious attacker may access and corrupt information of a target user through co-locating their virtual machines on the same physical server. To defend against cyber threats such as the CRA, early warning mechanisms have been developed with the aim to detect and block an attack at a nascent stage. In this paper, we study the optimal strategy of allocating early warning resources to defend against CRAs for the voting-based N-version programming (NVP) service running in the cloud. A probabilistic model is proposed to evaluate the failure probability of the NVP service program and further the expected cost of loss for the considered service. Optimization problems of co-determining the optimal numbers of service program versions and early warning agents are further solved to minimize the expected cost of loss. As demonstrated through examples, the resultant optimal strategies can effectively allocate service and defense resources to defend the NVP cloud service against CRAs.}
}
@article{JUN20171356,
title = {The Cloud Technology Double Live Data Center Information System Research and Design Based on Disaster Recovery Platform},
journal = {Procedia Engineering},
volume = {174},
pages = {1356-1370},
year = {2017},
note = {13th Global Congress on Manufacturing and Management Zhengzhou, China 28-30 November, 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.289},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817302898},
author = {Yu jun and Yang lihong},
keywords = {Double live data, disaster recovery storage, virtual storage, transparent encryption, filter driver},
abstract = {In the current information age, usually the data resources are much higher than the value of the computer equipment itself. In recent years, with the rapid development of network technology, the formation of a high concentration of data stored in the network environment, the simplified data management, improve operational efficiency, reduce the cost of investment at the same time, the data security is experiencing more severe test. As a kind of data disaster recovery data security technology, through a complete copy of the data in real time to the disaster recovery center, in order to keep a copy of the basic means of providing a last line of defense for the security of the data. Since the disaster recovery started, people have made a lot of research, but there are still some problems first, as the diversification of the storage subsystem structure of infrastructure, so to build a disaster recovery system need to do a lot of personalized treatment followed by a single, storage technology has been unable to meet the needs of various industries on the disaster intensity of third and if the security of the backup data cannot be guaranteed, for sensitive confidential data, even worse than no backup. To solve the above problems, this paper related to the key technology of disaster recovery data backup based on virtual storage research, the main work is as follows. Based on the characteristics of storage technology in disaster tolerant system in the analysis, choose to use the virtual storage technology support as the basis of disaster recovery system to realize data backup, and through in-depth analysis of the virtual storage technology and its application status in the disaster recovery system, pointed out that the current application of virtual storage technology in disaster tolerant system in the challenge and the starting point of this article. Analysis of the structure of the internal computer system with virtual memory and the disaster recovery system based on the needs of the design of a virtual network storage model and disaster tolerant architecture based on this model. The advantages and disadvantages of three virtual memory mapping mechanism at present, designed for disaster recovery system virtual memory mapping mechanism and cache mechanism. On the basis of this mapping mechanism is established on data disaster tolerance strategy can be realized, and the upper user language abstraction, form a complete set of disaster recovery mechanism. Through a detailed analysis of the virtual disk transparent encryption mechanism, proposed the use of the mechanism to realize the transparent protection of backup data, combined with the characteristics of backup data, and optimized the virtual disk transparent encryption mechanism, focus on strengthening the management of keys, the encryption mechanism is analyzed on how the role of disaster recovery system. Quasi double live disaster recovery storage based cloud computing data center technology problems of the equipment malfunction need to manually switch the business system, business recovery time. In order to solve this problem, this paper uses Net App 3250 double live disaster recovery storage technology Metro Cluster to build a double live a real sense of cloud computing data center. When the double live data center renyiyitai main memory based equipment malfunction, another main memory automatically and quickly take over the business, to achieve zero down live migration, to ensure the continued operation of the business system, provide a solid guarantee for the information construction unit.}
}
@article{SHUJA2016335,
title = {Towards native code offloading based MCC frameworks for multimedia applications: A survey},
journal = {Journal of Network and Computer Applications},
volume = {75},
pages = {335-354},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301898},
author = {Junaid Shuja and Abdullah Gani and Muhammad Habib ur Rehman and Ejaz Ahmed and Sajjad A. Madani and Muhammad Khurram Khan and Kwangman Ko},
keywords = {ARM, Virtualization, Emulation, Mobile Cloud Computing, Dynamic Binary Translation, SIMD},
abstract = {A number of resource-intensive applications, such as augmented reality, natural language processing, object recognition, and multimedia-based software are pushing the computational and energy boundaries of smartphones. Cloud-based services augment the resource-scare capabilities of smartphones while offloading compute-intensive methods to resource-rich cloud servers. The amalgam of cloud and mobile computing technologies has ushered the rise of Mobile Cloud Computing (MCC) paradigm which envisions operating smartphones and modern mobile devices beyond their intrinsic capabilities. System virtualization, application virtualization, and dynamic binary translation (DBT) techniques are required to address the heterogeneity of smartphone and cloud architectures. However, most of the current research work has only focused on the offloading of virtualized applications while giving limited consideration to native code offloading. Moreover, researchers have not attended to the requirements of multimedia based applications in MCC offloading frameworks. In this study, we present a survey and taxonomy of state-of-the-art MCC frameworks, DBT techniques for native offloading, and cross-platform execution techniques for multimedia based applications. We survey the MCC frameworks from the perspective of offload enabling techniques. We focus on native code offloading frameworks and analyze the DBT and emulation techniques of smartphones (ARM) on a cloud server (x86) architectures. Furthermore, we debate the open research issues and challenges to native offloading of multimedia based smartphone applications.}
}
@article{AZEEZ201997,
title = {Security and privacy issues in e-health cloud-based system: A comprehensive content analysis},
journal = {Egyptian Informatics Journal},
volume = {20},
number = {2},
pages = {97-108},
year = {2019},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866517302797},
author = {Nureni Ayofe Azeez and Charles Van {der Vyver}},
keywords = {E-Health, Security and privacy, Cloud, Vulnerability, Access control},
abstract = {The recent advancement in Information and Communication Technology (ICT) has undoubtedly improved services in all sectors in the world. Specifically, Information Technology (IT) has led to a very vital innovation in health sector called electronic health (e-Health). In order to optimize full and excellent benefits of this innovation, its implementation in a cloud-based environment is important. However, with noticeable and numerous benefits inherent from e-Health in a cloud computing, its full utilization is still being hampered by challenges of security and privacy. In this paper, we focused on extensive review of current and existing literatures of various approaches and mechanisms being used to handle security and privacy related matters in e-Health. Strengths and weaknesses of some of these approaches were enunciated. The literature review was carried out after selecting over One Hundred and Ten (1 1 0) original articles and figured out several models adopted in their solutions. After comparing models used, we arrived at the reviewed articles. Reviewed articles were narrowed down to the current number because of similarity observed in the models adopted by some researchers. Also, we give an acceptable and standard definition of e-Health. Effort was made to classify cloud-based models. Security and privacy requirements as recommended by Health Insurance Portability and Accountability Act (HIPAA) were also discussed and provided. Remarks and recommendations were made regarding the review process and future directions on security and privacy of e-Health in cloud computing was also provided. Finally, authors propose a secured and dependable architecture for electronic health that could guarantee efficiency, reliability and regulated access framework to health information. The architecture, though is currently under implementation, will guarantee absolute security and privacy between healthcare providers and the patients.}
}
@article{VERABAQUERO2016793,
title = {Real-time business activity monitoring and analysis of process performance on big-data domains},
journal = {Telematics and Informatics},
volume = {33},
number = {3},
pages = {793-807},
year = {2016},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0736585315301167},
author = {Alejandro Vera-Baquero and Ricardo Colomo-Palacios and Owen Molloy},
keywords = {Big data, Cloud computing, Business activity monitoring, Business Process Improvement},
abstract = {Real-time access to business performance information is critical for corporations to run a competitive business and respond to a continuously changing business environment with ever-higher levels of competition. The timely analysis and monitoring of business processes are essential to identify non-compliant situations and react immediately to those inconsistencies in order to respond quickly to competitors. In this regard, the integration of business intelligence (BI) systems with Process Aware Information Systems (PAIS) can become a key tool for business users in decision making. However, current BI systems are not suitable for optimising and improving end-to-end processes since these are normally business domain specific and are not sufficiently process-aware to support the needs of process improvement type activities. In addition, highly transactional business environments may produce vast amounts of event data that cannot be efficiently managed by the use of traditional storage systems which are not designed to manage vast amounts of event data. We introduce a cloud-based architecture that leverages big-data technology to support performance analysis on any business domain, in a timely manner and regardless of the underlying concerns of the operational systems. Likewise, we demonstrate the ability of the solution to provide real-time business activity monitoring on big-data environments with low hardware costs.}
}
@article{UM2014553,
title = {Factory Planning System Considering Energy-efficient Process under Cloud Manufacturing},
journal = {Procedia CIRP},
volume = {17},
pages = {553-558},
year = {2014},
note = {Variety Management in Manufacturing},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2014.01.084},
url = {https://www.sciencedirect.com/science/article/pii/S2212827114003369},
author = {Jumyung Um and Yong-Chan Choi and Ian Stroud},
keywords = {Cloud manufacturing, Factory planning, Order planning, Manufacturing network, STEP-NC, Energy-efficient process, Last minute selection, Milling machine},
abstract = {Cloud computing sets to make a change in business between enterprises over the internet based on services providing dynamically reconfigurable and virtualized networks. It is difficult for an aircraft manufacturer to consider the performance and status of each factory (i.e., capacity, inventory, order, technology, etc.) because of the huge amount of suppliers. In legacy environments, the supplier consumed a long time to identify the machine and process required and the energy to be consumed in order to satisfy a new customer demand. If the process occurred with a new order should be optimized, additional time and cost were needed. Cloud computing can enable the relationship between the manufacturer and suppliers of aircraft, which is representative of mass customized products, to be flexible and efficient. In order to enhance the response time during contracting with suppliers, this paper focuses on an architecture, providing a service for a decision with the concerns about the time and cost as well as energy with regard to process planning. For realizing this service the manufacturer provides the process plan described using neutral language for numerical controllers called ‘STEP-NC’ which is independent from a specific machine tool. On the supplier side, the machinability check, tool path generation and energy use estimation should be identified with machine tool specifications described by STEP-NC. In the Cloud computing environment, the analysis is performed to check the possibility that a supplier can complete a new order from the manufacturer on the due date with the analysis of machine tools and on-going production schedule. This function is developed in a system which can be installed on a virtual platform and provided by a cloud service.}
}
@article{YAN201756,
title = {Cloud robotics in Smart Manufacturing Environments: Challenges and countermeasures},
journal = {Computers & Electrical Engineering},
volume = {63},
pages = {56-65},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617313976},
author = {Hehua Yan and Qingsong Hua and Yingying Wang and Wenguo Wei and Muhammad Imran},
keywords = {Cloud robotics, Smart manufacturing, Simultaneous localization and mapping, Cloud computing},
abstract = {In Smart Manufacturing Environments (SME), the use of cloud robotics is based on the integration of cloud computing and industrial robots, which provides a new technological approach to task execution and resource sharing compared to traditional industrial robots. However, research on cloud robotics in SME still faces some challenges. First, highly flexible load scheduling mechanisms are immature. Second, traditional optimization mechanisms for the network service quality do not meet the requirements of smart manufacturing due to time variability and service quality dynamics. And, finally, existing learning algorithms used without cloud-assisted resources cause great resource wasting. Accordingly, this paper explores main technologies related to cloud robotics in SME. The research contents include self-adaptive adjustment mechanisms for the service quality of a cloud robot network, computing load allocation mechanisms for cloud robotics, and group learning based on a cloud platform. The results presented in this paper are helpful to understand the internal mechanisms of perception and interaction, intelligent scheduling and control of cloud robot systems oriented to smart manufacturing, and the design of a cloud architecture oriented to group learning.}
}
@article{CALA2016153,
title = {Scalable and efficient whole-exome data processing using workflows on the cloud},
journal = {Future Generation Computer Systems},
volume = {65},
pages = {153-168},
year = {2016},
note = {Special Issue on Big Data in the Cloud},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16000030},
author = {J. Cała and E. Marei and Y. Xu and K. Takeda and P. Missier},
keywords = {Workflow-based application, Whole-exome sequencing, Performance analysis, Cloud computing, HPC},
abstract = {Dataflow-style workflows offer a simple, high-level programming model for flexible prototyping of scientific applications as an attractive alternative to low-level scripting. At the same time, workflow management systems (WFMS) may support data parallelism over big datasets by providing scalable, distributed deployment and execution of the workflow over a cloud infrastructure. In theory, the combination of these properties makes workflows a natural choice for implementing Big Data processing pipelines, common for instance in bioinformatics. In practice, however, correct workflow design for parallel Big Data problems can be complex and very time-consuming. In this paper we present our experience in porting a genomics data processing pipeline from an existing scripted implementation deployed on a closed HPC cluster, to a workflow-based design deployed on the Microsoft Azure public cloud. We draw two contrasting and general conclusions from this project. On the positive side, we show that our solution based on the e-Science Central WFMS and deployed in the cloud clearly outperforms the original HPC-based implementation achieving up to 2.3× speed-up. However, in order to deliver such performance we describe the importance of optimising the workflow deployment model to best suit the characteristics of the cloud computing infrastructure. The main reason for the performance gains was the availability of fast, node-local SSD disks delivered by D-series Azure VMs combined with the implicit use of local disk resources by e-Science Central workflow engines. These conclusions suggest that, on parallel Big Data problems, it is important to couple understanding of the cloud computing architecture and its software stack with simplicity of design, and that further efforts in automating parallelisation of complex pipelines are required.}
}
@article{AMATO201752,
title = {Model transformations of MapReduce Design Patterns for automatic development and verification},
journal = {Journal of Parallel and Distributed Computing},
volume = {110},
pages = {52-59},
year = {2017},
note = {High Performance and Parallelism for Large Data Sets},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2016.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0743731516301939},
author = {Flora Amato and Francesco Moscato},
keywords = {Cloud computing, Cloud patterns, Formal verification, MapReduce, Model driven engineering},
abstract = {Mapping MapReduce frameworks to Cloud Architecture became a must in last years because of the need of managing large data sets and Big Data in fast, reliable (and as cheap as possible) way. Scientific Literature proposes many works about new architectures, frameworks and algorithms improving and optimizing performances while performing MapReduce operations. Anyway, MapReduce framework is only the starting point for building a plethora of services based on different analyses. This is the reason for recent application of Design Patterns methodologies to develop MapReduce applications. Here we propose a Model Driven Engineering methodology to design, verify and develop MapReduce applications on Cloud Systems. The methodology is driven by MapReduce Design Patterns and is used to analyse soundness and reliability of services based on MapReduce from early design stage to runtime.}
}
@article{LUO2019106265,
title = {Optimizing dynamic survivability and security of replicated data in cloud systems under co-residence attacks},
journal = {Reliability Engineering & System Safety},
volume = {192},
pages = {106265},
year = {2019},
note = {Complex Systems RAMS Optimization: Methods and Applications},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2018.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0951832017314321},
author = {Liang Luo and Liudong Xing and Gregory Levitin},
keywords = {Cloud system, Co-residence attack, Data corruption, Data replication, Data survivability, Data security, Virtual machine},
abstract = {Utilizing the virtualization technology, multiple virtual machines (VMs) can be created on a single physical server for different tasks, enabling cost-effective resource sharing in cloud computing systems. However, this co-resident VM architecture can be exploited by malicious attackers, posing unique survivability and security risks for cloud users. This paper addresses one of such risks called co-residence attacks, where a malicious attacker can steal or corrupt a user's sensitive information through co-residing the attacker's VM with the target user's VM on the same physical server. We model users’ data protection policy in which sensitive data are replicated and stored on different VMs to enhance data survivability. Both user's and attacker's VMs are distributed among cloud servers at random. The arrival of attacker's requests for creating VMs is modeled by a Poisson stochastic process. We propose a probabilistic model to obtain dynamic data survivability and security indices. Based on the suggested evaluation model, dynamic data replication policies are analyzed and optimized. Numerical examples are presented to demonstrate impacts of different model parameters on the dynamic data survivability and security.}
}
@incollection{CHOU2016397,
title = {16 - Big data analytics and cloud computing for sustainable building energy efficiency},
editor = {Fernando Pacheco-Torgal and Erik Rasmussen and Claes-Göran Granqvist and Volodymyr Ivanov and Arturas Kaklauskas and Stephen Makonin},
booktitle = {Start-Up Creation},
publisher = {Woodhead Publishing},
pages = {397-412},
year = {2016},
isbn = {978-0-08-100546-0},
doi = {https://doi.org/10.1016/B978-0-08-100546-0.00016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081005460000169},
author = {J.-S. Chou and N.-T. Ngo and W.K. Chong and G.E. Gibson},
keywords = {Artificial intelligence, Big data analytics, Building energy management, Cloud computing, Data mining, Decision support system, Energy efficiency, Machine learning, Nature-inspired metaheuristic optimization, Pattern recognition, Smart grid, Time-series data, Web-based system},
abstract = {Currently, big data analytics and cloud computing are emerging practices for sustainable energy systems and efficient energy management. Utilizing building energy usage data is critical for the successful deployment of energy efficiency. This chapter presents the framework of a smart decision support system (SDSS) that integrates smart grid big data analytics and cloud computing for building energy efficiency. The framework is based on a layered architecture that includes smart grid and data collection, an analytics bench, and a web-based portal. A real-world smart metering infrastructure was installed in a residential building for the experiment. The SDSS is expected to accurately identify the building energy consumption patterns and forecasted future energy usage. Moreover, end users can reduce electricity costs by using the system to optimize operation schedules of appliances, lighting systems, and heating, ventilation, and air conditioning. The proposed framework serves as a start-up creation in an application of big data analytics and cloud computing technology for sustainable building energy efficiency.}
}
@incollection{MEHDIPOUR201659,
title = {Chapter Two - Energy-Efficient Big Data Analytics in Datacenters},
editor = {Ali R. Hurson and Hamid Sarbazi-Azad},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {100},
pages = {59-101},
year = {2016},
booktitle = {Energy Efficiency in Data Centers and Clouds},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2015.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245815000613},
author = {Farhad Mehdipour and Hamid Noori and Bahman Javadi},
keywords = {Datacenter, Big data analytics, Cloud computing, Virtual machine, Interconnection networks, Energy and power optimization},
abstract = {The volume of generated data increases by the rapid growth of Internet of Things, leading to the big data proliferation and more opportunities for datacenters. Highly virtualized cloud-based datacenters are currently considered for big data analytics. However, big data requires datacenters with promoted infrastructure capable of undertaking more responsibilities for handling and analyzing data. Also, as the scale of the datacenter is increasingly expanding, minimizing energy consumption and operational cost is a vital concern. Future datacenters infrastructure including interconnection network, storage, and servers should be able to handle big data applications in an energy-efficient way. In this chapter, we aim to explore different aspects of could-based datacenters for big data analytics. First, the datacenter architecture including computing and networking technologies as well as datacenters for cloud-based services will be illustrated. Then the concept of big data, cloud computing, and some of the existing cloud-based datacenter platforms including tools for big data analytics will be introduced. We later discuss the techniques for improving energy efficiency in the cloud-based datacenters for big data analytics. Finally, the current and future trends for datacenters in particular with respect to energy consumption to support big data analytics will be discussed.}
}
@article{LOU2018171,
title = {Failure prediction by relevance vector regression with improved quantum-inspired gravitational search},
journal = {Journal of Network and Computer Applications},
volume = {103},
pages = {171-177},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517303946},
author = {Jungang Lou and Yunliang Jiang and Qing Shen and Ruiqin Wang},
keywords = {Cloud computing, Failure prediction, Relevance vector machine, Cloud security architectures},
abstract = {Modern data centers coordinate hundreds of thousands of heterogeneous tasks aiming at providing highly reliable cloud computing services. Failure prediction is of vital importance in the analysis of cloud reliability. Recently, a novel kernel learning method called relevance vector machine (RVM) has been widely applied to solve nonlinear predicting problems and has been verified to perform well in many situations. However, it remains a great challenge for existing approaches to acquire the optimal RVM parameters. In this research, an artificial immune system is introduced into a Quantum-inspired Binary Gravitational Search Algorithm (QBGSA) in order to improve the convergence rate of standard QBGSA. In addition, a hybrid model of RVM with improved QBGSA called IQBGSA-RVM is proposed that aims to predict the failure time of cloud services. To evaluate the effectiveness of IQBGSA-RVM in failure prediction, its predicting performance is compared with that of the following algorithms, all of which employs RVM: chaotic genetic algorithms, binary gravitational search algorithms, binary particle swarm optimization, quantum-inspired binary particle swarm optimization and standard QBGSA. The experimental results show that the IQBGSA-RVM model is either comparable to the other models or it outperforms them, to say the least.}
}
@article{KHOMH2018151,
title = {Understanding the impact of cloud patterns on performance and energy consumption},
journal = {Journal of Systems and Software},
volume = {141},
pages = {151-170},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300621},
author = {Foutse Khomh and S. Amirhossein Abtahizadeh},
keywords = {Cloud patterns, Energy consumption, Performance optimization, Energy efficiency},
abstract = {Cloud patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. In this work, we conduct an empirical study on two multi-processing and multi-threaded applications deployed in the cloud, to investigate the individual and the combined impact of six cloud patterns (Local Database Proxy, Local Sharding Based Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and Filters) on the energy consumption. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud-based application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Moreover, our findings show that migrating an application to a microservices architecture can improve the performance of the application, while significantly reducing its energy consumption. We summarize our contributions in the form of guidelines that developers and software architects can follow during the implementation of a cloud-based application.}
}
@article{CORRADI2014118,
title = {VM consolidation: A real case based on OpenStack Cloud},
journal = {Future Generation Computer Systems},
volume = {32},
pages = {118-127},
year = {2014},
note = {Special Section: The Management of Cloud Systems, Special Section: Cyber-Physical Society and Special Section: Special Issue on Exploiting Semantic Technologies with Particularization on Linked Data over Grid and Cloud Architectures},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2012.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X12001082},
author = {Antonio Corradi and Mario Fanelli and Luca Foschini},
keywords = {Cloud computing, VM consolidation, OpenStack},
abstract = {In recent years, Cloud computing has been emerging as the next big revolution in both computer networks and Web provisioning. Because of raised expectations, several vendors, such as Amazon and IBM, started designing, developing, and deploying Cloud solutions to optimize the usage of their own data centers, and some open-source solutions are also underway, such as Eucalyptus and OpenStack. Cloud architectures exploit virtualization techniques to provision multiple Virtual Machines (VMs) on the same physical host, so as to efficiently use available resources, for instance, to consolidate VMs in the minimal number of physical servers to reduce the runtime power consumption. VM consolidation has to carefully consider the aggregated resource consumption of co-located VMs, in order to avoid performance reductions and Service Level Agreement (SLA) violations. While various works have already treated the VM consolidation problem from a theoretical perspective, this paper focuses on it from a more practical viewpoint, with specific attention on the consolidation aspects related to power, CPU, and networking resource sharing. Moreover, the paper proposes a Cloud management platform to optimize VM consolidation along three main dimensions, namely power consumption, host resources, and networking. Reported experimental results point out that interferences between co-located VMs have to be carefully considered to avoid placement solutions that, although being feasible from a more theoretical viewpoint, cannot ensure VM provisioning with SLA guarantees.}
}
@article{NOUREDDINE20132269,
title = {An authentication model towards cloud federation in the enterprise},
journal = {Journal of Systems and Software},
volume = {86},
number = {9},
pages = {2269-2275},
year = {2013},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212003408},
author = {M. Noureddine and R. Bashroush},
keywords = {Engineering for federated clouds, Cloud security, Cloud performance},
abstract = {Cloud computing has emerged as a new paradigm which brought business opportunities as well as software engineering challenges. In The Cloud computing, business participants such as service providers, enterprise solutions, and marketplace applications are required to adopt a cloud architecture engineered for security and performance. Marketplace applications offer a great opportunity for enterprises to employ new Cloud capabilities to add value and extend business functionality. One of the major hurdles of formal adoption of Marketplace in the enterprise is performance. Enterprise applications (e.g. Lync Server, SAP, SharePoint, and Exchange Server) require a mechanism to predict and manage performance expectations. In previous research, we provided optimization for OAuth 2.0 adoption in the Enterprise. In this research, we extend the optimization to include identity federation in the Marketplace. This optimization is achieved by introducing provisioning steps to pre-establish trust amongst enterprise applications’ Resource Servers, its associated Authorization Server and the clients interested in access to protected resources. We then introduce the notion of referral tokens to enable Marketplace applications federation across organizations. In this architecture, trust is provisioned and synchronized as a pre-requisite step to authentication amongst all communicating entities in OAuth protocol, and referral tokens are used to establish trust federation for Marketplace applications across organizations. A real-life case study and a simulation test were used to validate the results.}
}
@article{APOSTU2012543,
title = {Modeling Cloud Architecture in Banking Systems},
journal = {Procedia Economics and Finance},
volume = {3},
pages = {543-548},
year = {2012},
note = {International Conference Emerging Markets Queries in Finance and Business, Petru Maior University of Tîrgu-Mures, ROMANIA, October 24th - 27th, 2012},
issn = {2212-5671},
doi = {https://doi.org/10.1016/S2212-5671(12)00193-1},
url = {https://www.sciencedirect.com/science/article/pii/S2212567112001931},
author = {Anca Apostu and Emanuil Rednic and Florina Puican},
keywords = {Cloud computing, optimization, banking, cloud model},
abstract = {With the development of the Internet's new technical functionalities, new concepts have started to take shape. These concepts have an important role especially in the development of corporate IT. Such a concept is “the Cloud”. Cloud computing represents a big change in the way computing is done in corporations. It encompasses all the optimizations that a company needs in order to succeed nowadays. Among the base elements of many familiar technologies across the years there were the distributed systems that contributed to the development of Cloud computing, Grid computing and Utility computing. From our work experience we are going to analyze and asses the cloud adoption decision in a banking environment's context. This paper presents a use-case that shows business problems addressed by using Cloud computing and business considerations that influence an organization to use Cloud computing.}
}
@article{ALAM2013194,
title = {5-Layered Architecture of Cloud Database Management System},
journal = {AASRI Procedia},
volume = {5},
pages = {194-199},
year = {2013},
note = {2013 AASRI Conference on Parallel and Distributed Computing and Systems},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2013.10.078},
url = {https://www.sciencedirect.com/science/article/pii/S2212671613000796},
author = {Bashir Alam and M.N. Doja and Mansaf Alam and Shweta Mongia},
keywords = {Cloud Computing, Cloud Database Management System, Database as a Service (DBaaS)},
abstract = {Cloud Database Management System is a new emerging concept recently introduced in the world. In Cloud the concept of Standard architecture of Cloud Database Management System is not yet been implemented. In this paper we are proposing a framework for 5-layered architecture in cloud database management system. First layer introduced is the External Layer, this layer is closest to the user, in which manageability, providing transparency and security are the important issue that should be considered. Second layer is the Conceptual Middleware Layer, as there are heterogeneous databases and clouds are available in the market, so here interoperability is the major issue. Third layer is the Conceptual Layer in which programming techniques, transaction management, query processing and optimization are the issues that should be considered. Forth layer is the Physical Middleware Layer, as there are various platforms available so here also, interoperability between various platforms are the biggest issue and the last layer is the Physical Layer in which how data can be stored so that it can be easily accessible without so much overhead so here data security, storage, backup, load balancing, partitioning, scaling, elasticity, fault tolerance and replication are the important issues that should be considered.}
}
@article{TORDSSON2012358,
title = {Cloud brokering mechanisms for optimized placement of virtual machines across multiple providers},
journal = {Future Generation Computer Systems},
volume = {28},
number = {2},
pages = {358-367},
year = {2012},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X11001373},
author = {Johan Tordsson and Rubén S. Montero and Rafael Moreno-Vozmediano and Ignacio M. Llorente},
keywords = {Cloud computing, Infrastructure as a Service (IaaS), Scheduling, Interoperability},
abstract = {In the past few years, we have witnessed the proliferation of a heterogeneous ecosystem of cloud providers, each one with a different infrastructure offer and pricing policy. We explore this heterogeneity in a novel cloud brokering approach that optimizes placement of virtual infrastructures across multiple clouds and also abstracts the deployment and management of infrastructure components in these clouds. The feasibility of our approach is evaluated in a high throughput computing cluster case study. Experimental results confirm that multi-cloud deployment provides better performance and lower costs compared to the usage of a single cloud only.}
}
@article{LIU20121100,
title = {SDMS-O: A service deployment management system for optimization in clouds while guaranteeing users’ QoS requirements},
journal = {Future Generation Computer Systems},
volume = {28},
number = {7},
pages = {1100-1109},
year = {2012},
note = {Special section: Quality of Service in Grid and Cloud Computing},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X11002226},
author = {Tiejiang Liu and Tun Lu and Wei Wang and Qi Wang and Zhenyu Liu and Ning Gu and Xianghua Ding},
keywords = {Cloud computing, Service family, Installation expression, Deployment optimization, QoS requirements},
abstract = {In a service oriented cloud, in order to meet users’ functional and non-functional requirements, cloud vendors must manage service deployment effectively. During the deployment process, the cost of deployment and its influence on the cloud are very important issues to consider; however, so far little research has been done to address them. In this paper, we present a Service Deployment Management System for Optimization (SDMS-O) designed with a novel optimization approach for service deployment to improve deployment efficiency and reduce deployment cost while guaranteeing the users’ QoS requirements. In SDMS-O, atom-services as its basic units of service applications are first divided into different service families according to compatibility and installation policy, and a service deployment requirement is expressed as an installation expression sequence. We then present three algorithms to automatically standardize, simplify and optimize this sequence during the service deployment process. Meanwhile, the backtracking technique is applied in each optimization phase of service deployment in order not to violate the users’ QoS constraints. The service deployment result such as safety and usability in each optimization step is also evaluated. A simulated experiment of SDMS-O demonstrates our approach’s effectiveness and efficiency.}
}
@article{ASIMAKOPOULOU201383,
title = {Centralized Micro-clouds: An Infrastructure for Service Distribution in Collaborative Smart Devices},
journal = {Procedia Computer Science},
volume = {21},
pages = {83-90},
year = {2013},
note = {The 4th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2013) and the 3rd International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913008065},
author = {Eleana Asimakopoulou and Stelios Sotiriadis and Nik Bessis and Ciprian Dobre and Valentin Cristea},
keywords = {Clouds, Micro-clouds, Internet of Things, SimIC, Message Exchanging Optimization.},
abstract = {In the current information-driven society, the massive use and impact of communications and mobile devices challenge the design of communication networks. This highlights the emergency of a new Internet structure namely the Internet of Things that refers to the transformation of physical objects to smart objects and their communication. Based on that the communication of such objects will offer an augmented infrastructure that is formed dynamically and on the fly based on transient links among objects. This is the concept behind cloud computing, to provide a computer-based environment where various services are available to be consumed by everyday users, anywhere and at anytime. Our vision encompasses a dynamic micro-cloud environment that is formed from devices that share computational power. This encompasses inter-linked smart objects and smart mobile devices available from a smart environment that can be formed dynamically. The proposed micro-cloud notion will be of apparent significance to maintain the required quality of service in dynamic scenarios such as those found in emergency and disaster situations. To represent such system we are focused on the development of such architecture into a novel simulation toolkit that allows the replication of Internet of Things scenarios.}
}
@article{KOLODZIEJ201477,
title = {Security, energy, and performance-aware resource allocation mechanisms for computational grids},
journal = {Future Generation Computer Systems},
volume = {31},
pages = {77-92},
year = {2014},
note = {Special Section: Advances in Computer Supported Collaboration: Systems and Technologies},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2012.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X12001823},
author = {Joanna Kołodziej and Samee Ullah Khan and Lizhe Wang and Marek Kisiel-Dorohinicki and Sajjad A. Madani and Ewa Niewiadomska-Szynkiewicz and Albert Y. Zomaya and Cheng-Zhong Xu},
keywords = {Distributed cyber physical systems, Secure computational grid, Resource reliability, Scheduling, Energy optimization, Dynamic voltage scaling, Evolutionary algorithm},
abstract = {Distributed Cyber Physical Systems (DCPSs) are networks of computing systems that utilize information from their physical surroundings to provide important services, such as smart health, energy efficient grid and cloud computing, and smart security-aware grids. Ensuring the energy efficiency, thermal safety, and long term uninterrupted computing operation increases the scalability and sustainability of these infrastructures. Achieving this goal often requires researchers to harness an understanding of the interactions between the computing equipment and its physical surroundings. Modeling these interactions can be computationally challenging with the resources on hand and the operating requirements of such systems. In this paper, we define the independent batch scheduling in Computational Grid (CG) as a three-objective global optimization problem with makespan, flowtime and energy consumption as the main scheduling criteria minimized according to different security constraints. We use the Dynamic Voltage Scaling (DVS) methodology for reducing the cumulative power energy utilized by the system resources. We develop six genetic-based single- and multi-population meta-heuristics for solving the considered optimization problem. The effectiveness of these algorithms has been empirically justified in two different grid architectural scenarios in static and dynamic modes.}
}
@article{BYUN20111011,
title = {Cost optimized provisioning of elastic resources for application workflows},
journal = {Future Generation Computer Systems},
volume = {27},
number = {8},
pages = {1011-1026},
year = {2011},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X11000744},
author = {Eun-Kyu Byun and Yang-Suk Kee and Jin-Soo Kim and Seungryoul Maeng},
keywords = {Resource capacity estimation, Resource allocation, Application workflow, Cloud computing economy},
abstract = {Workflow technologies have become a major vehicle for easy and efficient development of scientific applications. In the meantime, state-of-the-art resource provisioning technologies such as cloud computing enable users to acquire computing resources dynamically and elastically. A critical challenge in integrating workflow technologies with resource provisioning technologies is to determine the right amount of resources required for the execution of workflows in order to minimize the financial cost from the perspective of users and to maximize the resource utilization from the perspective of resource providers. This paper suggests an architecture for the automatic execution of large-scale workflow-based applications on dynamically and elastically provisioned computing resources. Especially, we focus on its core algorithm named PBTS (Partitioned Balanced Time Scheduling), which estimates the minimum number of computing hosts required to execute a workflow within a user-specified finish time. The PBTS algorithm is designed to fit both elastic resource provisioning models such as Amazon EC2 and malleable parallel application models such as MapReduce. The experimental results with a number of synthetic workflows and several real science workflows demonstrate that PBTS estimates the resource capacity close to the theoretical low bound.}
}
@article{LUO201164,
title = {Research on service-oriented policy-driven IAAS management},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {18},
pages = {64-70},
year = {2011},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(10)60208-7},
url = {https://www.sciencedirect.com/science/article/pii/S1005888510602087},
author = {Xiao-xiang LUO and Mei-na SONG and Jun-de SONG},
keywords = {virtualization, Cloud Computing, resource monitoring, service-oriented},
abstract = {Cloud computing refers to the provision of computational resources on demand via a computer network. It is considered to be the third IT revolution following personal computer revolution and the Internet revolution. IAAS, a kind of service provided by cloud computing, can supply users with the convenience and flexibility of virtual resources to achieve the same power supply as the water supply. Though there are kinds of existing IAAS solutions, but they are all common system architecture design, without taking into account the characteristics of applications resulting in inefficient service provision. In this paper, by referencing service-oriented philosophy IAAS platform is reconstructed. Based on loosely coupled physical resources by virtualization technology, a policy-based management system is provided to optimize service monitoring and service scheduling. Prototype implement show that it will effectively improve the performance of service running on IAAS.}
}
@article{FERRER201266,
title = {OPTIMIS: A holistic approach to cloud service provisioning},
journal = {Future Generation Computer Systems},
volume = {28},
number = {1},
pages = {66-77},
year = {2012},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1100104X},
author = {Ana Juan Ferrer and Francisco Hernández and Johan Tordsson and Erik Elmroth and Ahmed Ali-Eldin and Csilla Zsigri and Raül Sirvent and Jordi Guitart and Rosa M. Badia and Karim Djemame and Wolfgang Ziegler and Theo Dimitrakos and Srijith K. Nair and George Kousiouris and Kleopatra Konstanteli and Theodora Varvarigou and Benoit Hudzia and Alexander Kipp and Stefan Wesner and Marcelo Corrales and Nikolaus Forgó and Tabassum Sharif and Craig Sheridan},
abstract = {We present fundamental challenges for scalable and dependable service platforms and architectures that enable flexible and dynamic provisioning of cloud services. Our findings are incorporated in a toolkit targeting the cloud service and infrastructure providers. The innovations behind the toolkit are aimed at optimizing the whole service life cycle, including service construction, deployment, and operation, on a basis of aspects such as trust, risk, eco-efficiency and cost. Notably, adaptive self-preservation is crucial to meet predicted and unforeseen changes in resource requirements. By addressing the whole service life cycle, taking into account several cloud architectures, and by taking a holistic approach to sustainable service provisioning, the toolkit aims to provide a foundation for a reliable, sustainable, and trustful cloud computing industry.}
}
@article{LI2012379,
title = {CyberGuarder: A virtualization security assurance architecture for green cloud computing},
journal = {Future Generation Computer Systems},
volume = {28},
number = {2},
pages = {379-390},
year = {2012},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1100063X},
author = {Jianxin Li and Bo Li and Tianyu Wo and Chunming Hu and Jinpeng Huai and Lu Liu and K.P. Lam},
keywords = {Cloud computing, Green computing, Virtualization, Virtual security appliance, Security isolation},
abstract = {As the sizes of IT infrastructure continue to grow, cloud computing is a natural extension of virtualisation technologies that enable scalable management of virtual machines over a plethora of physically connected systems. The so-called virtualisation-based cloud computing paradigm offers a practical approach to green IT/clouds, which emphasise the construction and deployment of scalable, energy-efficient network software applications (NetApp) by virtue of improved utilisation of the underlying resources. The latter is typically achieved through increased sharing of hardware and data in a multi-tenant cloud architecture/environment and, as such, accentuates the critical requirement for enhanced security services as an integrated component of the virtual infrastructure management strategy. This paper analyses the key security challenges faced by contemporary green cloud computing environments, and proposes a virtualisation security assurance architecture, CyberGuarder, which is designed to address several key security problems within the ‘green’ cloud computing context. In particular, CyberGuarder provides three different kinds of services; namely, a virtual machine security service, a virtual network security service and a policy based trust management service. Specifically, the proposed virtual machine security service incorporates a number of new techniques which include (1) a VMM-based integrity measurement approach for NetApp trusted loading, (2) a multi-granularity NetApp isolation mechanism to enable OS user isolation, and (3) a dynamic approach to virtual machine and network isolation for multiple NetApp’s based on energy-efficiency and security requirements. Secondly, a virtual network security service has been developed successfully to provide an adaptive virtual security appliance deployment in a NetApp execution environment, whereby traditional security services such as IDS and firewalls can be encapsulated as VM images and deployed over a virtual security network in accordance with the practical configuration of the virtualised infrastructure. Thirdly, a security service providing policy based trust management is proposed to facilitate access control to the resources pool and a trust federation mechanism to support/optimise task privacy and cost requirements across multiple resource pools. Preliminary studies of these services have been carried out on our iVIC platform, with promising results. As part of our ongoing research in large-scale, energy-efficient/green cloud computing, we are currently developing a virtual laboratory for our campus courses using the virtualisation infrastructure of iVIC, which incorporates the important results and experience of CyberGuarder in a practical context.}
}
@incollection{BERNSTEIN2009349,
title = {Chapter 11 - Future Trends},
editor = {Philip A. Bernstein and Eric Newcomer},
booktitle = {Principles of Transaction Processing (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {San Francisco},
pages = {349-354},
year = {2009},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-1-55860-623-4},
doi = {https://doi.org/10.1016/B978-1-55860-623-4.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606234000111},
author = {Philip A. Bernstein and Eric Newcomer},
abstract = {Publisher Summary
Although transaction processing principles have remained fairly constant during the past 20 years or so, the technologies that implement the principles have been evolving. Recent changes starting to impact transactional middleware products include cloud computing, highly scalable computing designs, solid state memory, and streaming event processing. Cloud computing is a computing service offered over the Internet. A service provider may offer a complete application, such as e-mail, search, enterprise resource planning, customer relationship management, or social networking. The largest web sites are innovating in the use of custom designs for scalable computing. This customization is needed to cope with the unpredictability of workloads and the need to optimize fast response times for a good customer experience. Such customization is reminiscent of the early years of transaction processing technologies, where companies developed middleware for their internal applications. Among the techniques now being employed are additional caching at every level of the multitier TP architecture to improve performance and ensure good response time, increased use of queued transactions and business processes, and more dynamic application partitioning. The growing importance of processing data and event streams is another trend likely to affect transactional middleware products. Stream processing technology enables data to be processed in real time, while it is in flight—that is, before it is persisted. This is similar to message processing, but with higher performance and throughput.}
}
@article{HUANG2005108,
title = {Building a Portable File System for Heterogeneous Clusters},
journal = {Tsinghua Science & Technology},
volume = {10},
number = {1},
pages = {108-114},
year = {2005},
issn = {1007-0214},
doi = {https://doi.org/10.1016/S1007-0214(05)70016-X},
url = {https://www.sciencedirect.com/science/article/pii/S100702140570016X},
author = {Qifeng Huang and Guangwen Yang and Weimin Zheng and Meiming Shen and Yiyan Deng},
keywords = {cluster file system, portability, heterogeneity, file allocation, single system image, global cache},
abstract = {Existing in-kernel distributed file systems cannot cope with the higher requirements in well-equipped cluster environments, especially when the system becomes larger and inevitably heterogeneous. TH-CluFS is a cluster file system designed for large heterogeneous systems. TH-CluFS is implemented completely in the user space by emulating the network file system (NFS) V2 server, and is easily portable to other portable operating system interface (POSIX)-compliant platforms with application programming/binary interface API/ABI compliance. In addition, TH-CluFS uses a serverless architecture which flexibly distributes data at file granularity and achieves a consistent file system view from distributed metadata. The global cache makes full use of the aggregated memories and disks in the cluster to optimize system performance. Experimental results suggest that although TH-CluFS is implemented as user-level components, it functions as a portable, single system image, and scalable cluster file system with acceptable performance sacrifices.}
}
@article{KOUSIOURIS20111270,
title = {The effects of scheduling, workload type and consolidation scenarios on virtual machine performance and their prediction through optimized artificial neural networks},
journal = {Journal of Systems and Software},
volume = {84},
number = {8},
pages = {1270-1291},
year = {2011},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2011.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0164121211000951},
author = {George Kousiouris and Tommaso Cucinotta and Theodora Varvarigou},
keywords = {Virtualization, Real-time scheduling, Cloud computing, Artificial neural networks, Genetic algorithms, Performance prediction},
abstract = {The aim of this paper is to study and predict the effect of a number of critical parameters on the performance of virtual machines (VMs). These parameters include allocation percentages, real-time scheduling decisions and co-placement of VMs when these are deployed concurrently on the same physical node, as dictated by the server consolidation trend and the recent advances in the Cloud computing systems. Different combinations of VM workload types are investigated in relation to the aforementioned factors in order to find the optimal allocation strategies. What is more, different levels of memory sharing are applied, based on the coupling of VMs to cores on a multi-core architecture. For all the aforementioned cases, the effect on the score of specific benchmarks running inside the VMs is measured. Finally, a black box method based on genetically optimized artificial neural networks is inserted in order to investigate the degradation prediction ability a priori of the execution and is compared to the linear regression method.}
}
@article{VINCENTWANG2013232,
title = {An interoperable solution for Cloud manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {29},
number = {4},
pages = {232-247},
year = {2013},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2013.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0736584513000069},
author = {Xi {Vincent Wang} and Xun W. Xu},
keywords = {Cloud, Cloud computing, Cloud manufacturing, Service-oriented architecture, STEP},
abstract = {Cloud manufacturing is a new concept extending and adopting the concept of Cloud computing for manufacturing. The aim is to transform manufacturing businesses to a new paradigm in that manufacturing capabilities and resources are componentized, integrated and optimized globally. This study presents an interoperable manufacturing perspective based on Cloud manufacturing. A literature search has been undertaken regarding Cloud architecture and technologies that can assist Cloud manufacturing. Manufacturing resources and capabilities are discussed in terms of Cloud service. A service-oriented, interoperable Cloud manufacturing system is proposed. Service methodologies are developed to support two types of Cloud users, i.e., customer user and enterprise user, along with standardized data models describing Cloud service and relevant features. Two case studies are undertaken to evaluate the proposed system. Cloud technology brings into manufacturing industry with a number of benefits such as openness, cost-efficiency, resource sharing and production scalability.}
}
@article{MARCH2011618,
title = {μCloud: Towards a New Paradigm of Rich Mobile Applications},
journal = {Procedia Computer Science},
volume = {5},
pages = {618-624},
year = {2011},
note = {The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.07.080},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911004054},
author = {Verdi March and Yan Gu and Erwin Leonardi and George Goh and Markus Kirchberg and Bu Sung Lee},
keywords = {mobile cloud, component, composition},
abstract = {Rich mobile applications are characterized by rich functionality, offline usability and portability. However, it isnot trivial to simultaneously satisfy all the three criteria. Existing approaches such stand-alone applications and the thin-client architecture satisfy only a subset of these criteria. In this paper, we show that rich mobile applications can be achieved through the convergence of mobile and cloud computing. We address two main issues in cloud-enabled mobile applications, namely complexity of application development and offline usability. We then propose μCloud framework which models a rich mobile application as a graph of components distributed onto mobile devices and the cloud. Lastly, we discuss μCloud's major research issues, i.e., workflow language for interactive applications, offline usability, secure and scalable multi-tenancy, portability and energy optimization.}
}
@article{KOURTESIS2014307,
title = {Semantic-based QoS management in cloud systems: Current status and future challenges},
journal = {Future Generation Computer Systems},
volume = {32},
pages = {307-323},
year = {2014},
note = {Special Section: The Management of Cloud Systems, Special Section: Cyber-Physical Society and Special Section: Special Issue on Exploiting Semantic Technologies with Particularization on Linked Data over Grid and Cloud Architectures},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1300232X},
author = {Dimitrios Kourtesis and Jose María Alvarez-Rodríguez and Iraklis Paraskakis},
keywords = {Cloud systems, Quality of service, Service oriented architectures, Semantics, Ontologies, Linked data, Sensor data, Big data},
abstract = {Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.}
}
@article{JAIGANESH20123005,
title = {ACDP: Prediction of Application Cloud Data center Proficiency Using Fuzzy Modeling},
journal = {Procedia Engineering},
volume = {38},
pages = {3005-3018},
year = {2012},
note = {INTERNATIONAL CONFERENCE ON MODELLING OPTIMIZATION AND COMPUTING},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2012.06.351},
url = {https://www.sciencedirect.com/science/article/pii/S1877705812022643},
author = {M. Jaiganesh and A. Vincent Antony kumar and R. Sivasankari},
keywords = {Data cente, Manageme, Fuzzification, Defuzzification, Cloud Computing, Optimization},
abstract = {Cloud computing, the notion of outsourcing hardware and software to Internet service providers is showing the classic signs of constructive technology -it's good enough for the masses, and it has clear potential to shake things up. The Cloud computing enables the clients to utilize services by on demand techniques. Data centre is a sophisticated high definition server which runs applications virtually. We propose a novel approach to find proficiency of the data center in cloud computing. The goal is to optimize data center utilization with big three factors like Bandwidth, Memory and Virtual Machine (VM). We construct a fuzzy model for some of factors and obtain Application Cloud Datacenter Proficiency (ACDP) in cloud computing environments. The benefit of ACDP is providing estimation of application cloud architecture considerations. In this, fuzzy modeling optimization proceeds maximum gain in the Application cloud controlled by data centre proficiency diagonally a huge diversity of workloads using a fuzzy tool box kit.}
}
@incollection{SMOOT201299,
title = {Chapter 4 - Branch Consolidation and WAN Optimization},
editor = {Stephen R. Smoot and Nam K. Tan},
booktitle = {Private Cloud Computing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {99-125},
year = {2012},
isbn = {978-0-12-384919-9},
doi = {https://doi.org/10.1016/B978-0-12-384919-9.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123849199000040},
author = {Stephen R. Smoot and Nam K. Tan},
keywords = {WAN optimization, TCP acceleration, caching, Steelhead Appliance, compression, deduplication, scalable data referencing, application acceleration, WAN optimizers, QoS, TCP proxying, VDI, VSI},
abstract = {Publisher Summary
This chapter begins by examining the WAN performance problem from a theoretical viewpoint to derive the required elements of a solution. WAN optimization, the broad area for these solutions, helps to resolve WAN performance problems. Implementing WAN optimization devices into the network imposes engineering constraints. These constraints are discussed to examine the implications on other systems in the network. WAN optimization uses deduplication, compression, transmission control protocol (TCP) acceleration, and application acceleration to break through these network bottlenecks. Implementing WAN optimization results in several integration challenges stemming from changes in payload, packet headers, and integration with other network services. However, WAN optimization enables cloud services to be designed and implemented with a choice of different designs to suit enterprise business needs. These designs not only impact the branch office network design, but also the data center (DC). The chapter explores how an optimized WAN enables remote office server consolidation, virtual desktops, and other cost-saving architectures. The private cloud is challenged by both bandwidth and latency constraints. Various WAN optimization techniques and how they unleash high performing cloud applications are discussed. Saving bandwidth and removing latency at the application layer, which usually provides the greatest gains in performance, are discussed. Additionally, there is a combination effect—the techniques for latency reduction can also remove bytes from the network because they prevent redundant data from being sent. The last section examines techniques that can be applied to the TCP layer in isolation.}
}
@article{AO20139,
title = {Cost-effective deployment of distributed cloud based on generic expense model},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {20},
number = {5},
pages = {9-29},
year = {2013},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(13)60083-7},
url = {https://www.sciencedirect.com/science/article/pii/S1005888513600837},
author = {Nai-xiang AO and Ying-ying XU and Dan HUANG and Yong-xiang ZHAO and Chang-jia CHEN},
keywords = {distributed network, cost-effective, optimization, numerical simulation},
abstract = {Distributed cloud architecture which consists of many cloud computing-storage resources (CCSRs) distributed across a geographic large-area has been widely implemented. It has received significant attention from academia. However, little effort has been taken to examine changes in operating cost-structure brought by distributed cloud scheme, or explore how to reap economic benefits from its geo-diversity. To tackle such issue, this paper formulated cost optimizations for cloud platforms based on a generic expense model of distributed cloud, taking into account major components of operating cost. The best deployment schemes were obtained through numerical simulation. The optimal amount of edge CCSRs and their corresponding placements were found to be determined by the ratio among various overhead components. Both model study and numerical simulation shed light on practical deployment of distributed cloud with high cost-effectiveness.}
}
@article{CHEN20131881,
title = {Dynamic QoS Optimization Architecture for Cloud-based DDDAS},
journal = {Procedia Computer Science},
volume = {18},
pages = {1881-1890},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.357},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913005000},
author = {Tao Chen and Rami Bahsoon and Georgios Theodoropoulos},
keywords = {Cloud Computing, QoS Optimization, QoS Sensitivity, Multi-objective Optimization, Distributed Simulation},
abstract = {An emerging class of Dynamic Data Driven application systems heavily depends on cloud and Big Data. We refer to this class of DDDAS as cloud-based DDDAS. Despite the growing interest in marrying DDDAS with the cloud, there is a general lack for architectural frameworks explicating the cloud requirements, which can support cloud-based DDDAS. Given the unpredictable, dynamic and on-demand nature of the cloud, cloud-based DDDAS requires novel approaches for dynamic Quality of Service (QoS) optimization. This is important for providing timely and reliable predictions and for ensuring higher dependability in the solution, as it would be unrealistic to assume that optimal QoS can be achieved at design time. We propose a decentralized architectural style for cloud-based DDDAS, where dynamic QoS optimization is in the heart of the symbiotic adaptation. The architecture leverages on the classical DDDAS primitives to reach a refined decentralized style suited for the dynamic requirements of the cloud. We formulate the QoS optimization problem as a dynamic multi-objective optimization problem. We use a scenario to exemplify and evaluate the effectiveness of the style.}
}
@incollection{SMOOT2012299,
title = {Chapter 9 - Case Studies},
editor = {Stephen R. Smoot and Nam K. Tan},
booktitle = {Private Cloud Computing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {299-369},
year = {2012},
isbn = {978-0-12-384919-9},
doi = {https://doi.org/10.1016/B978-0-12-384919-9.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012384919900009X},
author = {Stephen R. Smoot and Nam K. Tan},
keywords = {virtual access layer, IaaS, ERSPAN, Nexus 1000V switch, unified fabric, FCoE, Nexus 5000 switch, NPV, NPIV, FCF, VFC, ToR, Nexus 2000 FEX, EoR, vPC, Nexus 7000 switch, FCIP, MDS switch, IVR, SOI, UCS, ACESM, WCCPv2, Steelhead Appliance, FWSM, WAN optimizer, DMVPN, VRF, MPLS VPN},
abstract = {Publisher Summary
This chapter is the concluding chapter on the design and deployment of private cloud computing based on the consolidation and virtualization processes as well as the provisioning of cloud infrastructure-as-a-service (IaaS) offerings over a service-oriented infrastructure (SOI). The topics are presented in the form of case studies. These case studies include design as well as deployment topics, basic configurations, and tutorials on some of the more complex concepts covered in the previous chapters. These case studies ensure that readers have exposure to the consolidation techniques, the virtualization strategies, and the construction of the SOI, leading to the deployment of private cloud computing and the provisioning of cloud IaaS offerings. The first design study involves the virtual access layer as it happens to be the front-end of server virtualization. Next, ERSPAN is briefly covered to gain visibility into network flows that are obscured by the virtual access layer. A simple WAN optimization study is covered next followed by a unified fabric design study that illustrates the unification of the data center (DC) LAN and DC SAN with FCoE at the physical access layer. The chapter then examines the top-of-rack (ToR) architecture as an enabler toward a “rack- and- roll” server deployment model, virtual PortChannel (vPC) as an enhancement to spanning tree protocol (STP), and Fibre Channel over IP (FCIP) as a robust SAN extension technique over the WAN. In the final design study all the topics covered are assembled together for the construction of the SOI, so that cloud IaaS can be provisioned over it.}
}
@article{LIN201377,
title = {MapReduce optimization algorithm based on machine learning in heterogeneous cloud environment},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {20},
number = {6},
pages = {77-121},
year = {2013},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(13)60112-0},
url = {https://www.sciencedirect.com/science/article/pii/S1005888513601120},
author = {Wen-hui LIN and Zhen-ming LEI and Jun LIU and Jie YANG and Fang LIU and Gang HE and Qin WANG},
keywords = {cloud computing, MapReduce, machine learning, heterogeneity},
abstract = {We present an approach to optimize the MapReduce architecture, which could make heterogeneous cloud environment more stable and efficient. Fundamentally different from previous methods, our approach introduces the machine learning technique into MapReduce framework, and dynamically improve MapReduce algorithm according to the statistics result of machine learning. There are three main aspects: learning machine performance, reduce task assignment algorithm based on learning result, and speculative execution optimization mechanism. Furthermore, there are two important features in our approach. First, the MapReduce framework can obtain nodes' performance values in the cluster through machine learning module. And machine learning module will daily calibrate nodes' performance values to make an accurate assessment of cluster performance. Second, with the optimization of tasks assignment algorithm, we can maximize the performance of heterogeneous clusters. According to our evaluation result, the cluster performance could have 19% improvement in current heterogeneous cloud environment, and the stability of cluster has greatly enhanced.}
}
@article{KOLLI20111447,
title = {A dynamic programming approach: Improving the performance of wireless networks},
journal = {Journal of Parallel and Distributed Computing},
volume = {71},
number = {11},
pages = {1447-1459},
year = {2011},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2011.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731511001419},
author = {Sandeep Kolli and Maciej Zawodniok},
keywords = {Wireless sensor networks (WSN), Dynamic programming (DP), Load balancing, Cloud computing},
abstract = {Traditional wireless networks focus on transparent data transmission where the data are processed at either the source or destination nodes. In contrast, the proposed approach aims at distributing data processing among the nodes in the network thus providing a higher processing capability than a single device. Moreover, energy consumption is balanced in the proposed scheme since the energy intensive processing will be distributed among the nodes. The performance of a wireless network is dependent on a number of factors including the available energy, energy–efficiency, data processing delay, transmission delay, routing decisions, security architecture etc. Typical existing distributed processing schemes have a fixed node or node type assigned to the processing at the design phase, for example a cluster head in wireless sensor networks aggregating the data. In contrast, the proposed approach aims to virtualize the processing, energy, and communication resources of the entire heterogeneous network and dynamically distribute processing steps along the communication path while optimizing performance. Moreover, the security of the communication is considered an important factor in the decision to either process or forward the data. Overall, the proposed scheme creates a wireless “computing cloud” where the processing tasks are dynamically assigned to the nodes using the Dynamic Programming (DP) methodology. The processing and transmission decisions are analytically derived from network models in order to optimize the utilization of the network resources including: available energy, processing capacity, security overhead, bandwidth etc. The proposed DP-based scheme is mathematically derived thus guaranteeing performance. Moreover, the scheme is verified through network simulations.}
}
@article{GUNARATHNE20131035,
title = {Scalable parallel computing on clouds using Twister4Azure iterative MapReduce},
journal = {Future Generation Computer Systems},
volume = {29},
number = {4},
pages = {1035-1048},
year = {2013},
note = {Special Section: Utility and Cloud Computing},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2012.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X12001379},
author = {Thilina Gunarathne and Bingjing Zhang and Tak-Lon Wu and Judy Qiu},
keywords = {Iterative MapReduce, Cloud computing, HPC, Scientific applications, Azure},
abstract = {Recent advances in data-intensive computing for science discovery are fueling a dramatic growth in the use of data-intensive iterative computations. The utility computing model introduced by cloud computing, combined with the rich set of cloud infrastructure and storage services, offers a very attractive environment in which scientists can perform data analytics. The challenges to large-scale distributed computations on cloud environments demand innovative computational frameworks that are specifically tailored for cloud characteristics to easily and effectively harness the power of clouds. Twister4Azure is a distributed decentralized iterative MapReduce runtime for Windows Azure Cloud. Twister4Azure extends the familiar, easy-to-use MapReduce programming model with iterative extensions, enabling a fault-tolerance execution of a wide array of data mining and data analysis applications on the Azure cloud. Twister4Azure utilizes the scalable, distributed and highly available Azure cloud services as the underlying building blocks, and employs a decentralized control architecture that avoids single point failures. Twister4Azure optimizes the iterative computations using a multi-level caching of data, a cache-aware decentralized task scheduling, hybrid tree-based data broadcasting and hybrid intermediate data communication. This paper presents the Twister4Azure iterative MapReduce runtime and a study of four real world data-intensive scientific applications implemented using Twister4Azure–two iterative applications, Multi-Dimensional Scaling and KMeans Clustering; and two pleasingly parallel applications, BLAST+ sequence searching and SmithWaterman sequence alignment. Performance measurements show comparable or a factor of 2 to 4 better results than the traditional MapReduce runtimes deployed on up to 256 instances and for jobs with tens of thousands of tasks. We also study and present solutions to several factors that affect the performance of iterative MapReduce applications on Windows Azure Cloud.}
}