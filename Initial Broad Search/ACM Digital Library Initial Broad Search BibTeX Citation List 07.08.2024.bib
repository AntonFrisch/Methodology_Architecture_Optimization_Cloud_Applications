@inproceedings{10.1145/3090354.3090427,
author = {Ramchoun, H. and Idrissi, M. A. Janati and Ghanou, Y. and Ettaouil, M.},
title = {Multilayer Perceptron: Architecture Optimization and training with mixed activation functions},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090427},
doi = {10.1145/3090354.3090427},
abstract = {Multilayer perceptron has a large wide of classification and regression applications in many fields: pattern recognition, voice and classification problems. But the architecture choice in particular the activation function type used for each neuron has a great impact on the convergence of these networks. In the present paper we introduce a new approach to optimize the network architecture and weights, for solving the obtained model we use the meta-heuristics and we train the network with a back-propagation algorithm.The numerical results assess the effectiveness of the results shown in this paper, and the advantages of the new modeling compared to the previous model in the literature.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {71},
numpages = {6},
keywords = {Non-linear optimization, Multilayer perceptron (MLP), Meta-heuristics, Architecture optimization, Activation function},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/2598394.2605686,
author = {Etemaadi, Ramin and Chaudron, Michel R.V.},
title = {Distributed optimization on super computers: case study on software architecture optimization framework},
year = {2014},
isbn = {9781450328814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598394.2605686},
doi = {10.1145/2598394.2605686},
abstract = {Nowadays advanced software systems need to satisfy large number of quality attributes at the same time. It is a very complex optimization problem which software architects must address. Evolutionary algorithms can help architects to find optimal solutions which meet these conflicting quality attributes. However, these metaheuristic approaches in multiobjective problems especially for high dimensions mostly take so long time to be executed. One of the best solutions to speed up this process is distributing execution of evolutionary algorithm on multiple nodes of a super computer or on the cloud.This paper presents the results of distributed execution of evolutionary algorithm for multiobjective optimization of software architecture. We report implementation of two different ways for distributed execution of evolutionary algorithm: (1) Actor-based approach, (2) MapReduce approach. The case study in this experiment is an industrial software system which is derived from a real world automotive embedded system.The results of this computationally-intensive experiment on a super computer give us 81.27% parallelization efficiency for distribution among 5 nodes.},
booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1125–1132},
numpages = {8},
keywords = {software architecture design optimization, metaheuristic optimization, mapreduce paradigm, evolutionary multiobjective optimization algorithms (emoa), distributed execution, component-based software engineering (cbse), actor-based distribution},
location = {Vancouver, BC, Canada},
series = {GECCO Comp '14}
}

@article{10.1145/3474058,
author = {Mbongue, Joel Mandebi and Kwadjo, Danielle Tchuinkou and Shuping, Alex and Bobda, Christophe},
title = {Deploying Multi-tenant FPGAs within Linux-based Cloud Infrastructure},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3474058},
doi = {10.1145/3474058},
abstract = {Cloud deployments now increasingly exploit Field-Programmable Gate Array (FPGA) accelerators as part of virtual instances. While cloud FPGAs are still essentially single-tenant, the growing demand for efficient hardware acceleration paves the way to FPGA multi-tenancy. It then becomes necessary to explore architectures, design flows, and resource management features that aim at exposing multi-tenant FPGAs to the cloud users. In this article, we discuss a hardware/software architecture that supports provisioning space-shared FPGAs in Kernel-based Virtual Machine (KVM) clouds. The proposed hardware/software architecture introduces an FPGA organization that improves hardware consolidation and support hardware elasticity with minimal data movement overhead. It also relies on VirtIO to decrease communication latency between hardware and software domains. Prototyping the proposed architecture with a Virtex UltraScale+ FPGA demonstrated near specification maximum frequency for on-chip data movement and high throughput in virtual instance access to hardware accelerators. We demonstrate similar performance compared to single-tenant deployment while increasing FPGA utilization, which is one of the goals of virtualization. Overall, our FPGA design achieved about 2\texttimes{} higher maximum frequency than the state of the art and a bandwidth reaching up to 28 Gbps on 32-bit data width.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {19},
numpages = {31},
keywords = {KVM, virtualization, network-on-chip, multi-tenancy, FPGA, Cloud}
}

@inproceedings{10.1145/3457784.3457817,
author = {Guo, FangMing and Song, Hua},
title = {The Research on Multi-level Performance Optimization of Course Selection System Based on Full Credit System},
year = {2021},
isbn = {9781450388825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457784.3457817},
doi = {10.1145/3457784.3457817},
abstract = {The full credit system is a trend in the teaching reform of Chinese universities in recent years. The comprehensive elective system is the core of the full credit system. For comprehensive universities, with the continuous expansion of enrollment scale, the number of majors, classes, and students has increased dramatically, and the amount of information about students' course selection has also doubled, making it very difficult to implement a comprehensive course selection system. In view of the characteristics of the comprehensive course selection system, this article starts with the basic needs of the course selection business, deeply analyzes the complexity of the course selection system, and adjusts the course selection strategy. At the same time, in view of the performance problems of the course selection system, a multi-level architecture optimization scheme was proposed, using data caching, data level segmentation, cloud platform and other technologies, which greatly reduced the concentration of data processing during the course selection. Practice shows that the performance of the optimized course selection system has been greatly improved, which can better meet the course selection requirements of comprehensive universities.},
booktitle = {Proceedings of the 2021 10th International Conference on Software and Computer Applications},
pages = {208–212},
numpages = {5},
keywords = {structure optimization, credit system, course selection system, cloud platform},
location = {Kuala Lumpur, Malaysia},
series = {ICSCA '21}
}

@article{10.1145/3603704,
author = {Li, Nan and Ma, Lianbo and Yu, Guo and Xue, Bing and Zhang, Mengjie and Jin, Yaochu},
title = {Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications, and Open Issues},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3603704},
doi = {10.1145/3603704},
abstract = {Over recent years, there has been a rapid development of deep learning (DL) in both industry and academia fields. However, finding the optimal hyperparameters of a DL model often needs high computational cost and human expertise. To mitigate the above issue, evolutionary computation (EC) as a powerful heuristic search approach has shown significant merits in the automated design of DL models, so-called evolutionary deep learning (EDL). This article aims to analyze EDL from the perspective of automated machine learning (AutoML). Specifically, we first illuminate EDL from DL and EC and regard EDL as an optimization problem. According to the DL pipeline, we systematically introduce EDL methods ranging from data preparation, model generation, to model deployment with a new taxonomy (i.e., what and how to evolve/optimize), and focus on the discussions of solution representation and search paradigm in handling the optimization problem by EC. Finally, key applications, open issues, and potentially promising lines of future research are suggested. This survey has reviewed recent developments of EDL and offers insightful guidelines for the development of EDL.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {41},
numpages = {34},
keywords = {model deployment, model generation, data preparation, evolutionary computation, Deep learning}
}

@inproceedings{10.1145/3171592.3171613,
author = {Bin, Wang and Shulin, Yang and Xuelei, Ren and Guyang, Wang},
title = {Research on Digital Publishing Application System Based on Micro-Service Architecture},
year = {2017},
isbn = {9781450353663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3171592.3171613},
doi = {10.1145/3171592.3171613},
abstract = {Micro-service architecture technology is an epoch-making technology in the Internet field, providing a good solution for the lack of traditional back-office architecture. This paper analyzes the key technologies of micro service architecture, especially the use of Docker container technology, and introduces micro service architecture technology into digital publishing industry, analyzes some existing problems in traditional digital publishing industry, and then analyzes the micro service architecture Technology in the digital publishing application of the two main aspects, one is the digital publishing system architecture optimization, and second, digital publishing production process improvement. Which provides some reference for the traditional digital publishing field transformation.},
booktitle = {Proceedings of the 2017 VI International Conference on Network, Communication and Computing},
pages = {140–144},
numpages = {5},
keywords = {docker, digital publishing, architecture, Micro-service},
location = {Kunming, China},
series = {ICNCC '17}
}

@inproceedings{10.1145/3351917.3351979,
author = {Zhao, Gang and Cao, Xian and Xiao, Wenlei and Zhu, Yakui and Cheng, Kang},
title = {Digital Twin for NC Machining Using Complete Process Information Expressed by STEP-NC Standard},
year = {2019},
isbn = {9781450371865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351917.3351979},
doi = {10.1145/3351917.3351979},
abstract = {The concept of Digital Twin has been accepted by more and more researchers and enterprises. Many applications of Digital Twin systems are reported such as airplane state monitor and smart factory. In addition, the digital twin is an important measure to achieve intelligent manufacturing and industry 4.0. The advancement of DT system in manufacturing environment is evident. However, current researches in manufacturing domain concentrated on the discussion of architecture optimization of digital twin system combining with existing cyber-physics researches. There are little research works reported practical DT system and discussed the technologies in implementation. Different from current works, this paper proposed an advanced Digital Twin system for CNC machine tool and discussed the critical enablers to make the DT system more practicality.First, data stream from CAD/CAM to CNC is improved with the STEP-NC standard which containing more structured process data to enhance the information in virtual environment to reflect the real machining process. Second, hardware connection method with CNC system is summarized and a uniform interface for data accessing is proposed to simplify the development of DT system. At last, a virtual machining environment (software named "GrapeSim") is presented which contains three functions: estimating the machining state according to the accessed data from CNC; material removing simulation that synchronous with the real machining process to illustrate the geometry shape of machined workpiece; a web server that allowing other devices to obtain the data in virtual environment as a data source of cloud manufacturing.The whole Digital Twin system is tested in COMAC, the manufacturer of ARJ21 and C919 jetliner, and all functions are proved stable and useful.},
booktitle = {Proceedings of the 2019 4th International Conference on Automation, Control and Robotics Engineering},
articleno = {45},
numpages = {6},
keywords = {STEP-NC, On-line machine tool monitor, Machining Simulation, Digital Twin},
location = {Shenzhen, China},
series = {CACRE2019}
}

@inproceedings{10.1145/3321707.3321721,
author = {Liang, Jason and Meyerson, Elliot and Hodjat, Babak and Fink, Dan and Mutch, Karl and Miikkulainen, Risto},
title = {Evolutionary neural AutoML for deep learning},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321721},
doi = {10.1145/3321707.3321721},
abstract = {Deep neural networks (DNNs) have produced state-of-the-art results in many benchmarks and problem domains. However, the success of DNNs depends on the proper configuration of its architecture and hyperparameters. Such a configuration is difficult and as a result, DNNs are often not used to their full potential. In addition, DNNs in commercial applications often need to satisfy real-world design constraints such as size or number of parameters. To make configuration easier, automatic machine learning (AutoML) systems for deep learning have been developed, focusing mostly on optimization of hyperparameters.This paper takes AutoML a step further. It introduces an evolutionary AutoML framework called LEAF that not only optimizes hyperparameters but also network architectures and the size of the network. LEAF makes use of both state-of-the-art evolutionary algorithms (EAs) and distributed computing frameworks. Experimental results on medical image classification and natural language analysis show that the framework can be used to achieve state-of-the-art performance. In particular, LEAF demonstrates that architecture optimization provides a significant boost over hyperparameter optimization, and that networks can be minimized at the same time with little drop in performance. LEAF therefore forms a foundation for democratizing and improving AI, as well as making AI practical in future applications.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {401–409},
numpages = {9},
keywords = {neural networks/deep learning, artificial intelligence, AutoML},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3544109.3544134,
author = {Zhang, Shaochen and Qu, Youyang and Wang, Peng},
title = {Design of cloud computing data center security system based on Virtualization environment},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544134},
doi = {10.1145/3544109.3544134},
abstract = {In order to improve the security of cloud computing data center in the virtualized environment, a security system design of cloud computing data center is proposed based on the virtualization environment. Firstly, the security architecture of cloud computing data center is constructed, and the security of data center is evaluated. By optimizing the system equipment structure and operation steps, the security performance of cloud computing data center can be improved. The experimental results show that the design method of cloud computing data center security architecture based on Virtualization environment has high precision, good practical effect and fully meets the research requirements.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {137–145},
numpages = {9},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/3638584.3638678,
author = {Zhong, Yuyanzhen and Yang, Chuan and Su, Xinyi and Li, Biao and Huang, Xiaoping and Shuai, Yong},
title = {Review on Research of Automated Machine Learning},
year = {2024},
isbn = {9798400708688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638584.3638678},
doi = {10.1145/3638584.3638678},
abstract = {Automated Machine Learning (AutoML) can automatically discover high-performance models to build deep learning systems without human assistance, with the ultimate goal of reducing the complexity and entry barriers of building deep learning systems. Although AutoML has achieved a certain degree of automation through four important steps: data preparation, feature engineering, model generation, and model evaluation, there is still a significant gap compared to the ultimate ideal of achieving truly intelligent lifelong learning. Therefore, a deep understanding of AutoML can help drive the development of artificial intelligence. Firstly, we comprehensively reviewed the latest technologies and achievements involved in these four steps, then we introduced their shortcomings and challenges. Secnodly, a detailed introduction was given to the existing AutoML libraries and the theoretical and practical applications of AutoML. Finally, we summarized AutoML models and Proposed an outlook.},
booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence},
pages = {526–532},
numpages = {7},
keywords = {Automated Machine Learning, Automatic Processing, Lifelong Learning, Neural Architecture Search},
location = {Beijing, China},
series = {CSAI '23}
}

@inproceedings{10.1145/3663529.3663860,
author = {\"{O}qvist, Karl and Messinger, Jacob and Wohlrab, Rebekka},
title = {Supporting Early Architectural Decision-Making through Tradeoff Analysis: A Study with Volvo Cars},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663860},
doi = {10.1145/3663529.3663860},
abstract = {As automotive companies increasingly move operations to the cloud, they need to carefully make architectural decisions. Currently, architectural decisions are made ad-hoc and depend on the experience of the involved architects. Recent research has proposed the use of data-driven techniques that help humans to understand complex design spaces and make thought-through decisions. This paper presents a design science study in which we explored the use of such techniques in collaboration with architects at Volvo Cars. We show how a software architecture can be simulated to make more principled design decisions and allow for architectural tradeoff analysis. Concretely, we apply machine learning-based techniques such as Principal Component Analysis, Decision Tree Learning, and clustering. Our findings show that the tradeoff analysis performed on the data from simulated architectures gave important insights into what the key tradeoffs are and what design decisions shall be taken early on to arrive at a high-quality architecture.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {411–416},
numpages = {6},
keywords = {architectural analysis, cloud systems, design decisions, principal component analysis, software architecture, tradeoff analysis},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.5555/3233397.3233452,
author = {Ashraf, Adnan and Byholm, Benjamin and Porres, Ivan},
title = {A multi-objective ACS algorithm to optimize cost, performance, and reliability in the cloud},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {In this paper, we present a novel Multi-Objective Ant Colony System algorithm to optimize Cost, Performance, and Reliability (MOACS-CoPeR) in the cloud. The proposed algorithm provides a metaheuristic-based approach for the multi-objective cloud-based software component deployment problem. MOACS-CoPeR explores the search-space of architecture design alternatives with respect to several architectural degrees of freedom and produces a set of Pareto-optimal deployment configurations. We also present a Java-based implementation of our proposed algorithm and compare its results with the Non-dominated Sorting Genetic Algorithm II (NSGA-II). We evaluate the two algorithms against a cloud-based storage service, which is loosely based on a real system.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {341–347},
numpages = {7},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/3320269.3384767,
author = {Berlato, Stefano and Carbone, Roberto and Lee, Adam J. and Ranise, Silvio},
title = {Exploring Architectures for Cryptographic Access Control Enforcement in the Cloud for Fun and Optimization},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384767},
doi = {10.1145/3320269.3384767},
abstract = {To facilitate the adoption of cloud by organizations, Cryptographic Access Control (CAC) is the obvious solution to control data sharing among users while preventing partially trusted Cloud Service Providers (CSP) from accessing sensitive data. Indeed, several CAC schemes have been proposed in the literature. Despite their differences, available solutions are based on a common set of entities---e.g., a data storage service or a proxy mediating the access of users to encrypted data---that operate in different (security) domains---e.g., on-premise or the CSP. However, the majority of the CAC schemes assume a fixed assignment of entities to domains; this has security and usability implications that are not made explicit and can make inappropriate the use of a CAC scheme in certain scenarios with specific requirements. For instance, assuming that the proxy runs at the premises of the organization avoids the vendor lock-in effect but may substantially undermine scalability.To the best of our knowledge, no previous work considers how to select the best possible architecture (i.e., the assignment of entities to domains) to deploy a CAC scheme for the requirements of a given scenario. In this paper, we propose a methodology to assist administrators in exploring different architectures of CAC schemes for a given scenario. We do this by identifying the possible architectures underlying the CAC schemes available in the literature and formalizing them in simple set theory. This allows us to reduce the problem of selecting the most suitable architecture satisfying a heterogeneous set of requirements arising from the considered scenario to a Multi-Objective Optimization Problem (MOOP) for which state-of-the-art solvers can be invoked. Finally, we show how the capability of solving the MOOP can be used to build a prototype tool assisting administrators to preliminary perform a "What-if'' analysis to explore the trade-offs among the various architectures and then use available standards and tools (such as TOSCA and Cloudify) for automated deployment in multiple CSPs.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {208–221},
numpages = {14},
keywords = {optimization, cryptographic access control, architecture},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@article{10.1145/3672359.3672365,
author = {Sisinni, E. and Flammini, A. and Gaffurini, M. and Ferrari, P.},
title = {Exploiting Container-Based Microservices forReliable Smart Mobility Applications},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1094-3641},
url = {https://doi.org/10.1145/3672359.3672365},
doi = {10.1145/3672359.3672365},
abstract = {Smart mobility is emerging, addressing heterogeneous scenarios with high impact on technology infrastructures, solutions, and people. Safety and availability are mandatory, forcing the design of new reliable services for localization, health monitoring of the user, maintenance of vehicle, and protection of the environment. This paper proposes a container-based microservice approach to the edge computing in IoT smart mobility scenarios. Since smart mobility backends must manage a large heterogeneity of applications, the proposed approach is promising with respect to the classical solutions (based on "monolithic hardware+software" devices), from the point of view of flexibility, upgradability, security, scalability, and reliability. A demo use case, based on industry-grade hardware and Docker, has been realized and multiple implementations of the same services have been executed in parallel, showing strong independence between them. Moreover, average delays of less than 10 ms are obtained, confirming the usability in several smart mobility (and smart city) applications.},
journal = {Ada Lett.},
month = jun,
pages = {52–56},
numpages = {5},
keywords = {cloud computing, docker, edge computing, light mobility, performance evaluation, smart mobility}
}

@article{10.1145/3544836,
author = {Goudarzi, Mohammad and Palaniswami, Marimuthu and Buyya, Rajkumar},
title = {Scheduling IoT Applications in Edge and Fog Computing Environments: A Taxonomy and Future Directions},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3544836},
doi = {10.1145/3544836},
abstract = {Fog computing, as a distributed paradigm, offers cloud-like services at the edge of the network with low latency and high-access bandwidth to support a diverse range of IoT application scenarios. To fully utilize the potential of this computing paradigm, scalable, adaptive, and accurate scheduling mechanisms and algorithms are required to efficiently capture the dynamics and requirements of users, IoT applications, environmental properties, and optimization targets. This article presents a taxonomy of recent literature on scheduling IoT applications in Fog computing. Based on our new classification schemes, current works in the literature are analyzed, research gaps of each category are identified, and respective future directions are described.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {152},
numpages = {41},
keywords = {performance evaluation, optimization characteristics, environmental architecture, application structure, scheduling taxonomy, Internet of Things, Fog computing}
}

@inproceedings{10.1145/3673277.3673351,
author = {Xie, Wenke},
title = {Optimization Design and Application of Enterprise Network Architecture Based on Multi-Factor Model Network Partition},
year = {2024},
isbn = {9798400716959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673277.3673351},
doi = {10.1145/3673277.3673351},
abstract = {This paper focuses on the optimization design and application of enterprise network architecture through a multi-factor model network partition approach. The study addresses challenges prevalent in traditional manufacturing enterprise networks, such as issues related to coarse management granularity, uneven business distribution, and insufficient scalability. To overcome these challenges, we propose a novel enterprise network architecture design and algorithm that leverages a multi-factor model for network partitioning. The algorithm takes into consideration various factors influencing the operation of enterprise networks and, in collaboration with the network partition concept, comprehensively calculates the impact values of each factor during business partitioning. The primary objective is to optimize the enterprise network architecture, aiming to enhance overall performance and scalability. Practical application of the proposed algorithm in enterprises has demonstrated its success in optimizing network performance and scalability, thereby improving network management efficiency. The outcomes of this research hold significant implications for addressing challenges in traditional manufacturing enterprise networks, particularly in the domain of computer science.},
booktitle = {Proceedings of the 2024 3rd International Conference on Cryptography, Network Security and Communication Technology},
pages = {426–433},
numpages = {8},
location = {Harbin, China},
series = {CNSCT '24}
}

@article{10.1145/3592598,
author = {Pallewatta, Samodha and Kostakos, Vassilis and Buyya, Rajkumar},
title = {Placement of Microservices-based IoT Applications in Fog Computing: A Taxonomy and Future Directions},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3592598},
doi = {10.1145/3592598},
abstract = {The Fog computing paradigm utilises distributed, heterogeneous and resource-constrained devices at the edge of the network for efficient deployment of latency-critical and bandwidth-hungry IoT application services. Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up with the rapid development and deployment needs of fast-evolving IoT applications. Due to the fine-grained modularity of the microservices and their independently deployable and scalable nature, MSA exhibits great potential in harnessing Fog and Cloud resources, thus giving rise to novel paradigms like Osmotic computing. The loosely coupled nature of the microservices, aided by the container orchestrators and service mesh technologies, enables the dynamic composition of distributed and scalable microservices to achieve diverse performance requirements of the IoT applications using distributed Fog resources. To this end, efficient placement of microservice plays a vital role, and scalable placement algorithms are required to utilise the said characteristics of the MSA while overcoming novel challenges introduced by the architecture. Thus, we present a comprehensive taxonomy of recent literature on microservices-based IoT applications placement within Fog computing environments. Furthermore, we organise multiple taxonomies to capture the main aspects of the placement problem, analyse and classify related works, identify research gaps within each category, and discuss future research directions.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {321},
numpages = {43},
keywords = {application placement, Osmotic computing, Internet of Things, microservice architecture, Fog computing}
}

@inproceedings{10.1145/3665283.3665342,
author = {Kumar M, Ajay and Kumar, Vineet and John, Deepu and Shanker, Shreejith},
title = {Implementation and analysis of custom instructions on RISC-V for Edge-AI applications},
year = {2024},
isbn = {9798400717277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665283.3665342},
doi = {10.1145/3665283.3665342},
abstract = {The growing popularity of open-source processors, such as RISC-V, is reshaping the landscape of System on Chips (SoCs). RISC-V has gained widespread popularity due to its capacity for adaptable expansion of its Instruction Set Architecture (ISA), catering to diverse application needs. This study investigates an ISA extension focusing on memory load and store operations, which are crucial for many applications in embedded systems, including Deep Neural Networks (DNNs). Continuous memory interaction within DNNs leads to increased power consumption and latency. We explored a new instruction for doubling memory access, achieving a reduction of around 50% in clock cycles and approximately 30% in power consumption during memory load and store operations while incurring only a minimal area overhead of approximately 4% and is validated on a modified RISC-V platform. This study also suggests additional ISA extensions, a work currently in progress, to facilitate support for the bfloat16 data type on RISC-V architecture to reduce quantisation losses in resource-constrained devices.},
booktitle = {Proceedings of the 14th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
pages = {126–129},
numpages = {4},
keywords = {Accelerators, Deep Neural Networks, Edge-AI, Memory, RISC-V},
location = {Porto, Portugal},
series = {HEART '24}
}

@inproceedings{10.1145/3654522.3654602,
author = {Ng, Tyne and Bin Rawi, Ahadiyat Abadi and Sum, Chin Sean and Tso, Ejoe and Yau, Peter Chunyu and Wong, Dennis},
title = {Migrating from Monolithic to Microservices with Hybrid Database Design Architecture},
year = {2024},
isbn = {9798400716713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654522.3654602},
doi = {10.1145/3654522.3654602},
abstract = {This paper explores the monolithic and microservice architecture, by proposing a migration strategy using the Domain-Driven Design, implementing a hybrid database design. By decomposing the legacy system's components and integrating microservice architecture, it improves system robustness, scalability, and flexibility. Additionally, enhancement strategies are proposed, such as reducing coupling through microservice decomposition, incorporating design patterns, handling distributed transactions, and implementing eventual consistency using the Saga pattern. This aims to improve system performance through enhanced fault tolerance, reduced service downtimes, and improved communication reliability.},
booktitle = {Proceedings of the 2024 9th International Conference on Intelligent Information Technology},
pages = {536–541},
numpages = {6},
keywords = {design methodology, domain-driven design, microservice, migration strategy, monolithic, saga choreography, software architecture},
location = {Ho Chi Minh City, Vietnam},
series = {ICIIT '24}
}

@inproceedings{10.1145/3651781.3651838,
author = {Verma, Nilesh Kumar and Khetavath, Jairam Naik},
title = {Exploring The Trade-off between Efficient Service Placement Time and Optimized Fog Colonies Utilizing Advanced Genetic Algorithm with Diverse Network Topologies},
year = {2024},
isbn = {9798400708329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651781.3651838},
doi = {10.1145/3651781.3651838},
abstract = {Optimized Fog colony is one of the aid of fog computing, consisting of fog devices, enable efficient management of large fog domains. Well-designed fog colonies can operate independently, resulting in effectively resource utilization and improved system performance. However, there is a lack of optimization approaches for organizing fog devices into colonies and determining which fog colony should be used to stipulated and execute the services required by an IoT application is its primary challenge known as the “efficient service placement time” (ESPT). Contrary to the prevalent practice of treating ESPT as a single objective optimization problem, which often proves insufficient for accommodating the escalating complexities of engineering practice, our study takes a different approach. We present a modeling framework that considers the ESPT in fog computing as a constrained multi-objective optimization problem. Our secondary objective is to minimize the response time of services within the fog colonies. We employ the advanced elitist non-dominated sorting genetic algorithm (MS-NSGA) to optimize the fog colonies for constrained multi-objective service placement problem. We compare our experiment for three different network topologies with various configuration. The experimental result demonstrate the best trade-off between service placement time and proposed scheme for Barabasi-Albert network topology. Additionally, the results also indicate a clear trend towards reduced response time.},
booktitle = {Proceedings of the 2024 13th International Conference on Software and Computer Applications},
pages = {373–378},
numpages = {6},
keywords = {Fog colony, Fog computing, Genetic Algorithm, Response time, Service placement time},
location = {Bali Island, Indonesia},
series = {ICSCA '24}
}

@inproceedings{10.1145/3498765.3498803,
author = {Song, Dongxiang and Ma, Jialuolun and Wang, Yiran},
title = {An Optimization Scheme for College Teacher Recruitment Management System Based on Blockchain and Text Recognition},
year = {2022},
isbn = {9781450385114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498765.3498803},
doi = {10.1145/3498765.3498803},
abstract = {At present, some colleges and universities use the recruitment system to complete the registration and review of candidates in the recruitment work, but the review work still requires reviewers to spend a lot of time to complete. In addition, candidates submit malicious incorrect registration information to disrupt recruitment, which not only increases the workload of reviewers, but also affects the efficiency of recruitment. Candidates need to upload scanned copies of certification materials (such as ID card, degree certificate, graduation certificate, etc.) when registering. Some candidates are worried about whether the information will be leaked and the reviewers are unfair in reviewing for personal gains. In response to these problems, this article proposes to add text recognition to the recruitment management system of a university teacher to complete the system's automatic registration review. Based on the traceability and non-tampering characteristics of blockchain technology, the security and traceability of registration information are guaranteed. Kind of optimization plan. By analyzing the system process, selecting a third-party API interface to complete text recognition, and combining the simple filtering function of the system to build an automated audit function of the system. Select the candidate's registration information as the traceability object, optimize the system process, write smart contracts, web3.js connect the system and the blockchain. Experiments have proved that the automated audit function is automatically executed after candidates submit the registration form, and the average audit speed is 550ms. Reduce the workload of reviewers and improve recruitment efficiency. If the candidate has questions about the review process, the candidate can retrospectively verify whether the registration information has been tampered with through the blockchain, which ensures the fairness of the recruitment work.},
booktitle = {Proceedings of the 13th International Conference on Education Technology and Computers},
pages = {243–251},
numpages = {9},
keywords = {Text recognition, Simple filtering, College Recruitment, Blockchain traceability, Automated Review},
location = {Wuhan, China},
series = {ICETC '21}
}

@inproceedings{10.1145/3603216.3624961,
author = {Singh, Prabhjot and Arun Naik, Shreya and Malekghaini, Navid and Barradas, Diogo and Limam, Noura},
title = {A First Look at Generating Website Fingerprinting Attacks via Neural Architecture Search},
year = {2023},
isbn = {9798400702358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603216.3624961},
doi = {10.1145/3603216.3624961},
abstract = {An adversary can use website fingerprinting (WF) attacks to breach the privacy of users who access the web through encrypted tunnels like Tor. These attacks have increasingly relied on the use of deep neural networks (DNNs) to build powerful classifiers that can match the traffic of a target user to the specific traffic pattern of a website.  In this paper, we study whether the use of neural architecture search (NAS) techniques can provide adversaries with a systematic way to find improved DNNs to launch WF attacks. Concretely, we study the performance of the prominent AutoKeras NAS tool on the WF scenario, under a limited exploration budget, and analyze the effectiveness and efficiency of the resulting DNNs.  Our evaluation reveals that AutoKeras's DNNs achieve a comparable accuracy to that of the state-of-the-art Tik-Tok attack on undefended Tor traffic, and obtain 5--8% accuracy improvements against the FRONT random padding defense, thus highlighting the potential of NAS techniques to enhance the effectiveness of WF.},
booktitle = {Proceedings of the 22nd Workshop on Privacy in the Electronic Society},
pages = {173–178},
numpages = {6},
keywords = {website fingerprinting, neural architecture search, deep learning},
location = {Copenhagen, Denmark},
series = {WPES '23}
}

@inproceedings{10.1145/3650400.3650657,
author = {Zeng, Wenying and Hu, Jianhua and Huang, Lixia and Wu, Weimei},
title = {Design and Implementation of Virtual Real Fusion Metaverse Scene Based on Deep Learning},
year = {2024},
isbn = {9798400708305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650400.3650657},
doi = {10.1145/3650400.3650657},
abstract = {With the rapid development and application of metaverse hotspot technology, it is necessary to simulate and construct various virtual scenes, which may be difficult to meet complex application requirements. To solve the problem of scene construction as above, relevant technical research has been conducted, and a fast method for constructing virtual scenes has been designed. Based on deep learning algorithms, a 3D digital space is constructed through virtual 2D characters and landscape maps to reproduce various real scenes; At the same time, it can interact, roam, update scenes, engage in fitness, socializing, and more. Realized the intelligent scene construction and fusion of virtual and real fusion in the metaverse. This provides a design and implementation reference for the generation of scenes in the metaverse.},
booktitle = {Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1532–1538},
numpages = {7},
location = {Xiamen, China},
series = {EITCE '23}
}

@inproceedings{10.1145/3502181.3531463,
author = {Chitty-Venkata, Krishna Teja and Emani, Murali and Vishwanath, Venkatram and Somani, Arun K.},
title = {Efficient Design Space Exploration for Sparse Mixed Precision Neural Architectures},
year = {2022},
isbn = {9781450391993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502181.3531463},
doi = {10.1145/3502181.3531463},
abstract = {Pruning and Quantization are two effective Deep Neural Network (DNN) compression methods for efficient inference on various hardware platforms. Pruning refers to removing unimportant weights or nodes, whereas Quantization converts the floating-point parameters to low-bit fixed integer representation. The pruned and low precision models result in smaller and faster inference models on hardware platforms with almost the same accuracy as the unoptimized network. Tensor Cores in Nvidia Ampere 100 (A100) GPU supports (1) 2:4 fine-grained sparse pruning where 2 out of every 4 elements are pruned, and (2) traditional dense multiplication to achieve a good accuracy and performance trade-off. The A100 Tensor Core also takes advantage of 1-bit, 4-bit, and 8-bit multiplication to speed up the inference of a model. Hence, finding the right matrix type (dense or 2:4 sparse) along with the precision for each layer becomes a combinatorial problem. Neural Architecture Search (NAS) can alleviate such problems by automating the architecture design process instead of a brute-force search. In this paper, we propose (i) Mixed Sparse and Precision Search (MSPS), a NAS framework to search for efficient sparse and mixed-precision quantized model within the predefined search space and fixed backbone neural network (Eg. ResNet50), and (ii) Architecture, Sparse and Precision Search (ASPS) to jointly search for kernel size and number of filters, and sparse-precision combination of each layer. We illustrate the effectiveness of our methods targeting A100 Tensor Core on Nvidia GPUs by searching efficient sparse-mixed precision networks on ResNet50 and achieving better accuracy-latency trade-off models compared to the manually designed Uniform Sparse Int8 networks.},
booktitle = {Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing},
pages = {265–276},
numpages = {12},
keywords = {sparsity, pruning, neural networks, mixed-precision quantization, hardware-aware neural architecture search, a100 tensor cores},
location = {Minneapolis, MN, USA},
series = {HPDC '22}
}

@article{10.1145/3543852,
author = {Song, Zhuoran and Jing, Naifeng and Liang, Xiaoyao},
title = {E2-VOR: An End-to-End En/Decoder Architecture for Efficient Video Object Recognition},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3543852},
doi = {10.1145/3543852},
abstract = {High-resolution video object recognition (VOR) evolves so fast but is very compute-intensive. This is because VOR leverages compute-intensive deep neural network (DNN) for better accuracy. Although many works have been proposed for speedup, they mostly focus on DNN algorithm and hardware acceleration on the edge side. We observe that most video streams need to be losslessly compressed before going online and an encoder should have all the video information. Moreover, as the cloud should have abundant computing power to handle sophisticated VOR algorithms, we propose to take a one-shot effort for a modified VOR algorithm at the encoding stage in cloud and integrate the full VOR regeneration into a slightly extended decoder on the device. The scheme can enable lightweight VOR with server-class accuracy by simply leveraging the classic and economic video decoder universal to any mobile device. Meanwhile, the scheme can save massive computing power for not repetitively processing the same video on different user devices that makes it extremely sustainable for green computing across the whole network.We propose E2-VOR, an end-to-end encoder and decoder architecture for efficient VOR. We carefully design the scheme to have minimum impact on the video bitstream transmitted. In the cloud, the VOR extended video encoder tracks on a macro-block basis and packs intelligent information into the video stream for increased VOR accuracy and fast regenerating process. On the edge device, we extend the traditional video decoder with a small piece of dedicated hardware to enable the efficient VOR regeneration. Our experiment shows that E2-VOR can achieve 5.0\texttimes{} performance improvement with less than 0.4% VOR accuracy loss compared to the state-of-the-art FAVOS scheme. On average, E2-VOR can run over 54 frames-per-second (FPS) for 480P videos on an edge device.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {10},
numpages = {21},
keywords = {end-to-end, accelerator, neural network, Video object recognition}
}

@inproceedings{10.5555/2820489.2820507,
author = {Casale, G. and Ardagna, D. and Artac, M. and Barbier, F. and Nitto, E. Di and Henry, A. and Iuhasz, G. and Joubert, C. and Merseguer, J. and Munteanu, V. I. and P\'{e}rez, J. F. and Petcu, D. and Rossi, M. and Sheridan, C. and Spais, I. and Vladu\v{s}i\v{c}, D.},
title = {DICE: quality-driven development of data-intensive cloud applications},
year = {2015},
publisher = {IEEE Press},
abstract = {Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.},
booktitle = {Proceedings of the Seventh International Workshop on Modeling in Software Engineering},
pages = {78–83},
numpages = {6},
keywords = {quality assurance, model-driven engineering, big data},
location = {Florence, Italy},
series = {MiSE '15}
}

@article{10.1145/3106158,
author = {Willnecker, Felix and Krcmar, Helmut},
title = {Multi-Objective Optimization of Deployment Topologies for Distributed Applications},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3106158},
doi = {10.1145/3106158},
abstract = {Modern applications are typically implemented as distributed systems comprising several components. Deciding where to deploy which component is a difficult task that today is usually assisted by logical topology recommendations. Choosing inefficient topologies allocates the wrong amount of resources, leads to unnecessary operation costs, or results in poor performance. Testing different topologies to find good solutions takes a lot of time and might delay productive operations. Therefore, this work introduces a software-based deployment topology optimization approach for distributed applications. We use an enhanced performance model generator that extracts models from operational monitoring data of running applications. The extracted model is used to simulate performance metrics (e.g., resource utilization, response times, throughput) and runtime costs of distributed applications. Subsequently, we introduce a deployment topology optimizer, which selects an optimized topology for a specified workload and considers on-premise, cloud, and hybrid topologies. The following three optimization goals are presented in this work: (i) minimum response time for an optimized user experience, (ii) approximate resource utilization around certain peaks, and (iii) minimum cost for running the application. To evaluate the approach, we use the SPECjEnterpriseNEXT industry benchmark as distributed application in an on-premise and in a cloud/on-premise hybrid environment. The evaluation demonstrates the accuracy of the simulation compared to the actual deployment by deploying an optimized topology and comparing measurements with simulation results.},
journal = {ACM Trans. Internet Technol.},
month = jan,
articleno = {21},
numpages = {21},
keywords = {performance model generation, performance model, memory simulation, distributed enterprise applications, Deployment topology optimzation}
}

@inproceedings{10.1145/3498851.3498923,
author = {Yang, Guanqun and Ma, Qiang and Liu, Meng and Zhangiu, Qiang and Xu, Hao and Chen, Xiaolu},
title = {The Unified Authority Platform based on High Performance Blockchain},
year = {2022},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498923},
doi = {10.1145/3498851.3498923},
abstract = {Along with the popularization of enterprise management information system (MIS), more and more business application systems are integrated on the MIS platform, and the need for user identity and authentication management is increasing quickly, which puts forward higher requirements for the security, reliability and disaster recovery of the unified identity authentication service of the unified authority platform. Therefore, there is an urgent need for a new technology architecture to meet the business management needs of unified identity authentication and continuously improve the operation guarantee ability of information communication. Blockchain technology has developed rapidly and has been successfully applied in many fields, especially in data security, with good results. This paper proposes a unified authority authentication mechanism for complex systems based on blockchain, and realizes accurate authority management by dynamically defining user roles, accessible resources and access methods. At the same time, data compression storage technology and parallel access method are used to realize rapid access to blockchain permissions, and improve access efficiency on the premise of ensuring security. Experiments show that the proposed method can realize the dynamic unified control of complex permissions. Compared with ordinary blockchain systems, it has higher access efficiency, and the average authentication time is reduced by more than 39%.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {13–16},
numpages = {4},
keywords = {Keyword number 4, Keyword number 3, Keyword number 2, Insert comma delimited author-supplied keyword list},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3535782.3535827,
author = {Cai, Yu and Wu, Jiahao and Zaheer, Muhammad},
title = {Analysis the Research Hotspots and Key Technical of Intelligent Manufacturing},
year = {2022},
isbn = {9781450395816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535782.3535827},
doi = {10.1145/3535782.3535827},
abstract = {Intelligent manufacturing is one of the keys to open up the future of life, and multidisciplinary interactions have shown its strong vitality. This study conducts a scientometric review of the global research published since 1987 on intelligent manufacturing, through citation, keyword, clustering and keywords burst analysis. A total of 3994 research documents are taken as data sources, which are retrieved from the Web of Science Core Collection database from 1987 to 2021. Using the CiteSpace analysis tool, this paper discussed the development process of the relevant years of intelligent manufacturing, analyzed the research hotspots and their technology, presented the emerging models and techniques, and identified the latest research evolution and trends. The results will provide reference for scientific scholars in future research in the field of intelligent manufacturing.},
booktitle = {Proceedings of the 4th International Conference on Management Science and Industrial Engineering},
pages = {342–349},
numpages = {8},
keywords = {Artificial intelligence, Bibliometrics, Intelligent manufacturing},
location = {Chiang Mai, Thailand},
series = {MSIE '22}
}

@article{10.1109/TNET.2022.3193686,
author = {Wu, Qiang and Zhai, Xiangping Bryce and Liu, Xi and Wu, Chun-Ming and Lou, Fangliang and Zhang, Hongke},
title = {Performance Tuning via Lean Measurements for Acceleration of Network Functions Virtualization},
year = {2022},
issue_date = {Feb. 2023},
publisher = {IEEE Press},
volume = {31},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3193686},
doi = {10.1109/TNET.2022.3193686},
abstract = {Network Functions Virtualization (NFV) replaces the specialized hardware with the software-based forwarding to promise the flexibility, scalability and automation benefits. With an increasing range of applications, NFV must ultimately forward packets at rates that are comparable to the native and specialized hardware-based approaches. However, the transition packet forwarding from specialized hardware to software-based has turned out to be more challenging than expected. Thus, NFV acceleration is desperately needed to play a crucial role in the development of NFV. It is an interesting issue how to address the persistent performance tuning in a way that provides far greater flexibility to meet the demands of power. The existing developments are very inefficient, since that the uncontrollable and unanticipated performance regressions frequently occur. Besides, the environments for full system simulations are traditionally expensive and time consuming to evaluate the system performance. In this paper, we propose the methodology named as “NFV Acceleration via Lean Measurements (NALM)” to tune the performance for the NFV acceleration. NALM provides a holistic measurement approach through combining individual measures to quickly identify the bottlenecks, which can help developers with a better understanding of the design tradeoffs. Moreover, the environments for large scale performance simulation are replaced by a debugger. Thus, the waste is eliminated in terms of time consumption and infrastructure costs of the full system simulation. The systematic analysis of the multi-cores speedup ratio highlights the potential optimization space and rules. We further propose the improvement recommendations on efficient practices. The experiments evaluate the specific effects, and the relationship between the metrics and forwarding performance.},
journal = {IEEE/ACM Trans. Netw.},
month = jul,
pages = {366–379},
numpages = {14}
}

@inproceedings{10.1145/3560905.3568520,
author = {Ling, Neiwen and Huang, Xuan and Zhao, Zhihe and Guan, Nan and Yan, Zhenyu and Xing, Guoliang},
title = {BlastNet: Exploiting Duo-Blocks for Cross-Processor Real-Time DNN Inference},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568520},
doi = {10.1145/3560905.3568520},
abstract = {In recent years, Deep Neural Network (DNN) has been increasingly adopted by a wide range of time-critical applications running on edge platforms with heterogeneous multiprocessors. To meet the stringent timing requirements of these applications, heterogeneous CPU and GPU resources must be efficiently utilized for the inference of multiple DNN models. Such a cross-processor real-time DNN inference paradigm poses major challenges due to the inherent performance imbalance among different processors and the lack of real-time support for cross-processor inference from existing deep learning frameworks. In this work, we propose a new system named BlastNet that exploits duo-block - a new model inference abstraction to support highly efficient cross-processor real-time DNN inference. Each duo-block has a dual model structure, enabling efficient fine-grained inference alternatively across different processors. BlastNet employs a novel block-level Neural Architecture Search (NAS) technique to generate duo-blocks, which accounts for computing characteristics and communication overhead. The duo-blocks are optimized at design time and then dynamically scheduled to achieve high resource utilization of heterogeneous CPU and GPU at runtime. BlastNet is implemented on an indoor autonomous driving platform and three popular edge platforms. Extensive results show that BlastNet achieves 35.07 % less deadline missing rate with a mere 1.63% of model accuracy loss.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {91–105},
numpages = {15},
keywords = {CPU-GPU heterogeneous platform, edge artificial intelligence, multi-DNN concurrent execution, neural architecture search, on-device deep learning, real-time scheduling},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1145/3539491.3539594,
author = {Tirana, Joana and Pappas, Christodoulos and Chatzopoulos, Dimitris and Lalis, Spyros and Vavalis, Manolis},
title = {The role of compute nodes in privacy-aware decentralized AI},
year = {2022},
isbn = {9781450394048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539491.3539594},
doi = {10.1145/3539491.3539594},
abstract = {Mobile devices generate and store voluminous data valuable for training machine learning (ML) models. Decentralized ML model training approaches eliminate the need for sharing such privacy-sensitive data with centralized entities by expecting each data owner that participates in an ML model training process to compute updates locally and share them with other entities. However, the size of state-of-the-art ML models and the computational needs for producing local updates in mobile devices prohibit the participation of mobile devices in decentralized training of such models. Split learning techniques can be combined with decentralized model training protocols to realize the involvement of mobile devices in model training while preserving the privacy of their data. Mobile devices can produce local updates by splitting the model they are training into multiple parts and delegating the processing of the computationally demanding parts to compute nodes. This work examines the impact of the number of available compute nodes and their interaction. We split ResNet101 ML model into 3,4, and 5 parts, keep the first and the last part in the data owner and assign the processing of the middle parts to compute nodes. Additionally, we analyze the training time when the compute nodes assist multiple data owners in parallel or are responsible for different model parts by forming a pipeline.},
booktitle = {Proceedings of the 6th International Workshop on Embedded and Mobile Deep Learning},
pages = {19–24},
numpages = {6},
keywords = {split learning, privacy-aware AI, decentralized AI},
location = {Portland, Oregon},
series = {EMDL '22}
}

@inproceedings{10.1145/3620666.3651328,
author = {Giordano, Massimo and Doshi, Rohan and Lu, Qianyun and Murmann, Boris},
title = {TinyForge: A Design Space Exploration to Advance Energy and Silicon Area Trade-offs in tinyML Compute Architectures with Custom Latch Arrays},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651328},
doi = {10.1145/3620666.3651328},
abstract = {The proliferation of smart IoT devices has given rise to tinyML, which deploys deep neural networks on resource-constrained systems, benefitting from custom hardware that optimizes for low silicon area and high energy efficiency amidst tinyML's characteristic small model sizes (50-500 KB) and low target frequencies (1-100 MHz). We introduce a novel custom latch array integrated with a compute memory fabric, achieving 8 μm2/B density and 11 fJ/B read energy, surpassing synthesized implementations by 7x in density and 5x in read energy. This advancement enables dataflows that do not require activation buffers, reducing memory overheads. By optimizing systolic vs. combinational scaling in a 2D compute array and using bit-serial instead of bit-parallel compute, we achieve a reduction of 4.8x in area and 2.3x in multiply-accumulate energy. To study the advantages of the proposed architecture and its performance at the system level, we architect tinyForge, a design space exploration to obtain Pareto-optimal architectures and compare the trade-offs with respect to traditional approaches. tinyForge comprises (1) a parameterized template for memory hierarchies and compute fabric, (2) estimations of power, area, and latency for hardware components, (3) a dataflow optimizer for efficient workload scheduling, (4) a genetic algorithm performing multi-objective optimization to find Pareto-optimal architectures. We evaluate the performance of our proposed architecture on all of the MLPerf Tiny Inference Benchmark workloads, and the BERT-Tiny transformer model, demonstrating its effectiveness in lowering the energy per inference while addressing the introduced area overheads. We show the importance of storing all the weights on-chip, reducing the energy per inference by 7.5x vs. utilizing off-chip memories. Finally, we demonstrate the potential of the custom latch arrays and bit-serial digital compute arrays to reduce by up to 1.8x the energy per inference, 2.2x the latency per inference, and 3.7x the silicon area.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {1033–1047},
numpages = {15},
keywords = {tinyML, on-device inference, design space exploration, memory hierarchy, custom latch arrays, energy latency and area optimization},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3489517.3530455,
author = {Shin, Jaekang and Choi, Seungkyu and Ra, Jongwoo and Kim, Lee-Sup},
title = {Algorithm/architecture co-design for energy-efficient acceleration of multi-task DNN},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530455},
doi = {10.1145/3489517.3530455},
abstract = {Real-world AI applications, such as augmented reality or autonomous driving, require processing multiple CV tasks simultaneously. However, the enormous data size and the memory footprint have been a crucial hurdle for deep neural networks to be applied in resource-constrained devices. To solve the problem, we propose an algorithm/architecture co-design. The proposed algorithmic scheme, named SqueeD, reduces per-task weight and activation size by 21.9x and 2.1x, respectively, by sharing those data between tasks. Moreover, we design architecture and dataflow to minimize DRAM access by fully utilizing benefits from SqueeD. As a result, the proposed architecture reduces the DRAM access increment and energy consumption increment per task by 2.2x and 1.3x, respectively.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {253–258},
numpages = {6},
location = {San Francisco, California},
series = {DAC '22}
}

@article{10.1145/3494534,
author = {Sohrabizadeh, Atefeh and Yu, Cody Hao and Gao, Min and Cong, Jason},
title = {AutoDSE: Enabling Software Programmers to Design Efficient FPGA Accelerators},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/3494534},
doi = {10.1145/3494534},
abstract = {Adopting FPGA as an accelerator in datacenters is becoming mainstream for customized computing, but the fact that FPGAs are hard to program creates a steep learning curve for software programmers. Even with the help of high-level synthesis (HLS), accelerator designers still have to manually perform code reconstruction and cumbersome parameter tuning to achieve optimal performance. While many learning models have been leveraged by existing work to automate the design of efficient accelerators, the unpredictability of modern HLS tools becomes a major obstacle for them to maintain high accuracy. To address this problem, we propose an automated DSE framework—AutoDSE—that leverages a bottleneck-guided coordinate optimizer to systematically find a better design point. AutoDSE detects the bottleneck of the design in each step and focuses on high-impact parameters to overcome it. The experimental results show that AutoDSE is able to identify the design point that achieves, on the geometric mean, 19.9\texttimes{} speedup over one CPU core for MachSuite and Rodinia benchmarks. Compared to the manually optimized HLS vision kernels in Xilinx Vitis libraries, AutoDSE can reduce their optimization pragmas by 26.38\texttimes{} while achieving similar performance. With less than one optimization pragma per design on average, we are making progress towards democratizing customizable computing by enabling software programmers to design efficient FPGA accelerators.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = feb,
articleno = {32},
numpages = {27},
keywords = {Merlin Compiler, HLS, customized computing, Bottleneck optimizer}
}

@article{10.1109/TASLP.2020.2995270,
author = {Fan, Yang and Tian, Fei and Xia, Yingce and Qin, Tao and Li, Xiang-Yang and Liu, Tie-Yan},
title = {Searching Better Architectures for Neural Machine Translation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2995270},
doi = {10.1109/TASLP.2020.2995270},
abstract = {Neural architecture search (NAS) has played important roles in the evolution of neural architectures. However, no much attention has been paid to improve neural machine translation (NMT) through NAS approaches. In this work, we propose a gradient-based NAS algorithm for NMT, which automatically discovers architectures with better performances. Compared with previous NAS work, we jointly search the network operations (e.g., LSTM, CNN, self-attention etc) as well as dropout rates to ensure better results. We show that with reasonable resources it is possible to discover novel neural network architectures for NMT, which achieve consistently better performances than Transformer [1], the state-of-the-art NMT model, across different tasks. On WMT'14 English-to-German translation, IWSLT'14 German-to-English translation and WMT'18 Finnish-to-English translation tasks, our discovered architectures could obtain 30.1, 36.1 and 26.4 BLEU scores, which are great improvement over Transformer baselines. We also empirically verify that the discovered model on one task can be transferred to other tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1574–1585},
numpages = {12}
}

@inproceedings{10.1145/3531437.3539720,
author = {Risso, Matteo and Burrello, Alessio and Benini, Luca and Macii, Enrico and Poncino, Massimo and Jahier Pagliari, Daniele},
title = {Multi-Complexity-Loss DNAS for Energy-Efficient and Memory-Constrained Deep Neural Networks},
year = {2022},
isbn = {9781450393546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531437.3539720},
doi = {10.1145/3531437.3539720},
abstract = {Neural Architecture Search (NAS) is increasingly popular to automatically explore the accuracy versus computational complexity trade-off of Deep Learning (DL) architectures. When targeting tiny edge devices, the main challenge for DL deployment is matching the tight memory constraints, hence most NAS algorithms consider model size as the complexity metric. Other methods reduce the energy or latency of DL models by trading off accuracy and number of inference operations. Energy and memory are rarely considered simultaneously, in particular by low-search-cost Differentiable NAS (DNAS) solutions. We overcome this limitation proposing the first DNAS that directly addresses the most realistic scenario from a designer’s perspective: the co-optimization of accuracy and energy (or latency) under a memory constraint, determined by the target HW. We do so by combining two complexity-dependent loss functions during training, with independent strength. Testing on three edge-relevant tasks from the MLPerf Tiny benchmark suite, we obtain rich Pareto sets of architectures in the energy vs. accuracy space, with memory footprints constraints spanning from 75% to 6.25% of the baseline networks. When deployed on a commercial edge device, the STM NUCLEO-H743ZI2, our networks span a range of 2.18x in energy consumption and 4.04% in accuracy for the same memory constraint, and reduce energy by up to 2.2 \texttimes{} with negligible accuracy drop with respect to the baseline.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {28},
numpages = {6},
keywords = {TinyML, NAS, Energy-efficiency, Deep Learning},
location = {Boston, MA, USA},
series = {ISLPED '22}
}

@article{10.1145/3598301,
author = {Demirel, Berken Utku and Chen, Luke and Al Faruque, Mohammad Abdullah},
title = {Data-driven Energy-efficient Adaptive Sampling Using Deep Reinforcement Learning},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3598301},
doi = {10.1145/3598301},
abstract = {This article presents a resource-efficient adaptive sampling methodology for classifying electrocardiogram (ECG) signals into different heart rhythms. We present our methodology in two folds: (i) the design of a novel real-time adaptive neural network architecture capable of classifying ECG signals with different sampling rates and (ii) a runtime implementation of sampling rate control using deep reinforcement learning (DRL). By using essential morphological details contained in the heartbeat waveform, the DRL agent can control the sampling rate and effectively reduce energy consumption at runtime. To evaluate our adaptive classifier, we use the MIT-BIH database and the recommendation of the AAMI to train the classifiers. The classifier is designed to recognize three major types of arrhythmias, which are supraventricular ectopic beats (SVEB), ventricular ectopic beats (VEB), and normal beats (N). The performance of the arrhythmia classification reaches an accuracy of 97.2% for SVEB and 97.6% for VEB beats. Moreover, the designed system is 7.3\texttimes{} more energy-efficient compared to the baseline architecture, where the adaptive sampling rate is not utilized. The proposed methodology can provide reliable and accurate real-time ECG signal analysis with performances comparable to state-of-the-art methods. Given its time-efficient, low-complexity, and low-memory-usage characteristics, the proposed methodology is also suitable for practical ECG applications, in our case for arrhythmia classification, using resource-constrained devices, especially wearable healthcare devices and implanted medical devices.},
journal = {ACM Trans. Comput. Healthcare},
month = sep,
articleno = {19},
numpages = {19},
keywords = {wearable devices, real-time, heart rate, energy-efficient, Adaptive sampling}
}

@inproceedings{10.1145/3468264.3468607,
author = {Pei, Kexin and Guan, Jonas and Broughton, Matthew and Chen, Zhongtian and Yao, Songchen and Williams-King, David and Ummadisetty, Vikas and Yang, Junfeng and Ray, Baishakhi and Jana, Suman},
title = {StateFormer: fine-grained type recovery from binaries using generative state modeling},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468607},
doi = {10.1145/3468264.3468607},
abstract = {Binary type inference is a critical reverse engineering task supporting many security applications, including vulnerability analysis, binary hardening, forensics, and decompilation. It is a difficult task because source-level type information is often stripped during compilation, leaving only binaries with untyped memory and register accesses. Existing approaches rely on hand-coded type inference rules defined by domain experts, which are brittle and require nontrivial effort to maintain and update. Even though machine learning approaches have shown promise at automatically learning the inference rules, their accuracy is still low, especially for optimized binaries.  We present StateFormer, a new neural architecture that is adept at accurate and robust type inference. StateFormer follows a two-step transfer learning paradigm. In the pretraining step, the model is trained with Generative State Modeling (GSM), a novel task that we design to teach the model to statically approximate execution effects of assembly instructions in both forward and backward directions. In the finetuning step, the pretrained model learns to use its knowledge of operational semantics to infer types.  We evaluate StateFormer's performance on a corpus of 33 popular open-source software projects containing over 1.67 billion variables of different types. The programs are compiled with GCC and LLVM over 4 optimization levels O0-O3, and 3 obfuscation passes based on LLVM. Our model significantly outperforms state-of-the-art ML-based tools by 14.6% in recovering types for both function arguments and variables. Our ablation studies show that GSM improves type inference accuracy by 33%.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {690–702},
numpages = {13},
keywords = {Type Inference, Transfer Learning, Reverse Engineering, Machine Learning for Program Analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3551659.3559051,
author = {Kain, Ruslan and Elsayed, Sara A. and Chen, Yuanzhu and Hassanein, Hossam S.},
title = {Multi-step Prediction of Worker Resource Usage at the Extreme Edge},
year = {2022},
isbn = {9781450394826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551659.3559051},
doi = {10.1145/3551659.3559051},
abstract = {Democratizing the edge by leveraging the prolific yet underutilized computational resources of end devices, referred to as Extreme Edge Devices (EEDs), can open a new edge computing tech market that is people-owned, democratically managed, and accessible/lucrative to all. Parallel computing at EEDs can also move the computing service much closer to end-users, which can help satisfy the stringent Quality-of-Service (QoS) requirements of delay-critical and/or data-intensive IoT applications. However, EEDs are heterogeneous user-owned devices, and are thus subject to a highly dynamic user access behavior (i.e., dynamic resource usage). This makes the process of determining the computational capability of EEDs increasingly challenging. Estimating the dynamic resource usage of EEDs (i.e., workers) has been mostly overlooked. The complexity of Machine Learning (ML)-based models renders them impractical for deployment at the edge for the purpose of such estimations. In this paper, we propose the Resource Usage Multi-step Prediction (RUMP) scheme to estimate the dynamic resource usage of workers over multiple steps ahead in a computationally efficient way while providing a relatively high prediction accuracy. Towards that end, RUMP exploits the use of the Hierarchical Dirichlet Process-Hidden Semi-Markov Model (HDP-HSMM) to estimate the dynamic resource usage of workers in EED-based computing paradigms. Extensive evaluations on a real testbed of heterogeneous workers for multi-step sizes show an 87.5% prediction accuracy for the starting point of 2-steps and coming to as little as a 16% average difference in prediction error compared to a representative of state-of-the-art ML-based schemes.},
booktitle = {Proceedings of the 25th International ACM Conference on Modeling Analysis and Simulation of Wireless and Mobile Systems},
pages = {25–32},
numpages = {8},
keywords = {multi-access edge computing, hidden semi-Markov model, extreme edge, edge democratization, dynamic resource usage, LSTM},
location = {Montreal, Quebec, Canada},
series = {MSWiM '22}
}

@inproceedings{10.1145/3605573.3605620,
author = {Bian, Yi and Zheng, Fangyu and Wang, Yuewu and Lei, Lingguang and Ma, Yuan and Dong, Jiankuo and Jing, Jiwu},
title = {AsyncGBP: Unleashing&nbsp;the&nbsp;Potential&nbsp;of&nbsp;Heterogeneous&nbsp; Computing&nbsp;for&nbsp;SSL/TLS&nbsp;with&nbsp;GPU-based&nbsp;Provider},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605620},
doi = {10.1145/3605573.3605620},
abstract = {The proliferation of IoT and 5G technologies has led to an explosion of data traffic that data centers must handle while ensuring secure transmission via SSL/TLS. The high volume of cryptographic operations required imposes performance bottlenecks. The GPU-based cryptographic accelerator is one of the competitive solutions. However, significant structural differences with practical applications confine their capacities to specific domains, such as offline cryptanalysis, undermining their potential for real-world cryptographic acceleration. This paper investigates the feasibility of using GPUs as cryptographic accelerators for concurrent data secure transmission scenarios like SSL/TLS. Specifically, we propose AsyncGBP, a framework that integrates the original OpenSSL software stack with the heterogeneous GPU-based accelerator. To enhance user-friendliness and take full advantage of GPUs’ SIMT execution model, AsyncGBP features an OpenSSL-compatible asynchronous design, which seamlessly converts cryptographic requests from synchronous to asynchronous mode, efficiently aggregates numerous requests, and rationally schedules GPU for computation. We also provide a fine-grained GPU-based cryptographic algorithm stack that includes X25519, Ed25519, and ChaCha20-Poly1305. A comprehensive evaluation shows that AsyncGBP can efficiently achieve up to 97% of GPU local performance on an RTX 3070, resulting in an improvement of up to 137x compared to the default OpenSSL provider in a single-process setting. Furthermore, AsyncGBP outperforms the existing fastest commercial-off-the-shelf OpenSSL-compatible TLS accelerator by a significant margin, achieving a 5.3x to 7.0x performance improvement.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {337–346},
numpages = {10},
keywords = {Graphics Processing Unit, Heterogeneous Computing, TLS 1.3},
location = {Salt Lake City, UT, USA},
series = {ICPP '23}
}

@inproceedings{10.1145/3576841.3585936,
author = {Robinette, Preston K. and Hamilton, Nathaniel P. and Johnson, Taylor T.},
title = {Self-Preserving Genetic Algorithms for Safe Learning in Discrete Action Spaces},
year = {2023},
isbn = {9798400700361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576841.3585936},
doi = {10.1145/3576841.3585936},
abstract = {Self-Preserving Genetic Algorithms (SPGA) combine the evolutionary strategy of a genetic algorithm with safety assurance methods commonly implemented in safe reinforcement learning (SRL), a branch of reinforcement learning (RL) that accounts for safety in the exploration and decision-making process of the agent. Safe learning approaches are especially important in safety-critical environments, where failure to account for the safety of the controlled system could result in the loss of millions of dollars in hardware or bodily harm to people working nearby, as is true of many cyber-physical systems. While SRL is a viable approach to safe learning, there are many challenges that must be taken into consideration when training agents, such as sample efficiency, stability, and exploration---an issue that is easily addressed by the evolutionary strategy of a genetic algorithm. By combining GAs with the safety mechanisms used with SRL, SPGA offers a safe learning alternative that is able to explore large areas of the solution space, addressing SRL's challenge of exploration. This work implements SPGA with both action masking and run time assurance safety strategies to evolve safe controllers for three types of discrete action space environments applicable to cyber physical systems (control, routing, and operations) and under various safety conditions. Training and testing evaluation metrics are compared with results from SRL trained controllers to validate results. SPGA and SRL controllers are trained across 5 random seeds and evaluated on 500 episodes to calculate average wall time to train, average expected return, and percentage of safe action evaluation metrics. SPGA achieves comparable reward and safety performance results with significantly improved training efficiency (55x faster on average), demonstrating the effectiveness of this safe learning approach.},
booktitle = {Proceedings of the ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2023)},
pages = {110–119},
numpages = {10},
keywords = {action masking, run time assurance, safe reinforcement learning, safe learning, genetic algorithms},
location = {San Antonio, TX, USA},
series = {ICCPS '23}
}

@inproceedings{10.1145/3508352.3549357,
author = {Hussain, Shehzeen and Sheybani, Nojan and Neekhara, Paarth and Zhang, Xinqiao and Duarte, Javier and Koushanfar, Farinaz},
title = {FastStamp: Accelerating Neural Steganography and Digital Watermarking of Images on FPGAs},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549357},
doi = {10.1145/3508352.3549357},
abstract = {Steganography and digital watermarking are the tasks of hiding recoverable data in image pixels. Deep neural network (DNN) based image steganography and watermarking techniques are quickly replacing traditional hand-engineered pipelines. DNN based water-marking techniques have drastically improved the message capacity, imperceptibility and robustness of the embedded watermarks. However, this improvement comes at the cost of increased computational overhead of the watermark encoder neural network. In this work, we design the first accelerator platform FastStamp to perform DNN based steganography and digital watermarking of images on hardware. We first propose a parameter efficient DNN model for embedding recoverable bit-strings in image pixels. Our proposed model can match the success metrics of prior state-of-the-art DNN based watermarking methods while being significantly faster and lighter in terms of memory footprint. We then design an FPGA based accelerator framework to further improve the model throughput and power consumption by leveraging data parallelism and customized computation paths. FastStamp allows embedding hardware signatures into images to establish media authenticity and ownership of digital media. Our best design achieves 68\texttimes{} faster inference as compared to GPU implementations of prior DNN based watermark encoder while consuming less power.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {41},
numpages = {9},
keywords = {steganography, digital watermarking, deep learning, FPGA},
location = {San Diego, California},
series = {ICCAD '22}
}

@inproceedings{10.1145/3377170.3377218,
author = {Li, Liangding and Chi, Jiapeng and Wang, Jun},
title = {Applying LSTM to Enable Cache Prefetching to Optimize Flow Table Update Efficiency in SDN Switches},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377218},
doi = {10.1145/3377170.3377218},
abstract = {Due to the fast growth of the Internet and social network, a massive amount of data has been generated and move to the cloud. With the ability to separate the control and data plane, SDN (Software Defined Network) provide an emerging solution for data transportation management tasks in the data center. In recent years, more literature focused on using SDN to manage data center network. It has been shown that SDN switch can support fine-grained rule matching with more than 12-tuple flow. However, Ternary Content Addressable Memory (TCAM), which used to store the flow table in SDN switch, has limited capacity and power-hungry. The performance of the data center throughput would reduce dramatically due to flow table overflow. Previous literature proposed two kinds of solutions, rule replacement and rule caching. In this paper, we propose a new rule caching method based on Long short-term memory (LSTM) to improve the cache hit ratio in SDN switches. From the experiment result, we surprisingly find that the deep learning based prefetching model can predict future flow rules with high accuracy. And then improve the cache hit ratio on TCAM compare with the famous FIFO and LRU cache.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {126–130},
numpages = {5},
keywords = {neural network, flow table, TCAM, SDN, LSTM},
location = {Shanghai, China},
series = {ICIT '19}
}

@article{10.1109/TCBB.2021.3128366,
author = {Bo\v{z}a, Vladim\'{\i}r and Pere\v{s}\'{\i}ni, Peter and Brejov\'{a}, Bro\v{n}a and Vina\v{r}, Tom\'{a}\v{s}},
title = {Dynamic Pooling Improves Nanopore Base Calling Accuracy},
year = {2021},
issue_date = {Nov.-Dec. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3128366},
doi = {10.1109/TCBB.2021.3128366},
abstract = {In nanopore sequencing, electrical signal is measured as DNA molecules pass through the sequencing pores. Translating these signals into DNA bases (base calling) is a highly non-trivial task, and its quality has a large impact on the sequencing accuracy. The most successful nanopore base callers to date use convolutional neural networks (CNN) to accomplish the task. Convolutional layers in CNNs are typically composed of filters with constant window size, performing best in analysis of signals with uniform speed. However, the speed of nanopore sequencing varies greatly both within reads and between sequencing runs. Here, we present dynamic pooling, a novel neural network component, which addresses this problem by adaptively adjusting the pooling ratio. To demonstrate the usefulness of dynamic pooling, we developed two base callers: Heron and Osprey. Heron improves the accuracy beyond the experimental high-accuracy base caller Bonito developed by Oxford Nanopore. Osprey is a fast base caller that can compete in accuracy with Guppy high-accuracy mode, but does not require GPU acceleration and achieves a near real-time speed on common desktop CPUs. Availability: &lt;uri&gt;https://github.com/fmfi-compbio/osprey&lt;/uri&gt;, &lt;uri&gt;https://github.com/fmfi-compbio/heron&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = nov,
pages = {3416–3424},
numpages = {9}
}

@inproceedings{10.1109/ASP-DAC52403.2022.9712485,
author = {Neda, Negar and Ullah, Salim and Ghanbari, Azam and Mahdiani, Hoda and Modarressi, Mehdi and Kumar, Akash},
title = {Multi-Precision Deep Neural Network Acceleration on FPGAs},
year = {2022},
isbn = {9781665421355},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC52403.2022.9712485},
doi = {10.1109/ASP-DAC52403.2022.9712485},
abstract = {Quantization is a promising approach to reduce the computational load of neural networks. The minimum bit-width that preserves the original accuracy varies significantly across different neural networks and even across different layers of a single neural network. Most existing designs over-provision neural network accelerators with sufficient bit-width to preserve the required accuracy across a wide range of neural networks. In this paper, we present mpDNN, a multi-precision multiplier with dynamically adjustable bit-width for deep neural network acceleration. The design supports run-time splitting an arithmetic operator into multiple independent operators with smaller bit-width, effectively increasing throughput when lower precision is required. The proposed architecture is designed for FPGAs, in that the multipliers and bit-width adjustment mechanism are optimized for the LUT-based structure of FPGAs. Experimental results show that by enabling run-time precision adjustment, mpDNN can offer 3-15x improvement in throughput.},
booktitle = {Proceedings of the 27th Asia and South Pacific Design Automation Conference},
pages = {454–459},
numpages = {6},
location = {Taipei, Taiwan},
series = {ASPDAC '22}
}

@article{10.1109/TNET.2024.3364176,
author = {Li, Fuliang and Lv, Yiming and Yan, Yangsheng and Gao, Chengxi and Wang, Xingwei and Cao, Jiannong},
title = {Learning-Based Sketch for Adaptive and High-Performance Network Measurement},
year = {2024},
issue_date = {June 2024},
publisher = {IEEE Press},
volume = {32},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3364176},
doi = {10.1109/TNET.2024.3364176},
abstract = {With the development of network measurement technologies, a hybrid measurement architecture can effectively optimize the sketch structure in switches, making it more adaptable to the current complex and volatile network environment. However, current optimization technologies based on hybrid measurement architectures generally suffer from insufficient automation, difficulty of learning effective numerical features, and lack of generality, resulting in poor scalability in real deployment. To solve these problems, we propose the &lt;italic&gt;TalentSketch&lt;/italic&gt; framework, based on which we further develop &lt;italic&gt;DeepSketch&lt;/italic&gt; for effective sketch optimization. First, we use &lt;italic&gt;Seq2Seq&lt;/italic&gt; to automatically identify target flows instead of relying on manual thresholds. Second, we propose a new training strategy that extracts low-precision flows for models with weak learning capabilities. Last, we develop a new sketch optimization framework that can optimize different kinds of sketches only by changing the training data for generality. A large number of experimental results show that &lt;italic&gt;DeepSketch&lt;/italic&gt; exhibits superior performance. For example: (1) the accuracy of optimized sketches has increased by 20% to 73%, (2) Without replacing the model structure, the accuracy of the optimized sketches can generally reach over 80%. (3) The impact of low sampling rates on accuracy is less than 1% on various sketches.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {2571–2585},
numpages = {15}
}

@inproceedings{10.1145/3172871.3172886,
author = {Srinivasan, Madhan Kumar and Revathy, P.},
title = {State-of-the-art Big Data Security Taxonomies},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172886},
doi = {10.1145/3172871.3172886},
abstract = {Today's businesses accumulate an astonishing amount of digital data, which can be leveraged to unlock new sources of economic value and provide fresh insights into business trends. The real challenge in this process is the design of computing, storage infrastructure and algorithms needed to handle this "Big Data". Hence, organizations are looking at different ways in which they can make use of Big Data in their business. There's no doubt that the creation of a Hadoop-powered Data Lake can provide a robust foundation for a new generation of analytics and intuitive results. At the same time, it is also very necessary to consider security before launching or expanding a Hadoop initiative. As we move towards a stage where Hadoop is considered for real-time production scenarios rather than just experimentation levels, a major chunk of production data is normally sensitive, or subject to many industry regulations and governance controls. This paper analyzes the current security challenges in big data implementations based on state-of-the-art big data security taxonomies.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {16},
numpages = {7},
keywords = {NoSQL Concerns, Identity Management, Hadoop Security, Hadoop Framework Security, Hadoop, Cloud Security, Big Data Security Taxonomies, Big Data Security, Big Data Analytics, Big Data},
location = {Hyderabad, India},
series = {ISEC '18}
}

@article{10.1145/3654966,
author = {Le, Van-Hoang and Zhang, Hongyu},
title = {PreLog: A Pre-trained Model for Log Analytics},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654966},
doi = {10.1145/3654966},
abstract = {Large-scale software-intensive systems often produce a large volume of logs to record runtime status and events for troubleshooting purposes. The rich information in log data enables a variety of system management and diagnosis tasks. Over the years, many approaches have been proposed for automated log analytics. However, these approaches usually design separate models for each specific task, which cannot be generalized to other tasks. They are also not robust when dealing with logs from heterogeneous sources. In this paper, we propose PreLog, a novel pre-trained model for log analytics. PreLog is pre-trained on a large amount of unlabelled log data to capture the semantic meaning of logs. We design two log-specific pre-training objectives, including entry-level and sequence-level objectives, which enable PreLog to better understand the hidden structure and semantics of logs. To perform downstream log analytics tasks, we leverage a prompt tuning paradigm to convert downstream tasks' objectives into a similar form as the pre-training stage. We have conducted extensive experiments on two main log analytics tasks (i.e., log parsing and log-based anomaly detection). Experimental results show that PreLog achieves better or comparable results in comparison with the state-of-the-art, task-specific approaches. PreLog is cost-effective and can be uniformly applied to many log analytics tasks through the prompt tuning paradigm.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {163},
numpages = {28},
keywords = {log analytics, log data, log parsing, log-based anomaly detection, pre-training}
}

@inproceedings{10.1145/3229556.3229562,
author = {Li, En and Zhou, Zhi and Chen, Xu},
title = {Edge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy},
year = {2018},
isbn = {9781450359061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229556.3229562},
doi = {10.1145/3229556.3229562},
abstract = {As the backbone technology of machine learning, deep neural networks (DNNs) have have quickly ascended to the spotlight. Running DNNs on resource-constrained mobile devices is, however, by no means trivial, since it incurs high performance and energy overhead. While offloading DNNs to the cloud for execution suffers unpredictable performance, due to the uncontrolled long wide-area network latency. To address these challenges, in this paper, we propose Edgent, a collaborative and on-demand DNN co-inference framework with device-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that adaptively partitions DNN computation between device and edge, in order to leverage hybrid computation resources in proximity for real-time DNN inference. (2) DNN right-sizing that accelerates DNN inference through early-exit at a proper intermediate DNN layer to further reduce the computation latency. The prototype implementation and extensive evaluations based on Raspberry Pi demonstrate Edgent's effectiveness in enabling on-demand low-latency edge intelligence.},
booktitle = {Proceedings of the 2018 Workshop on Mobile Edge Communications},
pages = {31–36},
numpages = {6},
keywords = {Edge Intelligence, Edge Computing, Deep Learning, Computation Offloading},
location = {Budapest, Hungary},
series = {MECOMM'18}
}

@inproceedings{10.1145/3613904.3642109,
author = {Hohman, Fred and Kery, Mary Beth and Ren, Donghao and Moritz, Dominik},
title = {Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642109},
doi = {10.1145/3613904.3642109},
abstract = {On-device machine learning (ML) promises to improve the privacy, responsiveness, and proliferation of new, intelligent user experiences by moving ML computation onto everyday personal devices. However, today’s large ML models must be drastically compressed to run efficiently on-device, a hurtle that requires deep, yet currently niche expertise. To engage the broader human-centered ML community in on-device ML experiences, we present the results from an interview study with 30 experts at Apple that specialize in producing efficient models. We compile tacit knowledge that experts have developed through practical experience with model compression across different hardware platforms. Our findings offer pragmatic considerations missing from prior work, covering the design process, trade-offs, and technical strategies that go into creating efficient models. Finally, we distill design recommendations for tooling to help ease the difficulty of this work and bring on-device ML into to more widespread practice.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {645},
numpages = {18},
keywords = {Efficient machine learning, design directions, interactive systems, interview study, model compression, on-device machine learning},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3526113.3545709,
author = {Lee, Jungeun and Kim, Sungnam and Cheon, Minki and Ju, Hyojin and Lee, JaeEun and Hwang, Inseok},
title = {SleepGuru: Personalized Sleep Planning System for Real-life Actionability and Negotiability},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545709},
doi = {10.1145/3526113.3545709},
abstract = {Widely-accepted sleep guidelines advise regular bedtimes and sleep hygiene. An individual’s adherence is often viewed as a matter of self-regulation and anti-procrastination. We pose a question from a different perspective: What if it comes to a matter of one’s social or professional duty that mandates irregular daily life, making it incompatible with the premise of standard guidelines? We propose SleepGuru, an individually actionable sleep planning system featuring one’s real-life compatibility and extended forecast. Adopting theories on sleep physiology, SleepGuru builds a personalized predictor on the progression of the user’s sleep pressure over a course of upcoming schedules and past activities sourced from her online calendar and wearable fitness tracker. Then, SleepGuru service provides individually actionable multi-day sleep schedules which respect the user’s inevitable real-life irregularities while regulating her week-long sleep pressure. We elaborate on the underlying physiological principles and mathematical models, followed by a 3-stage study and deployment. We develop a mobile user interface providing individual predictions and adjustability backed by cloud-side optimization. We deploy SleepGuru in-the-wild to 20 users for 8 weeks, where we found positive effects of SleepGuru in sleep quality, compliance rate, sleep efficiency, alertness, long-term followability, and so on.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {52},
numpages = {16},
keywords = {real-life constraints, personally optimized sleep schedule, computational sleep model, actionable sleep},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3297156.3297245,
author = {Xiang, Zhenggui},
title = {How Deep Is Optimal for Learning Locally on Smartphone},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297245},
doi = {10.1145/3297156.3297245},
abstract = {In this paper, we introduced some methods to optimize such deep learning models as convolutional neural network and recurrent neural network. Firstly, we introduced some dropout techniques. By regularizing with dropout, we will prevent overfitting and balance the depth of the neural network and the width of each layer. Secondly, we designed an architecture with less layers but more sophisticated activation functions. Thirdly, we adjusted the learning rate and momentum of stochastic gradient descent (SGD) optimization algorithm. SGD can lead to fast convergence by following the negative gradient of the objective based on a mini batch of training subset. Finally, we discussed model compression. The aim is to train locally the deep learning model on smartphone.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {126–130},
numpages = {5},
keywords = {Smartphone, Optimization, Optimal Architecture, Deep Learning},
location = {Shenzhen, China},
series = {CSAI '18}
}

@article{10.1145/3638531,
author = {Chen, Huaming and Babar, M. Ali},
title = {Security for Machine Learning-based Software Systems: A Survey of Threats, Practices, and Challenges},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3638531},
doi = {10.1145/3638531},
abstract = {The rapid development of Machine Learning (ML) has demonstrated superior performance in many areas, such as computer vision and video and speech recognition. It has now been increasingly leveraged in software systems to automate the core tasks. However, how to securely develop the machine learning-based modern software systems (MLBSS) remains a big challenge, for which the insufficient consideration will largely limit its application in safety-critical domains. One concern is that the present MLBSS development tends to be rushed, and the latent vulnerabilities and privacy issues exposed to external users and attackers will be largely neglected and hard to be identified. Additionally, machine learning-based software systems exhibit different liabilities towards novel vulnerabilities at different development stages from requirement analysis to system maintenance, due to its inherent limitations from the model and data and the external adversary capabilities. The successful generation of such intelligent systems will thus solicit dedicated efforts jointly from different research areas, i.e., software engineering, system security, and machine learning. Most of the recent works regarding the security issues for ML have a strong focus on the data and models, which has brought adversarial attacks into consideration. In this work, we consider that security for machine learning-based software systems may arise from inherent system defects or external adversarial attacks, and the secure development practices should be taken throughout the whole lifecycle. While machine learning has become a new threat domain for existing software engineering practices, there is no such review work covering the topic. Overall, we present a holistic review regarding the security for MLBSS, which covers a systematic understanding from a structure review of three distinct aspects in terms of security threats. Moreover, it provides a thorough state-of-the-practice for MLBSS secure development. Finally, we summarize the literature for system security assurance and motivate the future research directions with open challenges. We anticipate this work provides sufficient discussion and novel insights to incorporate system security engineering for future exploration.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {151},
numpages = {38},
keywords = {Machine learning, system security, secure development practices, software engineering}
}

@inproceedings{10.1145/3307650.3322227,
author = {Sriraman, Akshitha and Dhanotia, Abhishek and Wenisch, Thomas F.},
title = {SoftSKU: optimizing server architectures for microservice diversity @scale},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322227},
doi = {10.1145/3307650.3322227},
abstract = {The variety and complexity of microservices in warehouse-scale data centers has grown precipitously over the last few years to support a growing user base and an evolving product portfolio. Despite accelerating microservice diversity, there is a strong requirement to limit diversity in underlying server hardware to maintain hardware resource fungibility, preserve procurement economies of scale, and curb qualification/test overheads. As such, there is an urgent need for strategies that enable limited server CPU architectures (a.k.a "SKUs") to provide performance and energy efficiency over diverse microservices. To this end, we first undertake a comprehensive characterization of the top seven microservices that run on the compute-optimized data center fleet at Facebook.Our characterization reveals profound diversity in OS and I/O interaction, cache misses, memory bandwidth utilization, instruction mix, and CPU stall behavior. Whereas customizing a CPU SKU for each microservice might be beneficial, it is prohibitive. Instead, we argue for "soft SKUs", wherein we exploit coarse-grain (e.g., boot time) configuration knobs to tune the platform for a particular microservice. We develop a tool, μSKU, that automates search over a soft-SKU design space using A/B testing in production and demonstrate how it can obtain statistically significant gains (up to 7.2% and 4.5% performance improvement over stock and production servers, respectively) with no additional hardware requirements.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {513–526},
numpages = {14},
keywords = {soft SKU, resource fungibility, microservice},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3579371.3589049,
author = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jabbour, Jason and Uchendu, Ikechukwu and Ghosh, Susobhan and Boroujerdian, Behzad and Richins, Daniel and Tripathy, Devashree and Faust, Aleksandra and Janapa Reddi, Vijay},
title = {ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589049},
doi = {10.1145/3579371.3589049},
abstract = {Machine learning (ML) has become a prevalent approach to tame the complexity of design space exploration for domain-specific architectures. While appealing, using ML for design space exploration poses several challenges. First, it is not straightforward to identify the most suitable algorithm from an ever-increasing pool of ML methods. Second, assessing the trade-offs between performance and sample efficiency across these methods is inconclusive. Finally, the lack of a holistic framework for fair, reproducible, and objective comparison across these methods hinders the progress of adopting ML-aided architecture design space exploration and impedes creating repeatable artifacts. To mitigate these challenges, we introduce ArchGym, an open-source gymnasium and easy-to-extend framework that connects a diverse range of search algorithms to architecture simulators. To demonstrate its utility, we evaluate ArchGym across multiple vanilla and domain-specific search algorithms in the design of a custom memory controller, deep neural network accelerators, and a custom SoC for AR/VR workloads, collectively encompassing over 21K experiments. The results suggest that with an unlimited number of samples, ML algorithms are equally favorable to meet the user-defined target specification if its hyperparameters are tuned thoroughly; no one solution is necessarily better than another (e.g., reinforcement learning vs. Bayesian methods). We coin the term "hyperparameter lottery" to describe the relatively probable chance for a search algorithm to find an optimal design provided meticulously selected hyperparameters. Additionally, the ease of data collection and aggregation in ArchGym facilitates research in ML-aided architecture design space exploration. As a case study, we show this advantage by developing a proxy cost model with an RMSE of 0.61% that offers a 2,000-fold reduction in simulation time. Code and data for ArchGym is available at https://bit.ly/ArchGym.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {14},
numpages = {16},
keywords = {reproducibility, baselines, open source, bayesian optimization, reinforcement learning, machine learning for system, machine learning for computer architecture, machine learning},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3377049.3377060,
author = {Nawjis, Nafiul and Alam, mohammad Shafiul and Emu, Mahzabeen},
title = {Hybridization of Evolutionary and Swarm Intelligence Algorithms for improved performance: A case study with TSP problem},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377060},
doi = {10.1145/3377049.3377060},
abstract = {This paper conducts the hybridization of Swarm intelligence and Evolutionary Algorithm for Continuous and Discrete optimization. Optimization is the process of selecting the best element by following some rules and criteria from some set of available alternatives. Function optimization means finding the best available value of some given objective function in a defined domain. In this work we have proposed an innovative approach, by hybridizing Genetic Algorithm (GA) and Swarm Intelligence Algorithm (SIA). In this paper work we have implemented one evolutionary programming based algorithm - Improved First Evolutionary Programming (IFEP) and one swarm intelligence algorithm - Ant Colony Optimization (ACO). We have also used Travelling Salesman Problem (TSP) as a discrete problem. We have implemented both GA and ACO also to solve the Travelling Salesman Problem. We have compared the result produced by IFEP and ACO for Continuous Optimization. From the comparative study we have found that ACO is the better among the two. We also have compared the result produced by GA and ACO for Discrete Optimization and from the comparative study we have found that ACO often works better. We have conducted some experiments to optimize the parameters of ACO and GA and the amount of exploration and exploitation needed for ACO to produce the best result. using the best found parameter we have implemented a hybrid of Genetic Algorithm and Swarm Intelligence Algorithm and tested it with different strategies. Then we have conducted a comparative study between the hybrid and two other conventional Genetic and Swarm Intelligence Algorithms to observe the performance of our proposed hybrid algorithm. In some cases we have observed better performance from our proposed hybrid algorithm.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {4},
numpages = {7},
keywords = {Ant Colony Optimization (ACO), Evolutionary Algorithm, Genetic Algorithm, Proposed hybrid GACO Architecture, Swarm intelligence Algorithm (SIA), Travelling Salesman Problem(TSP)},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@inproceedings{10.1145/2847263.2847280,
author = {Zgheib, Grace and Lortkipanidze, Manana and Owaida, Muhsen and Novo, David and Ienne, Paolo},
title = {FPRESSO: Enabling Express Transistor-Level Exploration of FPGA Architectures},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847280},
doi = {10.1145/2847263.2847280},
abstract = {In theory, tools like VTR---a retargetable toolchain mapping circuits onto easily-described hypothetical FPGA architectures---could play a key role in the development of wildly innovative FPGA architectures. In practice, however, the experiments that one can conduct with these tools are severely limited by the ability of FPGA architects to produce reliable delay and area models---these depend on transistor-level design techniques which require a different set of skills. In this paper, we introduce a novel approach, which we call Fpresso, to model the delay and area of a wide range of largely different FPGA architectures quickly and with reasonable accuracy. We take inspiration from the way a standard-cell flow performs large scale transistor-size optimization and apply the same concepts to FPGAs, only at a coarser granularity. Skilled users prepare for fpresso locally optimized libraries of basic components with a variety of driving strengths. Then, ordinary users specify arbitrary FPGA architectures as interconnects of basic components. This is globally optimized within minutes through an ordinary logic synthesis tool which chooses the most fitting version of each cell and adds buffers wherever appropriate. The resulting delay and area characteristics can be automatically used for VTR. Our results show that fpresso provides models that are on average within some 10-20% of those by a state-of-the-art FPGA optimization tool and is orders of magnitude faster. Although the modelling error may appear relatively high,we show that it seldom results in misranking a set of architectures, thus indicating a reasonable modeling faithfulness.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {80–89},
numpages = {10},
keywords = {transistor design, fpga exploration, fpga architecture, delay-area modelling, characterization},
location = {Monterey, California, USA},
series = {FPGA '16}
}

@inproceedings{10.1145/3377811.3380376,
author = {Zhang, Yuxia and Zhou, Minghui and Stol, Klaas-Jan and Wu, Jianyu and Jin, Zhi},
title = {How do companies collaborate in open source ecosystems? an empirical study of OpenStack},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380376},
doi = {10.1145/3377811.3380376},
abstract = {Open Source Software (OSS) has come to play a critical role in the software industry. Some large ecosystems enjoy the participation of large numbers of companies, each of which has its own focus and goals. Indeed, companies that otherwise compete, may become collaborators within the OSS ecosystem they participate in. Prior research has largely focused on commercial involvement in OSS projects, but there is a scarcity of research focusing on company collaborations within OSS ecosystems. Some of these ecosystems have become critical building blocks for organizations worldwide; hence, a clear understanding of how companies collaborate within large ecosystems is essential. This paper presents the results of an empirical study of the OpenStack ecosystem, in which hundreds of companies collaborate on thousands of project repositories to deliver cloud distributions. Based on a detailed analysis, we identify clusters of collaborations, and identify four strategies that companies adopt to engage with the OpenStack ecosystem. We alsofind that companies may engage in intentional or passive collaborations, or may work in an isolated fashion. Further, wefi nd that a company's position in the collaboration network is positively associated with its productivity in OpenStack. Our study sheds light on how large OSS ecosystems work, and in particular on the patterns of collaboration within one such large ecosystem.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1196–1208},
numpages = {13},
keywords = {OSS ecosystem, company participation, open collaboration, open source software, openstack, software development},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3128128.3128157,
author = {Das, Sajal K. and Yamana, Hayato},
title = {Securing big data and IoT networks in smart cyber-physical environments},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128157},
doi = {10.1145/3128128.3128157},
abstract = {This position paper highlights security and privacy issues in smart environments based on cyber-physical systems. It also summarizes some of our recent research activities and projects in this area.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {189–194},
numpages = {6},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@article{10.1145/3624010,
author = {Rigaki, Maria and Garcia, Sebastian},
title = {A Survey of Privacy Attacks in Machine Learning},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3624010},
doi = {10.1145/3624010},
abstract = {As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years, research on the privacy aspects of machine learning has received less focus than the security aspects. Our contribution in this research is an analysis of more than 45 papers related to privacy attacks against machine learning that have been published during the past seven years. We propose an attack taxonomy, together with a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses and a discussion of the open problems and future directions identified during our analysis.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {101},
numpages = {34},
keywords = {model inversion, reconstruction, model extraction, property inference, membership inference, machine learning, Privacy}
}

@inproceedings{10.1145/3285017.3285019,
author = {Meloni, P. and Loi, D. and Deriu, G. and Pimentel, A. D. and Sapra, D. and Moser, B. and Shepeleva, N. and Conti, F. and Benini, L. and Ripolles, O. and Solans, D. and Pintor, M. and Biggio, B. and Stefanov, T. and Minakova, S. and Fragoulis, N. and Theodorakopoulos, I. and Masin, M. and Palumbo, F.},
title = {ALOHA: an architectural-aware framework for deep learning at the edge},
year = {2018},
isbn = {9781450365987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285017.3285019},
doi = {10.1145/3285017.3285019},
abstract = {Novel Deep Learning (DL) algorithms show ever-increasing accuracy and precision in multiple application domains. However, some steps further are needed towards the ubiquitous adoption of this kind of instrument. First, effort and skills required to develop new DL models, or to adapt existing ones to new use-cases, are hardly available for small- and medium-sized businesses. Second, DL inference must be brought at the edge, to overcome limitations posed by the classically-used cloud computing paradigm. This requires implementation on low-energy computing nodes, often heterogenous and parallel, that are usually more complex to program and to manage. This work describes the ALOHA framework, that proposes a solution to these issue by means of an integrated tool flow that automates most phases of the development process. The framework introduces architecture-awareness, considering the target inference platform very early, already during algorithm selection, and driving the optimal porting of the resulting embedded application. Moreover it considers security, power efficiency and adaptiveness as main objectives during the whole development process.},
booktitle = {Proceedings of the Workshop on INTelligent Embedded Systems Architectures and Applications},
pages = {19–26},
numpages = {8},
keywords = {deep learning, convolutional neural networks, computer aided design},
location = {Turin, Italy},
series = {INTESA '18}
}

@inproceedings{10.1145/3230905.3230906,
author = {Memeti, Suejb and Pllana, Sabri and Binotto, Al\'{e}cio and Ko\l{}odziej, Joanna and Brandic, Ivona},
title = {A Review of Machine Learning and Meta-heuristic Methods for Scheduling Parallel Computing Systems},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230906},
doi = {10.1145/3230905.3230906},
abstract = {Optimized software execution on parallel computing systems demands consideration of many parameters at run-time. Determining the optimal set of parameters in a given execution context is a complex task, and therefore to address this issue researchers have proposed different approaches that use heuristic search or machine learning. In this paper, we undertake a systematic literature review to aggregate, analyze and classify the existing software optimization methods for parallel computing systems. We review approaches that use machine learning or meta-heuristics for scheduling parallel computing systems. Additionally, we discuss challenges and future research directions. The results of this study may help to better understand the state-of-the-art techniques that use machine learning and meta-heuristics to deal with the complexity of scheduling parallel computing systems. Furthermore, it may aid in understanding the limitations of existing approaches and identification of areas for improvement.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {5},
numpages = {6},
keywords = {scheduling, meta-heuristics, machine learning, Parallel computing},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@article{10.1145/3604802,
author = {Xu, Rui and Ma, Sheng and Guo, Yang and Li, Dongsheng},
title = {A Survey of Design and Optimization for Systolic Array-based DNN Accelerators},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604802},
doi = {10.1145/3604802},
abstract = {In recent years, it has been witnessed that the systolic array is a successful architecture for DNN hardware accelerators. However, the design of systolic arrays also encountered many challenges. As DNN structures and applications become more complex, a DNN hardware accelerator based on the typical systolic array architecture suffers severe performance and efficiency penalties. So, it has motivated a significant amount of research on the redesign and optimization of the systolic array architecture. In this article, we survey these works on analyzing, redesigning, and improving the performance and efficiency of the systolic array architecture. These works are critical to the design flow of DNN accelerators based on systolic arrays. We also provide a technique classification of these works on the basis of their main research idea. Further, we attempt to compare the advantages and disadvantages of different designs and different technologies and provide quantitative results for reference. The aim of this survey is to provide researchers with knowledge of the state-of-the-art in the systolic array architecture and motivate them to design highly efficient DNN accelerators of tomorrow.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {20},
numpages = {37},
keywords = {evaluation strategy, design automation, architectural techniques, hardware-aware software design, hardware accelerator, Systolic array}
}

@inproceedings{10.1145/3264820.3264821,
author = {Shalev, Noam and Partush, Nimrod},
title = {Binary Similarity Detection Using Machine Learning},
year = {2018},
isbn = {9781450359931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264820.3264821},
doi = {10.1145/3264820.3264821},
abstract = {Finding similar procedures in stripped binaries has various use cases in the domains of cyber security and intellectual property. Previous works have attended this problem and came up with approaches that either trade throughput for accuracy or address a more relaxed problem.In this paper, we present a cross-compiler-and-architecture approach for detecting similarity between binary procedures, which achieves both high accuracy and peerless throughput. For this purpose, we employ machine learning alongside similarity by composition: we decompose the code into smaller comparable fragments, transform these fragments to vectors, and build machine learning-based predictors for detecting similarity between vectors that originate from similar procedures.We implement our approach in a tool called Zeek and evaluate it by searching similarities in open source projects that we crawl from the world-wide-web. Our results show that we perform 250X faster than state-of-the-art tools without harming accuracy.},
booktitle = {Proceedings of the 13th Workshop on Programming Languages and Analysis for Security},
pages = {42–47},
numpages = {6},
keywords = {proc2vec, binary similarity},
location = {Toronto, Canada},
series = {PLAS '18}
}

@inproceedings{10.1145/3339186.3339202,
author = {Tsuji, Yohei and Osawa, Kazuki and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
title = {Performance Optimizations and Analysis of Distributed Deep Learning with Approximated Second-Order Optimization Method},
year = {2019},
isbn = {9781450371964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339186.3339202},
doi = {10.1145/3339186.3339202},
abstract = {Faster training of deep neural networks is desired to speed up the research and development cycle in deep learning. Distributed deep learning and second-order optimization methods are two different techniques to accelerate the training of deep neural networks. In the previous work, researchers show that an approximated second-order optimization method, called K-FAC, can mitigate each other drawbacks of the two techniques. However, there was no detailed discussion on the performance, which is critical for the usage in practice. In this work, we propose several performance optimization techniques to reduce the overheads of K-FAC and to accelerate the overall training. Applying all performance optimizations, we are able to speed up the training 1.64 times per iteration compared to a baseline. Additional to the performance optimizations, we construct a simple performance model to predict model training performance to help the users to determine whether distributed K-FAC is appropriate or not for their training in terms of wall-time.},
booktitle = {Workshop Proceedings of the 48th International Conference on Parallel Processing},
articleno = {21},
numpages = {8},
keywords = {second-order optimization, neural networks, deep learning},
location = {Kyoto, Japan},
series = {ICPP Workshops '19}
}

@inproceedings{10.1145/3318216.3363306,
author = {Boubin, Jayson G. and Babu, Naveen T. R. and Stewart, Christopher and Chumley, John and Zhang, Shiqi},
title = {Managing edge resources for fully autonomous aerial systems},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363306},
doi = {10.1145/3318216.3363306},
abstract = {Fully autonomous aerial systems (FAAS) fly complex missions guided wholly by software. If users choose software, compute hardware and aircraft well, FAAS can complete missions faster and safer than unmanned aerial systems piloted by humans. On the other hand, poorly managed edge resources slow down missions, waste energy and inflate costs. This paper presents a model-driven approach to manage FAAS. We fly real FAAS missions, profile compute and aircraft resource usage and model expected demands. Naive profiling approaches use traces from previous flights to infer resource usage. However, edge resources can affect where FAAS fly and which data they sense. Usage profiles can diverge greatly across edge management policies. Instead of using traces, we characterize whole flight areas to accurately model resource usage for any flight path. We combine expected resource demands to model mission throughput, i.e., missions completed per fully charged battery. We validated our model by creating FAAS, measuring mission throughput across many system settings. Our FAAS benchmarks, released through our open source FAAS suite SoftwarePilot, execute realistic missions: autonomous photography, search and rescue, and agricultural scouting using well-known software. Our model predicted throughput with 4% error across mission, software and hardware settings. Competing approaches yielded 10--24% error. We used our SoftwarePilot benchmarks to study (1) GPU acceleration, scale up, and scale out, (2) onboard, edge and cloud computing, (3) energy and monetary budgets, and (4) software driven GPU management. We found that model-driven management can boost mission throughput by 10X and reduce costs by 87%.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {74–87},
numpages = {14},
location = {Arlington, Virginia},
series = {SEC '19}
}

@inproceedings{10.1145/3342195.3387541,
author = {Orenbach, Meni and Baumann, Andrew and Silberstein, Mark},
title = {Autarky: closing controlled channels with self-paging enclaves},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387541},
doi = {10.1145/3342195.3387541},
abstract = {As the first widely-deployed secure enclave hardware, Intel SGX shows promise as a practical basis for confidential cloud computing. However, side channels remain SGX's greatest security weakness. Inparticular, the "controlled-channel attack" on enclave page faults exploits a longstanding architectural side channel and still lacks effective mitigation.We propose Autarky: a set of minor, backward-compatible modifications to the SGX ISA that hide an enclave's page access trace from the host, and give the enclave full control over its page faults. A trusted library OS implements an enclave self-paging policy.We prototype Autarky on current SGX hardware and the Graphene library OS, implementing three paging schemes: a fast software oblivious RAM system made practical by leveraging the proposed ISA, a novel page cluster abstraction for application-aware secure self-paging, and a rate-limiting paging mechanism for unmodified binaries. Overall, Autarky provides a comprehensive defense for controlled-channel attacks which supports efficient secure demand paging, and adds no overheads in page-fault free execution.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {7},
numpages = {16},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{10.1109/MICRO.2018.00056,
author = {Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
title = {CounterMiner: mining big performance data from hardware counters},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00056},
doi = {10.1109/MICRO.2018.00056},
abstract = {Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events1. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running ina"24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {613–626},
numpages = {14},
keywords = {performance counters, data mining, computer architecture, big data},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1145/2797433.2797484,
author = {M\"{a}rtin, Lukas and Koziolek, Anne and Reussner, Ralf},
title = {Quality-oriented Decision Support for maintaining Architectures of fault-tolerant Space Systems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797484},
doi = {10.1145/2797433.2797484},
abstract = {Due to hostile environments, space systems are equipped with hardware redundancies to guarantee proper operation. For reconfigurations beyond redundancies, manual decision making is needed, which results in down times, communication efforts and man hours in maintenance phases.We investigate automated reconfiguration decision support that determines Pareto-optimal architectures w.r.t. variable hardware availability and quality properties. Reconfiguration options for control software according to available sensing and actuation hardware are derived and prioritised w.r.t. predicted qualitative impacts. The knowledge about relations of the system's variations is persisted in a decision model at design time on the level of software architectures. Upon a resources fault, the model is traversed for an alternative architecture. This promotes a transparent analysis of available deployments as well as an acceleration of the reconfiguration process during maintenance. We provide tool support for analysis and a concept for reconfigurations during operation.For evaluation, we inspect a reengineered extension of the attitude control system of the TET-1 micro satellite.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {49},
numpages = {5},
keywords = {software architecture, reconfiguration, maintainability, decision support, Pareto optimization},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@article{10.1145/3183351,
author = {Ko, Glenn G. and Rutenbar, Rob A.},
title = {Real-Time and Low-Power Streaming Source Separation Using Markov Random Field},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1550-4832},
url = {https://doi.org/10.1145/3183351},
doi = {10.1145/3183351},
abstract = {Machine learning (ML) has revolutionized a wide range of recognition tasks, ranging from text analysis to speech to vision, most notably in cloud deployments. However, mobile deployment of these ideas involves a very different category of design problems. In this article, we develop a hardware architecture for a sound source separation task, intended for deployment on a mobile phone. We focus on a novel Markov random field (MRF) sound source separation algorithm that uses expectation-maximization and Gibbs sampling to learn MRF parameters on the fly and infer the best separation of sources. The intrinsically iterative algorithm suggests challenges for both speed and power. A real-time streaming FPGA implementation runs at 150MHz with 207KB RAM, achieves a speed-up of 22\texttimes{} over a software reference, performs with an SDR of up to 7.021dB with 1.601ms latency, and exhibits excellent perceived audio quality. A 45nm CMOS ASIC virtual prototype simulated at 20MHz shows that this architecture is small (&lt;10 million gates) and consumes only 70mW, which is less than 2% of the power of an ARM Cortex-A9 software version. To the best of our knowledge, this is the first Gibbs sampling inference accelerator designed in conventional FPGA/ASIC technology that targets a realistic mobile perceptual application.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = may,
articleno = {17},
numpages = {22},
keywords = {maximum a posteriori inference, blind source separation, Real-time streaming, Markov random field, Gibbs sampling}
}

@inproceedings{10.1145/2590651.2590682,
author = {Urra, Enrique and Cabrera-Paniagua, Daniel and Cubillos, Claudio},
title = {Towards a distributed hyperheuristic deploy architecture},
year = {2014},
isbn = {9781450324359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590651.2590682},
doi = {10.1145/2590651.2590682},
abstract = {The hyperheuristic term is known in the optimization field as an automated methodology for selecting or generating heuristics to solve hard computational search problems. From the design perspective, it is based on decoupling the solving intelligence from the domain expertise, allowing to reuse the same solver for multiple, usually related problem domains. There are few works in which hyperheuristics have been designed and evaluated in distributed environments. In this paper, we propose a conceptual design of a distributed hyperheuristic architecture, from the problem domain deploying perspective, which allows to communicate different optimization environments (such as solver and domain) and to offering a "solving service". Different problems domains could be addressed using an encapsulated hyperheuristic solver, and through well defined interfaces, users can provide different heuristic components to perform the optimization process. The proposed architecture is only an initial step for which different modeling, design and implementation issues must be addressed. Such research should be focused on defining how conceptual design contributions must be leveraged to implement well defined interfaces, capable of connecting hyperheuristic solvers and problem domains within distributed environments.},
booktitle = {Proceedings of the 7th Euro American Conference on Telematics and Information Systems},
articleno = {31},
numpages = {4},
keywords = {optimization, hyperheuristics, distributed architecture},
location = {Valparaiso, Chile},
series = {EATIS '14}
}

@proceedings{10.1145/3643488,
title = {ICDAR '24: Proceedings of the 5th ACM Workshop on Intelligent Cross-Data Analysis and Retrieval},
year = {2024},
isbn = {9798400705496},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Phuket, Thailand}
}

@inproceedings{10.1145/3373376.3378500,
author = {Li, Gushu and Ding, Yufei and Xie, Yuan},
title = {Towards Efficient Superconducting Quantum Processor Architecture Design},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378500},
doi = {10.1145/3373376.3378500},
abstract = {More computational resources (i.e., more physical qubits and qubit connections) on a superconducting quantum processor not only improve the performance but also result in more complex chip architecture with lower yield rate. Optimizing both of them simultaneously is a difficult problem due to their intrinsic trade-off. Inspired by the application-specific design principle, this paper proposes an automatic design flow to generate simplified superconducting quantum processor architecture with negligible performance loss for different quantum programs. Our architecture-design-oriented profiling method identifies program components and patterns critical to both the performance and the yield rate. A follow-up hardware design flow decomposes the complicated design procedure into three subroutines, each of which focuses on different hardware components and cooperates with corresponding profiling results and physical constraints. Experimental results show that our design methodology could outperform IBM's general-purpose design schemes with better Pareto-optimal results.,0},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1031–1045},
numpages = {15},
keywords = {superconducting quantum circuit, quantum computing, architecture design, application-specific architecture},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@proceedings{10.1145/3603287,
title = {ACMSE '24: Proceedings of the 2024 ACM Southeast Conference},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 2024 ACM Southeast Conference (ACMSE 2024) sponsored by ACM and the College of Computing and Software Engineering (CCSE) at Kennesaw State University, Marietta, Georgia, USA. ACMSE 2024 continues the ACM Southeast Conference tradition of participation in all areas of computing disciplines. We hope this conference will be an excellent opportunity to share current and future hot research trends amongst researchers from around the world.},
location = {Marietta, GA, USA}
}

@proceedings{10.1145/3643660,
title = {Designing '24: Proceedings of the 1st International Workshop on Designing Software},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goals of this workshop are to: (1) bring together a group of researchers, practitioners, and educators interested in software design, (2) identify open challenges and new directions for the design of modern software systems, including grand challenges for the community, and (3) discuss novel approaches to designing as well as teaching design. Although the workshop welcomes discussions related to any aspect of software design, the primary focus will be on improving our understanding of design as an activity rather than as an artifact or end product (hence the word designing in the title).},
location = {Lisbon, Portugal}
}

@article{10.1145/3284127,
author = {Wang, Yu and Lee, Victor and Wei, Gu-Yeon and Brooks, David},
title = {Predicting New Workload or CPU Performance by Analyzing Public Datasets},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3284127},
doi = {10.1145/3284127},
abstract = {The marketplace for general-purpose microprocessors offers hundreds of functionally similar models, differing by traits like frequency, core count, cache size, memory bandwidth, and power consumption. Their performance depends not only on microarchitecture, but also on the nature of the workloads being executed. Given a set of intended workloads, the consumer needs both performance and price information to make rational buying decisions. Many benchmark suites have been developed to measure processor performance, and their results for large collections of CPUs are often publicly available. However, repositories of benchmark results are not always helpful when consumers need performance data for new processors or new workloads. Moreover, the aggregate scores for benchmark suites designed to cover a broad spectrum of workload types can be misleading. To address these problems, we have developed a deep neural network (DNN) model, and we have used it to learn the relationship between the specifications of Intel CPUs and their performance on the SPEC CPU2006 and Geekbench 3 benchmark suites. We show that we can generate useful predictions for new processors and new workloads. We also cross-predict the two benchmark suites and compare their performance scores. The results quantify the self-similarity of these suites for the first time in the literature. This work should discourage consumers from basing purchasing decisions exclusively on Geekbench 3, and it should encourage academics to evaluate research using more diverse workloads than the SPEC CPU suites alone.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {53},
numpages = {21},
keywords = {performance comparison, data mining, benchmarking, Performance prediction}
}

@inproceedings{10.1145/2693561.2693566,
author = {Petriu, Dorina C.},
title = {Challenges in Integrating the Analysis of Multiple Non-Functional Properties in Model-Driven Software Engineering},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693566},
doi = {10.1145/2693561.2693566},
abstract = {This vision paper discusses the challenges of integrating the analysis of multiple Non-Functional Properties (NFP) in the model-driven software engineering process, where formal analysis models are generated by model transformations from annotated software models. The paper proposes an integration approach based on an ecosystem of inter-related heterogeneous modeling artifacts intended to support consistent co-evolution of the software and analysis models, cross-model traceability, incremental propagation of changes across models and (semi)automated software process steps. Another goal is to investigate new metaheuristics approaches for reducing the size of the design space to be explored in the search for a design solution that will meet all the non-functional requirements.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {41–46},
numpages = {6},
keywords = {non-functional properties, model-driven engineering, model-driven analysis, ecosystem of models},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@article{10.1145/2724714,
author = {Aysu, Aydin and Yuce, Bilgiday and Schaumont, Patrick},
title = {The Future of Real-Time Security: Latency-Optimized Lattice-Based Digital Signatures},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2724714},
doi = {10.1145/2724714},
abstract = {Advances in quantum computing have spurred a significant amount of research into public-key cryptographic algorithms that are resistant against postquantum cryptanalysis. Lattice-based cryptography is one of the important candidates because of its reasonable complexity combined with reasonable signature sizes. However, in a postquantum world, not only the cryptography will change but also the computing platforms. Large amounts of resource-constrained embedded systems will connect to a cloud of powerful server computers. We present an optimization technique for lattice-based signature generation on such embedded systems; our goal is to optimize latency rather than throughput. Indeed, on an embedded system, the latency of a single signature for user identification or message authentication is more important than the aggregate signature generation rate. We build a high-performance implementation using hardware/software codesign techniques. The key idea is to partition the signature generation scheme into offline and online phases. The signature scheme allows this separation because a large portion of the computation does not depend on the message to be signed and can be handled before the message is given. Then, we can map complex precomputation operations in software on a low-cost processor and utilize hardware resources to accelerate simpler online operations. To find the optimum hardware architecture for the target platform, we define and explore the design space and implement two design configurations. We realize our solutions on the Altera Cyclone-IV CGX150 FPGA. The implementation consists of a NIOS soft-core processor and a low-latency hash and polynomial multiplication engine. On average, the proposed low-latency architecture can generate a signature with a latency of 96 clock cycles at 40MHz, resulting in a response time of 2.4μs for a signing request. On equivalent platforms, this corresponds to a performance improvement of 33 and 105 times compared to previous hardware and software implementations, respectively.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = apr,
articleno = {43},
numpages = {18},
keywords = {lattice-based cryptography, digital signatures, Hardware/software codesign, FPGA}
}

@article{10.1145/3450964,
author = {Charles, Subodha and Mishra, Prabhat},
title = {A Survey of Network-on-Chip Security Attacks and Countermeasures},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3450964},
doi = {10.1145/3450964},
abstract = {With the advances of chip manufacturing technologies, computer architects have been able to integrate an increasing number of processors and other heterogeneous components on the same chip. Network-on-Chip (NoC) is widely employed by multicore System-on-Chip (SoC) architectures to cater to their communication requirements. NoC has received significant attention from both attackers and defenders. The increased usage of NoC and its distributed nature across the chip has made it a focal point of potential security attacks. Due to its prime location in the SoC coupled with connectivity with various components, NoC can be effectively utilized to implement security countermeasures to protect the SoC from potential attacks. There is a wide variety of existing literature on NoC security attacks and countermeasures. In this article, we provide a comprehensive survey of security vulnerabilities in NoC-based SoC architectures and discuss relevant countermeasures.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {101},
numpages = {36},
keywords = {machine learning, Hardware security}
}

@article{10.1145/3447556.3447567,
author = {Chen, Yi-Wei and Song, Qingquan and Hu, Xia},
title = {Techniques for Automated Machine Learning},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3447556.3447567},
doi = {10.1145/3447556.3447567},
abstract = {Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a problem description, its task type, and datasets. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we portray AutoML as a bi-level optimization problem, where one problem is nested within another to search the optimum in the search space, and review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter tuning (AutoMHT), and automated deep learning (AutoDL). Stateof- the-art techniques in the three categories are presented. The iterative solver is proposed to generalize AutoML techniques. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {35–50},
numpages = {16}
}

@article{10.5555/3648699.3648967,
author = {Dwivedi, Raaz and Singh, Chandan and Yu, Bin and Wainwright, Martin},
title = {Revisiting minimum description length complexity in overparameterized models},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Complexity is a fundamental concept underlying statistical learning theory that aims to inform generalization performance. Parameter count, while successful in low-dimensional settings, is not well-justified for overparameterized settings when the number of parameters is more than the number of training samples. We revisit complexity measures based on Rissanen's principle of minimum description length (MDL) and define a novel MDL-based complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We provide an extensive theoretical characterization of MDL-COMP for linear models and kernel methods and show that it is not just a function of parameter count, but rather a function of the singular values of the design or the kernel matrix and the signal-to-noise ratio. For a linear model with n observations, d parameters, and i.i.d. Gaussian predictors, MDL-COMP scales linearly with d when d &lt; n, but the scaling is exponentially smaller—log d for d &gt; n. For kernel methods, we show that MDL-COMP informs minimax in-sample error, and can decrease as the dimensionality of the input increases. We also prove that MDL-COMP upper bounds the in-sample mean squared error (MSE). Via an array of simulations and real-data experiments, we show that a data-driven Prac-MDL-COMP informs hyper-parameter tuning for optimizing test MSE with ridge regression in limited data settings, sometimes improving upon cross-validation and (always) saving computational costs. Finally, our findings also suggest that the recently observed double decent phenomenons in overparameterized models might be a consequence of the choice of non-ideal estimators.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {268},
numpages = {59},
keywords = {complexity, minimum description length, high-dimensional models, ridge regression, kernel regression}
}

@proceedings{10.1145/3638584,
title = {CSAI '23: Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence},
year = {2023},
isbn = {9798400708688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3583788,
title = {ICMLSC '23: Proceedings of the 2023 7th International Conference on Machine Learning and Soft Computing},
year = {2023},
isbn = {9781450398633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chongqing, China}
}

@proceedings{10.1145/3526071,
title = {RoSE '22: Proceedings of the 4th International Workshop on Robotics Software Engineering},
year = {2022},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software engineering is a crucial enabler for successful deployment of robotic applications. The research communities advancing software engineering in robotics, however, are spread over various spe-cialized conferences, such as ICRA, IROS, SIMPAR - each attended mostly by robotics researchers and practitioners - or ICSE andMODELS - mostly attended by software engineering researchers and practitioners. At robotics conferences, software engineering lacks visibility and vice versa.The objective of RoSE is bringing together researchers and practitioners from both domains at a prominent conference to foster cross-fertilization between the two domains. Being the most prominent conference in software engineering, ICSE is the best venue to attract experts from both domains. Hosting this workshop at ICSE enables software engineering researchers to learn more about the challenges of robotics practitioners that (i) require further research from the software engineering community or (ii) are already solved but solutions are unnoticed by roboticists, yet.},
location = {Pittsburgh, Pennsylvania}
}

@proceedings{10.1145/3623264,
title = {MIG '23: Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rennes, France}
}

@article{10.1145/2535575,
author = {Meyer, Brett H. and Hartman, Adam S. and Thomas, Donald E.},
title = {Cost-effective lifetime and yield optimization for NoC-based MPSoCs},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2535575},
doi = {10.1145/2535575},
abstract = {As manufacturing processes scale, designers are increasingly dependent on techniques to mitigate manufacturing defect and permanent failure. In embedded systems-on-chip, system lifetime and yield can be increased using slack—under-utilization in execution and storage resources—so that when components are defective, data and tasks can be remapped and rescheduled. For any given system, the design space of possible slack allocations is both large and complex, consisting of every possible way to replace each component in the initial system with another from the component library. Based on the observation that useful slack is often quantized, we have developed Critical Quantity Slack Allocation (CQSA), an approach that effectively and efficiently allocates execution and storage slack to jointly optimize system yield and cost. While exploring less than 1.4% of the slack allocation design space, our approach consistently outperforms alternative slack allocation techniques to find sets of designs within 1.4% of the lifetime-cost Pareto-optimal front. When applied to yield-cost optimization, our approach again outperforms alternative techniques, exploring less than 1.62% of the design space to find sets of designs within 4.27% of the yield-cost Pareto-optimal front. One advantage of managing failure at the system level is that the same techniques that improve lifetime often also improve yield. As a result, with little modification, CQSA is further able to perform effective joint optimization of lifetime and yield, finding designs within 1.6% of the Pareto-optimal front.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar,
articleno = {12},
numpages = {33},
keywords = {yield optimization, system-level design, slack allocation, network-on-chip, Lifetime optimization}
}

@proceedings{10.1145/3665283,
title = {HEART '24: Proceedings of the 14th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
year = {2024},
isbn = {9798400717277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Porto, Portugal}
}

@proceedings{10.1145/3603165,
title = {ACM TURC '23: Proceedings of the ACM Turing Award Celebration Conference - China 2023},
year = {2023},
isbn = {9798400702334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wuhan, China}
}

@proceedings{10.1145/3637494,
title = {CECCT '23: Proceedings of the 2023 International Conference on Electronics, Computers and Communication Technology},
year = {2023},
isbn = {9798400716300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@proceedings{10.1145/3599957,
title = {RACS '23: Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {With the expansion of both the Internet and the advanced information technology development profession, reliable and convergent computing has attracted increasing interest in both academia and industry. To cope with this important problem, the Research in Adaptive and Convergent Systems (RACS) provides a forum for exchanging highly original ideas about an important class of computing systems. The RACS aims primarily at researchers who have experience in reliable and convergent computing systems and are engaged in the design and implementation of new computing applications. Each year RACS brings together engineers and scientists from diverse communities with interests in practical computing technologies and creates an environment for them to discuss and report experimental results, novel designs, work-in-progress, experiences, case studies, and trend-setting ideas.},
location = {Gdansk, Poland}
}

@proceedings{10.1145/3649476,
title = {GLSVLSI '24: Proceedings of the Great Lakes Symposium on VLSI 2024},
year = {2024},
isbn = {9798400706059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Clearwater, FL, USA}
}

@proceedings{10.1145/3673277,
title = {CNSCT '24: Proceedings of the 2024 3rd International Conference on Cryptography, Network Security and Communication Technology},
year = {2024},
isbn = {9798400716959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Harbin, China}
}

@proceedings{10.1145/3620666,
title = {ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
abstract = {Welcome to the third volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is mostly dedicated to the 2024 fall cycle but also provides some statistics summarizing all three cycles.We introduced several notable changes to ASPLOS this year, most of which were discussed in our previous messages from program chairs in Volume 1 and 2, including: (1) significantly increasing the program committee size to over 220 members (more than twice the size of last year); (2) foregoing synchronous program committee (PC) meetings and instead making all decisions online; (3) overhauling the review assignment process; (4) developing an automated submission format violation identifier script that uncovers, e.g., disallowed vertical space manipulations that "squeeze" space; (5) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee; and (6) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it and highlighting how we believe that it should be handled in the future.Assuming readers have read our previous messages, here, we will only describe differences between the current cycle and the previous ones. These include: (1) Finally unifying submission and acceptance paper formatting instructions (forgoing the `jpaper' class) to rid authors of accepted papers from the need to reformat; (2) Describing the methodology we employed to select best papers, which we believe ensures quality and hope will persist; and (3) Reporting the ethical incidents we encountered and how we handled them. In the final, fourth volume, when the outcome of the ASPLOS'24 fall major revisions will become known, we plan to conduct a broader analysis of all the data we have gathered throughout the year.Following are some key statistics of the fall cycle: 340 submissions were finalized (43% more than last year's fall count and 17% less than our summer cycle) of which 111 are related to accelerators/FPGAs/GPUs, 105 to machine learning, 54 to security, 50 to datacenter/cloud and 50 to storage/memory; 183 (54%) submissions were promoted to the second review round; 39 (11.5%) papers were accepted (of which 19 were awarded artifact evaluation badges); 33 (9.7%) submissions were allowed to submit major revisions and are currently under review (these will be addressed in the fourth volume of ASPLOS'24 and will be presented in ASPLOS'25 if accepted); 1,368 reviews were uploaded; and 4,949 comments were generated during online discussions, of which 4,070 were dedicated to the submissions that made it to the second review round.This year, in the submission form, we asked authors to specify which of the three ASPLOS research areas are related to their submitted work. Analyzing this data revealed that 80%, 39%, and 29% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, generating the highest difference we have observed across the cycles between architecture and the other two. About 46% of the fall submissions are "interdisciplinary," namely, were associated with two or more of the three areas.Overall, throughout all the ASPLOS'24 cycles, we received 922 submissions, constituting a 1.54x increase compared to last year. Our reviewers submitted a total of 3,634 reviews containing more than 2.6 million words, and we also generated 12,655 online comments consisting of nearly 1.2 million words. As planned, PC members submitted an average of 15.7 reviews and a median of 15, and external review committee (ERC) members submitted an average of 4.7 and a median of 5.We accepted 170 papers thus far, written by 1100 authors, leading to an 18.4% acceptance rate, with the aforementioned 33 major revisions still under review. Assuming that the revision acceptance rate will be similar to that of previous cycles, we estimate that ASPLOS'24 will accept nearly 200 (!) papers, namely, 21%–22% of the submissions.The ASPLOS'24 program consists of 193 papers: the 170 papers we accepted thus far and, in addition, 23 major revisions from the fall cycle of ASPLOS'23, which were re-reviewed and accepted. The full details are available in the PDF of the front matter.},
location = {La Jolla, CA, USA}
}

@proceedings{10.1145/3677182,
title = {ASENS '24: Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security},
year = {2024},
isbn = {9798400709784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanchang, China}
}

@proceedings{10.1145/3653804,
title = {CVDL '24: Proceedings of the International Conference on Computer Vision and Deep Learning},
year = {2024},
isbn = {9798400718199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Changsha, China}
}

@proceedings{10.1145/3651781,
title = {ICSCA '24: Proceedings of the 2024 13th International Conference on Software and Computer Applications},
year = {2024},
isbn = {9798400708329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bali Island, Indonesia}
}

@proceedings{10.1145/3576841,
title = {ICCPS '23: Proceedings of the ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2023)},
year = {2023},
isbn = {9798400700361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Antonio, TX, USA}
}

@proceedings{10.1145/3617232,
title = {ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
year = {2024},
isbn = {9798400703720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
abstract = {Welcome to the first volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. For the second year, ASPLOS employs a model of three submission deadlines - spring, summer and fall - along with a major revision mechanism, which, as an alternative to rejection, gives the authors of some submissions the opportunity to fix a list of problems and then resubmit their work to the subsequent review cycle.We introduced several notable changes to ASPLOS this year. Briefly, these include significantly increasing the program committee size to over 220 members (more than twice the size of last year), foregoing synchronous PC meetings and instead making all decisions online, and overhauling the review assignment process. The overhaul includes comparing the textual contents of submissions to the contents of papers authored by the reviewers and using a metric that quantifies the goodness of the match to guide the assignment of reviewers to submissions. The overhaul additionally involves asking reviewers to predict the expertise of their future reviews for a subset of the submissions and using this input as well, among others, for the assignment process.Key statistics of the ASPLOS'24 spring cycle include: 173 submissions were finalized (nearly double last year's spring count), with 47 (27%) related to machine learning, 41 to storage/memory, 39 to accelerators/FPGAs/GPUs, and 27 to security; 87 (51%) submissions were promoted to the second review round; 28 (16.2%) papers were accepted, with 16, 13, and 9 awarded artifact evaluation badges of "available," "functional," and "reproduced," respectively; 27 (15.6%) submissions were allowed to submit major revisions, of which 22 were subsequently accepted during the summer cycle; 762 reviews were uploaded; and 2,868 comments were generated during online discussions.Another change we introduced this year is asking authors to specify their per-submission most-related broader areas of research, which revealed that 54%, 42%, and 25% of the submissions are associated with architecture, operating systems, and programming languages, respectively, with only 21% being interdisciplinary. The full details are available in the PDF of the front matter.},
location = {La Jolla, CA, USA}
}

@proceedings{10.1145/3672758,
title = {CAICE '24: Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering},
year = {2024},
isbn = {9798400716942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi' an, China}
}

@proceedings{10.1145/3654522,
title = {ICIIT '24: Proceedings of the 2024 9th International Conference on Intelligent Information Technology},
year = {2024},
isbn = {9798400716713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ho Chi Minh City, Vietnam}
}

@proceedings{10.1145/3639856,
title = {AIMLSystems '23: Proceedings of the Third International Conference on AI-ML Systems},
year = {2023},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@proceedings{10.1145/3605573,
title = {ICPP '23: Proceedings of the 52nd International Conference on Parallel Processing},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salt Lake City, UT, USA}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@proceedings{10.1145/3660853,
title = {AICCONF '24: Proceedings of the Cognitive Models and Artificial Intelligence Conference},
year = {2024},
isbn = {9798400716928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {undefinedstanbul, Turkiye}
}

@proceedings{10.1145/3649153,
title = {CF '24: Proceedings of the 21st ACM International Conference on Computing Frontiers},
year = {2024},
isbn = {9798400705977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ischia, Italy}
}

@proceedings{10.1145/3607199,
title = {RAID '23: Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, China}
}

@proceedings{10.1145/3587716,
title = {ICMLC '23: Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Zhuhai, China}
}

@proceedings{10.1145/3600100,
title = {BuildSys '23: Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Istanbul, Turkey}
}

@proceedings{10.1145/3631882,
title = {MEMSYS '23: Proceedings of the International Symposium on Memory Systems},
year = {2023},
isbn = {9798400716447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Alexandria, VA, USA}
}

@proceedings{10.1145/3650400,
title = {EITCE '23: Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering},
year = {2023},
isbn = {9798400708305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1109/3655039,
title = {ASPDAC '24: Proceedings of the 29th Asia and South Pacific Design Automation Conference},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
abstract = {ASP-DAC is a high-quality and premium conference on Electronic Design Automation (EDA) area like other sister conferences such as Design Automation Conference (DAC), Design, Automation &amp; Test in Europe (DATE), International Conference on Computer-Aided Design (ICCAD), and Embedded Systems Week (ESWEEK). ASP-DAC started in 1995 and has continuously offered opportunity to know the recent advanced technologies on LSI design and design automation areas, and to communicate each other for researchers and designers around Asia and South Pacific regions.},
location = {Incheon, Republic of Korea}
}

@proceedings{10.1145/3674225,
title = {PEAI '24: Proceedings of the 2024 International Conference on Power Electronics and Artificial Intelligence},
year = {2024},
isbn = {9798400716638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3665689,
title = {BIC '24: Proceedings of the 2024 4th International Conference on Bioinformatics and Intelligent Computing},
year = {2024},
isbn = {9798400716645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3560905,
title = {SenSys '22: Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SenSys 2022, the 20th ACM Conference on Embedded Networked Sensor Systems, the premier computer systems conference focused on networked sensing systems and applications.},
location = {Boston, Massachusetts}
}

@proceedings{10.1145/3638884,
title = {ICCIP '23: Proceedings of the 2023 9th International Conference on Communication and Information Processing},
year = {2023},
isbn = {9798400708909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lingshui, China}
}

@proceedings{10.1145/3573428,
title = {EITCE '22: Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
year = {2022},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@proceedings{10.1145/3582016,
title = {ASPLOS 2023: Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to introduce the third and final volume of ASPLOS&nbsp;’23. For the first time, ASPLOS has embarked on a new multi-deadline review model. ASPLOS&nbsp;’23 featured 3 deadlines spaced throughout the year and published papers in three volumes. Multiple deadlines are meant to encourage authors to submit their papers when ready and to facilitate the selection of some papers for revision. In this, our final program chairs’ message, we will provide details on the execution of the third submission cycle along with a detailed discussion of the entire ASPLOS&nbsp;’23 process.},
location = {Vancouver, BC, Canada}
}

@proceedings{10.1145/3624062,
title = {SC-W '23: Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3579371,
title = {ISCA '23: Proceedings of the 50th Annual International Symposium on Computer Architecture},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1109/3656336,
title = {ASPDAC '22: Proceedings of the 27th Asia and South Pacific Design Automation Conference},
year = {2022},
isbn = {9781665421355},
publisher = {IEEE Press},
abstract = {ASP-DAC is a high-quality and premium conference on Electronic Design Automation (EDA) like other sister conferences such as Design Automation Conference (DAC), Design, Automation &amp; Test in Europe (DATE), International Conference on Computer Aided Design (ICCAD), and Embedded Systems Week (ESWEEK). ASP-DAC started in 1995 and has continuously offered opportunities for researchers and designers in the world to learn about the advancements on design and automation of electronic systems.},
location = {Taipei, Taiwan}
}

@proceedings{10.1145/3533271,
title = {ICAIF '22: Proceedings of the Third ACM International Conference on AI in Finance},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3508352,
title = {ICCAD '22: Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Jointly sponsored by ACM and IEEE, ICCAD is the premier forum to explore new challenges, present leading-edge innovative solutions, and identify emerging technologies in the Electronic Design Automation (EDA) research areas. ICCAD covers the full range of Computer-Aided Design (CAD) topics - from device and circuit-level up through system-level, as well as post-CMOS design.},
location = {San Diego, California}
}

@proceedings{10.1145/3638529,
title = {GECCO '24: Proceedings of the Genetic and Evolutionary Computation Conference},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3613904,
title = {CHI '24: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Honolulu, HI, USA}
}